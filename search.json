[
  {
    "objectID": "posts/5-anomaly.html",
    "href": "posts/5-anomaly.html",
    "title": "Anomaly/outlier detection",
    "section": "",
    "text": "Image credit: Ajitesh Kumar\n\n\n\n\nRun linear regression on uncorrelated data\nfrom pandas import DataFrame\nfrom numpy.random import normal, random, seed\nimport plotly.express as px\nseed(5805)\n\nn = 100\ndf = DataFrame({'x': normal(size=n-1) + random(n-1), 'y': normal(size=n-1)})\nfig = px.scatter(df, x='x', y='y', trendline='ols').show();\n\n\n\n\n                                                \nFigure 1: Uncorrelated data\n\n\n\n\n\nRun linear regression on data with an outlier\ndf.loc[n-1] = {'x': 10, 'y': 10, 'outlier': True}\nfig = px.scatter(df, x='x', y='y', trendline='ols')\nfig.update_traces(marker=dict(color=[\"#636efa\"] * (n-1) + [\"#EF553B\"]))\nfig.show();\n\n\n\n\n                                                \nFigure 2: Regular OLS\n\n\n\n\\[\nL_\\delta(a) = \\begin{cases}\n    \\frac{1}{2} a^2 & \\text{if } |a| \\leq \\delta\\ ,\\\\\n    \\delta(|a| - \\frac{1}{2} \\delta) & \\text{otherwise}\\ .\n\\end{cases}\n\\tag{1}\\]\n\n\n\nImage credit: Wikipedia\n\n\n\n\nRun linear regression with Huber loss\nfrom numpy import linspace\nimport plotly.graph_objects as go\nfrom sklearn.linear_model import HuberRegressor\n\nfig = px.scatter(df, x='x', y='y')\nfig.update_traces(marker=dict(color=[\"#636efa\"] * (n-1) + [\"#EF553B\"]))\nX = df['x'].values.reshape(-1, 1)\nx_range = linspace(X.min(), X.max(), 100).reshape(-1, 1)\ny = df['y']\n\nmodel = HuberRegressor(epsilon=1)\nmodel.fit(X, y)\ny_fit = model.predict(x_range)\nfig.add_traces(go.Scatter(x=x_range.squeeze(), y=y_fit,\n    showlegend=False, line=dict(color=\"#636efa\"))).show();\n\n\n\n\n                                                \nFigure 3: OLS with Huber loss"
  },
  {
    "objectID": "posts/5-anomaly.html#outliers",
    "href": "posts/5-anomaly.html#outliers",
    "title": "Anomaly/outlier detection",
    "section": "",
    "text": "Image credit: Ajitesh Kumar\n\n\n\n\nRun linear regression on uncorrelated data\nfrom pandas import DataFrame\nfrom numpy.random import normal, random, seed\nimport plotly.express as px\nseed(5805)\n\nn = 100\ndf = DataFrame({'x': normal(size=n-1) + random(n-1), 'y': normal(size=n-1)})\nfig = px.scatter(df, x='x', y='y', trendline='ols').show();\n\n\n\n\n                                                \nFigure 1: Uncorrelated data\n\n\n\n\n\nRun linear regression on data with an outlier\ndf.loc[n-1] = {'x': 10, 'y': 10, 'outlier': True}\nfig = px.scatter(df, x='x', y='y', trendline='ols')\nfig.update_traces(marker=dict(color=[\"#636efa\"] * (n-1) + [\"#EF553B\"]))\nfig.show();\n\n\n\n\n                                                \nFigure 2: Regular OLS\n\n\n\n\\[\nL_\\delta(a) = \\begin{cases}\n    \\frac{1}{2} a^2 & \\text{if } |a| \\leq \\delta\\ ,\\\\\n    \\delta(|a| - \\frac{1}{2} \\delta) & \\text{otherwise}\\ .\n\\end{cases}\n\\tag{1}\\]\n\n\n\nImage credit: Wikipedia\n\n\n\n\nRun linear regression with Huber loss\nfrom numpy import linspace\nimport plotly.graph_objects as go\nfrom sklearn.linear_model import HuberRegressor\n\nfig = px.scatter(df, x='x', y='y')\nfig.update_traces(marker=dict(color=[\"#636efa\"] * (n-1) + [\"#EF553B\"]))\nX = df['x'].values.reshape(-1, 1)\nx_range = linspace(X.min(), X.max(), 100).reshape(-1, 1)\ny = df['y']\n\nmodel = HuberRegressor(epsilon=1)\nmodel.fit(X, y)\ny_fit = model.predict(x_range)\nfig.add_traces(go.Scatter(x=x_range.squeeze(), y=y_fit,\n    showlegend=False, line=dict(color=\"#636efa\"))).show();\n\n\n\n\n                                                \nFigure 3: OLS with Huber loss"
  },
  {
    "objectID": "posts/5-anomaly.html#changepoint-detection",
    "href": "posts/5-anomaly.html#changepoint-detection",
    "title": "Anomaly/outlier detection",
    "section": "Changepoint detection",
    "text": "Changepoint detection\n\n\nLoad and display the Nile dataset\nimport statsmodels.api as sm\nimport plotly.express as px\n\n# Time series data\nnile = sm.datasets.get_rdataset(\"Nile\").data\nfig = px.line(nile, x=\"time\", y=\"value\")\n\n# Ground truth change point\nfig.add_vline(x=1899, line_width=2, line_dash=\"dash\", line_color=\"black\")\n\n\n\n\n                                                \nFigure 4: Yearly volume of the Nile river at Aswan. A dam was built in 1898.\n\n\n\n\nCUSUM\nPage (1954)\nMATLAB has a built-in cusum function\n\n\nA visual example of changepoint detection\nfrom numpy import concatenate, cumsum, mean\n\n# Simulate coin flipping\nn, t = [75, 25], 50\np0, p1 = 0.45, 0.75\ncoin = concatenate((random((n[0], t)) &lt; p0, random((n[1], t)) &lt; p1))\ncumul = mean(cumsum(2*coin-1, axis=0), axis=1)\n\n# Display accumulated counts\navg0 = (2*p0 - 1) * n[0]\navg1 = (2*p1 - 1) * n[1] + avg0\nfig = px.scatter(y=cumul, labels={'x': 'Flip count', 'y': \"Accumulated (Heads - Tails)\"})\nfig.add_traces(go.Scatter(x=[n[0],n[0]+n[1]], y=[avg0, avg1],\n    mode=\"lines\", name=\"Changed\", line=dict(dash='dash')))\nfig.add_traces(go.Scatter(x=[0,n[0]], y=[0, avg0], mode=\"lines\", name=\"Original\"))\n\n\n\n\n                                                \nFigure 5: Cumulative outcomes of flipping a coin with changing probabilities.\n\n\n\n\\(Z_n = (X_n - \\bar{x}) / s\\)\n\\[\n\\begin{align}\n    H_n = \\max(0, H_{n-1} + Z_n - \\omega)\\ ,\\\\\n    L_n = \\min(0, L_{n-1} + Z_n + \\omega)\\ ,\n\\end{align}\n\\tag{2}\\]\nwhere we set \\(H_0 = L_0 = 0\\).\n\n\nImplementation of the CUSUM algorithm\nimport numpy as np\n\ndef cusum(X, lim=5, w=0, mean=None, std=None, plot=True):\n    '''Detect changes in mean using the CUSUM test.\n\n    ARGUMENTS:\n    lim: number of standard deviations in drift to be detected.\n    w: number of standard devitaions as damping coefficient.\n    mean: expected mean or None to compute from 25 leading samples.\n    std: expected standard deviation or None to compute from 25 leading samples.\n    plot: whether to display a visualization.\n\n    RETURNS:\n    index of the first detected change or None.\n    '''\n    # Estimate mean and standard deviation\n    if mean is None:\n        mean = np.mean(X[:25])\n    if std is None:\n        std = np.mean(X[:25])\n    Z = (X - mean) / std\n\n    n = len(Z)\n    H = np.zeros(n)\n    L = np.zeros(n)\n    for i, z in enumerate(Z):\n        H[i] = max(0, H[i-1] + z - w)\n        L[i] = min(0, L[i-1] + z + w)\n\n    idx_H = np.argmax(np.append(H, lim) &gt;= lim)\n    idx_L = np.argmax(np.append(L, -lim) &lt;= -lim)\n    idx = min(idx_H, idx_L)\n\n    if plot:\n        fig = go.Figure()\n        fig.add_traces(go.Scatter(y=H, name=\"Upper cumulative sum\"))\n        fig.add_traces(go.Scatter(y=L, name=\"Lower cumulative sum\"))\n        fig.add_hline(y=lim, line_dash='dash', line_color=\"#636efa\")\n        fig.add_hline(y=-lim, line_dash='dash', line_color=\"#EF553B\")\n        if idx &lt; n:\n            HL = H if idx_H &lt;= idx_L else L\n            fig.add_traces(go.Scatter(x=[idx], y=[HL[idx]],\n                mode=\"markers\", marker_symbol='circle-open', name=\"Changepoint\",\n                marker=dict(size=12, line_width=4, color='black')))\n        fig.show()\n\n    return idx if idx &lt; n else None\n\nchange = cusum(nile['value'], lim=2)\nprint(f\"Detected changepoint at year {nile['time'][change]}.\")\n\n\n\n\n                                                \nFigure 6: CUSUM control chart.\n\n\n\nDetected changepoint at year 1905."
  },
  {
    "objectID": "posts/3-regression.html",
    "href": "posts/3-regression.html",
    "title": "Linear and nonlinear regression",
    "section": "",
    "text": "glm(y ~ x, ...) or glm(y ~ ., ...)\n\n\nCreate artificial data\nimport plotly.express as px\nfrom numpy.random import rand, normal, seed\nfrom numpy import repeat, concatenate, power\nfrom pandas import DataFrame\nseed(5805)\n\n# Create artificial data\nn, r, s, b = 100, 100, 5, 10\nlin = repeat(rand(n, 1) * r, 2, axis=1) + normal(scale=s, size=(n, 2))\nexp = power(b, lin[:,1:2] / r) * r / b\nexp = concatenate((lin[:,0:1], exp), axis=1)\n\n# Plot data\nlin_df = DataFrame({'x': lin[:,0], 'y': lin[:,1]})\nexp_df = DataFrame({'x': exp[:,0], 'y': exp[:,1]})\npx.scatter(lin_df, x='x', y='y').show()\npx.scatter(exp_df, x='x', y='y').show();\n\n\n\n\n\n\n\n                                                \n(a) Linear growth\n\n\n\n\n\n                                                \n(b) Exponential growth\n\n\n\nFigure 1: A couple of artificial datasets"
  },
  {
    "objectID": "posts/3-regression.html#regression",
    "href": "posts/3-regression.html#regression",
    "title": "Linear and nonlinear regression",
    "section": "",
    "text": "glm(y ~ x, ...) or glm(y ~ ., ...)\n\n\nCreate artificial data\nimport plotly.express as px\nfrom numpy.random import rand, normal, seed\nfrom numpy import repeat, concatenate, power\nfrom pandas import DataFrame\nseed(5805)\n\n# Create artificial data\nn, r, s, b = 100, 100, 5, 10\nlin = repeat(rand(n, 1) * r, 2, axis=1) + normal(scale=s, size=(n, 2))\nexp = power(b, lin[:,1:2] / r) * r / b\nexp = concatenate((lin[:,0:1], exp), axis=1)\n\n# Plot data\nlin_df = DataFrame({'x': lin[:,0], 'y': lin[:,1]})\nexp_df = DataFrame({'x': exp[:,0], 'y': exp[:,1]})\npx.scatter(lin_df, x='x', y='y').show()\npx.scatter(exp_df, x='x', y='y').show();\n\n\n\n\n\n\n\n                                                \n(a) Linear growth\n\n\n\n\n\n                                                \n(b) Exponential growth\n\n\n\nFigure 1: A couple of artificial datasets"
  },
  {
    "objectID": "posts/3-regression.html#linear-regression",
    "href": "posts/3-regression.html#linear-regression",
    "title": "Linear and nonlinear regression",
    "section": "Linear regression",
    "text": "Linear regression\n\n\n\nImage credit: Wikipedia\n\n\n\\[\n\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y = X^\\dagger y\\ ,\n\\tag{1}\\]\nwhere \\(X^\\dagger\\) denotes the left pseudoinverse of \\(X\\).\n\n\nCode for polynomial OLS\n# Taken from https://plotly.com/python/ml-regression/\nfrom numpy import linspace\nimport plotly.graph_objects as go\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\ndef plot(df, trend, **kwargs):\n    fig = px.scatter(df, x='x', y='y')\n    X = df['x'].values.reshape(-1, 1)\n    x_range = linspace(X.min(), X.max(), 100).reshape(-1, 1)\n    y = df['y']\n\n    y_fit = trend(X, x_range, y, **kwargs)\n    fig.add_traces(go.Scatter(x=x_range.squeeze(), y=y_fit,\n        showlegend=False, line=dict(color='black')))\n    return fig\n\ndef poly(X, x_range, y, degree=2):\n    poly = PolynomialFeatures(degree)\n    poly.fit(X)\n    X_poly = poly.transform(X)\n    x_range_poly = poly.transform(x_range)\n\n    model = LinearRegression(fit_intercept=False)\n    model.fit(X_poly, y)\n    return model.predict(x_range_poly)\n\ndef show(fig):\n    fig.update_layout(\n            margin=dict(l=0, r=20, t=20, b=20),\n            height=380\n        ).show()\n\n\n\n\nFit trendlines via ordinary least squares\nshow(px.scatter(lin_df, x='x', y='y', trendline='ols',\n    trendline_color_override='black'))\nshow(plot(exp_df, poly, degree=2))\nshow(px.scatter(exp_df, x='x', y='y', trendline='ols',\n    trendline_options=dict(log_y=True),\n    trendline_color_override='black'))\n\n\n\n\n\n\n\n                                                \n(a) Ordinary least squares\n\n\n\n\n\n                                                \n(b) OLS of y ~ x^2 + x\n\n\n\n\n\n                                                \n(c) OLS of log(y) ~ x\n\n\n\nFigure 2: Trendlines via ordinary least squares\n\n\n\n\n\nCode for KNN regression\n# Taken from https://plotly.com/python/ml-regression/\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsRegressor\n\ndef knn(X, x_range, y, k=20, weights='uniform'):\n    knn_uni = KNeighborsRegressor(10, weights=weights)\n    knn_uni.fit(X, y)\n    return knn_uni.predict(x_range.reshape(-1, 1))\n\n\n\n\nFit trendlines via miscellaneous methods\nfrom scipy.optimize import curve_fit\n\n# Nonlinear regression\nf = lambda x, a, b, c: a * x / (b + x) + c\ndef nonlinear(X, x_range, y, f, init=None):\n    args, _ = curve_fit(f, exp_df['x'], exp_df['y'], p0=init)\n    return f(x_range, *args).squeeze()\nshow(plot(exp_df, nonlinear, f=f, init=[-90, -200, 10]))\n\n# Non-parametric regression\nshow(plot(exp_df, knn, k=20))\nshow(px.scatter(exp_df, x='x', y='y', trendline='lowess',\n   trendline_color_override='black'))\n\n\n\n\n\n\n\n                                                \n(a) Nonlinear least squares\n\n\n\n\n\n                                                \n(b) KNN Regression\n\n\n\n\n\n                                                \n(c) LOWESS\n\n\n\nFigure 3: Trendlines via other regression methods\n\n\n\n\n\nFit a multiratiate regression model\n# Taken from https://plotly.com/python/ml-regression/\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom sklearn.svm import SVR\n\nmesh_size = .02\nmargin = 0\n\ndf = px.data.iris()\n\nX = df[['sepal_width', 'sepal_length']]\ny = df['petal_width']\n\n# Condition the model on sepal width and length, predict the petal width\nmodel = SVR(C=1.)\nmodel.fit(X, y)\n\n# Create a mesh grid on which we will run our model\nx_min, x_max = X.sepal_width.min() - margin, X.sepal_width.max() + margin\ny_min, y_max = X.sepal_length.min() - margin, X.sepal_length.max() + margin\nxrange = np.arange(x_min, x_max, mesh_size)\nyrange = np.arange(y_min, y_max, mesh_size)\nxx, yy = np.meshgrid(xrange, yrange)\n\n# Run model\npred = model.predict(np.c_[xx.ravel(), yy.ravel()])\npred = pred.reshape(xx.shape)\n\n# Generate the plot\nfig = px.scatter_3d(df, x='sepal_width', y='sepal_length', z='petal_width')\nfig.update_traces(marker=dict(size=5))\nfig.add_traces(go.Surface(x=xrange, y=yrange, z=pred, name='pred_surface', opacity=0.75))\nfig.show()\n\n\n\n\n                                                \nFigure 4: Support Vector Regression on the Iris dataset"
  },
  {
    "objectID": "posts/1-probability.html",
    "href": "posts/1-probability.html",
    "title": "Probability theory and random variables",
    "section": "",
    "text": "You may remember them from your intro to statistics class. They’re less like a number and more like a distribution. Less like -1°C (the temperature here, now) and more like -3.7°C to 7.2°C (the daily mean temperature in Blacksburg in December). Less like a particular experiment outcome and more like the experiment itself. Basically, a number that isn’t fixed and can vary.\nPretty simple, right? Here’s the definition; taken from Wikipedia, which in order takes it from Fristedt and Gray (1996, 11).\n\nDefinition 1 (Random variable). Let \\((\\Omega, \\mathcal{F}, P)\\) be a probability space and \\((E, \\mathcal{E})\\) a measurable space. Then an \\((E, \\mathcal{E})\\)-valued random variable is a measurable function \\(X : \\Omega \\to E\\), which measures that, for every subset \\(B \\in \\mathcal{E}\\), its preimage is \\(\\mathcal{F}\\)-measurable; \\(X^{-1}(B) \\in \\mathcal{F}\\), where \\(X^{-1}(B) = \\{\\omega : X(\\omega) \\in B\\}\\).\n\nUh… what? Let’s break this down a little.\n\n\nLet \\(\\Omega = \\{😸,😿\\}\\) denote the outcomes of an experiment, with associated probabilities \\(\\mathrm{P}(😸) = 0.5\\) and \\(\\mathrm{P}(😿) = 0.5\\). What is the average outcome of this experiment? Certainly, it can’t be \\(0.5😸 + 0.5😿\\); that quantity doesn’t make sense. We instead need something we can count, such as the number of happy cats. This we can denode by a random variable \\(X\\), something that maps event outcomes in \\(\\Omega\\) to measurable quantities in \\(E\\), which in this case is \\(\\{0, 1\\}\\). Let \\(X(😸) = 1\\) and \\(X(😿) = 0\\); then, to get the probability of a particular outcome in \\(E\\) we must find all entries in \\(\\Omega\\) that map to it and sum their probabilities. This can be written as \\[\n\\mathrm{P}(X = 1) = \\mathrm{P}(\\{ \\omega \\in \\Omega : X(\\omega) = 1 \\}) = \\mathrm{P}(😸) = 0.5\\ ,\n\\tag{1}\\]\nand analogously for \\(\\mathrm{P}(X = 0) = \\mathrm{P}(😿) = 0.5\\). We can thus evaluate the expected value as \\[\n\\mathbb{E}[X] = \\sum_{x \\in \\Omega} x \\mathrm{P}(X = x) = 0.5 \\mathrm{P}(X = 😸) + 0.5 \\mathrm{P}(X = 😿) = 0.5 \\cdot 0 + 0.5 \\cdot 1 = 0.5\\ ,\n\\tag{2}\\]\nmeaning that on average the experiment produces one half of a happy cat.\nThe reason for all the remaining clunkiness in the definition is that its formality stems from measure theory, a branch of probability theory that can deal with otherwise confusing situations like distributions that are part-discrete and part-continuous. For instance, we need to define \\(\\mathcal{F}\\) and \\(\\mathcal{E}\\) as collections of subsets of \\(\\Omega\\) and \\(E\\) since we can’t map events and values to probabilities directly (continuous distributions would have zero probability), and must instead map sets. When calculating \\(\\mathbb{E}[X]\\) we would then have to take the Lebesgue integral \\(\\int_\\Omega X dP\\).\nOne additional small note: we can arrange random variables \\(X_1, \\ldots, X_n\\) in a vector to obtain a random vector \\(X = \\begin{bmatrix} X_1, \\ldots, X_n \\end{bmatrix}^\\top\\). The only mathematical caveat is that the variables must act on the same probability and measurable spaces.\n\n\n\nIn short, I think the best way to remember what random variables are, is by what they aren’t. Specifically, they are not random, and they are not variables. They don’t themselves serve as a source of randomness, instead only taking as an input the outcomes of an already random experiment. They do not vary; despite potentially giving different values upon being sampled, the mathematical object they describe is fixed. Although, given the context in which they typically appear, I can certainly see the appeal of the name."
  },
  {
    "objectID": "posts/1-probability.html#random-variables",
    "href": "posts/1-probability.html#random-variables",
    "title": "Probability theory and random variables",
    "section": "",
    "text": "You may remember them from your intro to statistics class. They’re less like a number and more like a distribution. Less like -1°C (the temperature here, now) and more like -3.7°C to 7.2°C (the daily mean temperature in Blacksburg in December). Less like a particular experiment outcome and more like the experiment itself. Basically, a number that isn’t fixed and can vary.\nPretty simple, right? Here’s the definition; taken from Wikipedia, which in order takes it from Fristedt and Gray (1996, 11).\n\nDefinition 1 (Random variable). Let \\((\\Omega, \\mathcal{F}, P)\\) be a probability space and \\((E, \\mathcal{E})\\) a measurable space. Then an \\((E, \\mathcal{E})\\)-valued random variable is a measurable function \\(X : \\Omega \\to E\\), which measures that, for every subset \\(B \\in \\mathcal{E}\\), its preimage is \\(\\mathcal{F}\\)-measurable; \\(X^{-1}(B) \\in \\mathcal{F}\\), where \\(X^{-1}(B) = \\{\\omega : X(\\omega) \\in B\\}\\).\n\nUh… what? Let’s break this down a little.\n\n\nLet \\(\\Omega = \\{😸,😿\\}\\) denote the outcomes of an experiment, with associated probabilities \\(\\mathrm{P}(😸) = 0.5\\) and \\(\\mathrm{P}(😿) = 0.5\\). What is the average outcome of this experiment? Certainly, it can’t be \\(0.5😸 + 0.5😿\\); that quantity doesn’t make sense. We instead need something we can count, such as the number of happy cats. This we can denode by a random variable \\(X\\), something that maps event outcomes in \\(\\Omega\\) to measurable quantities in \\(E\\), which in this case is \\(\\{0, 1\\}\\). Let \\(X(😸) = 1\\) and \\(X(😿) = 0\\); then, to get the probability of a particular outcome in \\(E\\) we must find all entries in \\(\\Omega\\) that map to it and sum their probabilities. This can be written as \\[\n\\mathrm{P}(X = 1) = \\mathrm{P}(\\{ \\omega \\in \\Omega : X(\\omega) = 1 \\}) = \\mathrm{P}(😸) = 0.5\\ ,\n\\tag{1}\\]\nand analogously for \\(\\mathrm{P}(X = 0) = \\mathrm{P}(😿) = 0.5\\). We can thus evaluate the expected value as \\[\n\\mathbb{E}[X] = \\sum_{x \\in \\Omega} x \\mathrm{P}(X = x) = 0.5 \\mathrm{P}(X = 😸) + 0.5 \\mathrm{P}(X = 😿) = 0.5 \\cdot 0 + 0.5 \\cdot 1 = 0.5\\ ,\n\\tag{2}\\]\nmeaning that on average the experiment produces one half of a happy cat.\nThe reason for all the remaining clunkiness in the definition is that its formality stems from measure theory, a branch of probability theory that can deal with otherwise confusing situations like distributions that are part-discrete and part-continuous. For instance, we need to define \\(\\mathcal{F}\\) and \\(\\mathcal{E}\\) as collections of subsets of \\(\\Omega\\) and \\(E\\) since we can’t map events and values to probabilities directly (continuous distributions would have zero probability), and must instead map sets. When calculating \\(\\mathbb{E}[X]\\) we would then have to take the Lebesgue integral \\(\\int_\\Omega X dP\\).\nOne additional small note: we can arrange random variables \\(X_1, \\ldots, X_n\\) in a vector to obtain a random vector \\(X = \\begin{bmatrix} X_1, \\ldots, X_n \\end{bmatrix}^\\top\\). The only mathematical caveat is that the variables must act on the same probability and measurable spaces.\n\n\n\nIn short, I think the best way to remember what random variables are, is by what they aren’t. Specifically, they are not random, and they are not variables. They don’t themselves serve as a source of randomness, instead only taking as an input the outcomes of an already random experiment. They do not vary; despite potentially giving different values upon being sampled, the mathematical object they describe is fixed. Although, given the context in which they typically appear, I can certainly see the appeal of the name."
  },
  {
    "objectID": "posts/1-probability.html#concentration-bounds",
    "href": "posts/1-probability.html#concentration-bounds",
    "title": "Probability theory and random variables",
    "section": "Concentration bounds",
    "text": "Concentration bounds\nSuppose we have a random variable \\(X\\) that is promised to give values in \\([0,1]\\). We may study such an \\(X\\) via its parameters, such as its expected value \\(\\mathbb{E}[X]\\). While we may have access to that during mathematical analysis, in practice we typically have to rely on some data1 \\(\\{X_i\\}_{i=1}^n\\) sampled from \\(X\\) to compute statistics such as the sample mean \\(\\frac{1}{n} \\sum_{i=1}^n X_i\\). But, do we know how accurate our approximations are? For instance, can we find some \\(f\\) such that \\[\n\\Biggr \\lvert \\frac{1}{n} \\sum_{i=1}^n X_i - \\mathbb{E}[X] \\Biggr \\rvert &lt; f(n)\\ ?\n\\tag{3}\\]\n\nHoeffding’s inequality\nThis famous inequality provides one such bound.\n\nTheorem 1 (Hoeffding’s inequality). Let \\(X_1, \\ldots, X_n\\) be independent random variables where with probability \\(1\\), \\(X_i \\in [a_i, b_i]\\) (i.e., bounded). Let \\(R_i \\coloneqq b_i - a_i\\) be the range. Then, for all \\(\\epsilon \\geq 0\\), \\[\nP \\left( \\Biggr \\lvert \\sum_{i=1}^n (X_i - \\mathbb{E}[X_i]) \\Biggr \\rvert \\leq \\epsilon \\right) \\geq 2 \\exp \\left( -\\frac{2 \\epsilon^2}{\\sum_{i=1}^n R_i^2} \\right)\\ .\n\\tag{4}\\]\n\nWe can re-arrange the variables to obtain that for any fixed \\(\\delta \\in (0, 1]\\), with probability at least \\(1 - \\delta\\), \\[\n\\Biggr \\lvert \\sum_{i=1}^n (X_i - \\mathbb{E}[X_i]) \\Biggr \\rvert \\leq \\sqrt{\\frac{\\sum_{i=1}^n R_i^2 \\log(2/\\delta)}{2}}\\ .\n\\tag{5}\\]\n\n\nApplication\nThe following example is provided by Ji (2022, Lecture 13):\nSuppose you are given a coin that lands on heads with probability \\(\\mu\\). Every time you flip it, you get an outcome \\(X_i\\) - a Bernoulli distribution with mean \\(\\mu\\). After \\(n\\) flips, you compute the sample mean \\(\\hat{\\mu}_n \\coloneqq \\frac{1}{n} \\sum_{i=1}^n X_i\\). By Hoeffding’s inequality, with probability at least \\(1-\\delta\\), \\[\n| \\hat{\\mu}_n - \\mu | \\leq \\sqrt{\\frac{\\log(2/\\delta)}{2 n}}\\ .\n\\tag{6}\\]\nYou may recall that this is consistent with the central limit theorem, which states that the sampling distribution of sample means is \\(\\sigma_{\\bar{X}} = \\sigma / \\sqrt{n}\\)."
  },
  {
    "objectID": "posts/1-probability.html#multi-armed-bandits",
    "href": "posts/1-probability.html#multi-armed-bandits",
    "title": "Probability theory and random variables",
    "section": "Multi-armed bandits",
    "text": "Multi-armed bandits\nI first encountered these through a graduate course I took with Dr. Bo Ji (2022); the algorithms and analyses below (as well as the concentration bounds discussion above) are all taken from lecture notes from that class, although they are originally based on a book by Slivkins (2022).\n\n\n\nImage credit: Wikipedia\n\n\nThe image above shows a row of slot machines. They are otherwise known as one-armed bandits, as they steal money from you and have one arm you must pull to spun the wheel (although in the ones seen above it seems to have been replaced by buttons).\nGeneralizing a bit, let’s imagine that we instead have \\(K\\) such arms. Maybe they belong to the same, now much more tricky bandit. Or, maybe they model the possibility of playing different games in the same casino. In any case, at every time point \\(t\\) we must choose one such action \\(a_t \\in [K]\\) to play, only learning information about its underlying distribution through the rewards we receive.\nThere are various possible types of the bandit setting, but here we will only consider stochastic (as opposed to adversarial) losses and multi-armed (as opposed to linear or gaussian process) bandits.\n\n\n\\begin{algorithm} \\caption{Stochastic MAB (Framework)} \\begin{algorithmic} \\Require{$K$ and $T$ (both known), unknown reward distributions $\\mathcal{D}_a$.} \\For{$t = 1, 2, \\ldots$} \\State Choose an action $a_t \\in [K]$. \\State Suffer loss $z_t[a_t]$ and also only observe $z_t[a_t]$. \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\nHere, \\(z_t\\) is a random vector sampled from the unknown distributions, i.e., \\(z_t[a] \\sim \\mathcal{D}_a\\). The defining feature separating this problem from Online Convex Optimization is being only provided one value \\(z_t[a_t]\\) corresponding to the played arm \\(a_t\\), a.k.a., bandit feedback.\nOur goal in this setting is to design an algorithm fitting this framework that minimizes the following quantity, known as regret \\[\n\\mathcal{R}_T = \\sum_{t=1}^T z_t[a_t] - \\min_i \\sum_{t=1}^T z_t[i]\\ .\n\\tag{7}\\]\nSuch a definition makes regret a useful metric in many situations, even when experiencing persintent penalties or unreasonable adversaries. In particular, it evaluates performance against the best possible fixed action in hindsight - a value that will compensate for the above scenarios. Note that the definition given in Equation 7 makes \\(\\mathcal{R}_T\\) a random variable, meaning that in practice we aim to bound \\(\\mathbb{E}[\\mathcal{R}_T]\\), the expected regret.2\nWe may use the terms loss and reward interchangeably, as they can be thought of as negatives of each other. This is again owed to the definition of regret in Equation 7, by which minimizing losses is the same as maximizing rewards.\n\nExplore-then-exploit\nA central issue in the multi-armed bandit problem is that of exploration versus exploitation - whether to prioritize investigating the yet unknown, or to take advantage of information already learned. It is not a coincidence this problem appears both here and in reinforcement learning - both fall within the realm of online learning (when decisions and information are processed sequentially), the only thing separating them is the addition of states.\nWe start with a simple algorithm that does the two stages separately (Ji 2022, Lecture 14).\n\n\n\\begin{algorithm} \\caption{Explore-then-exploit} \\begin{algorithmic} \\Require{$K$ and $T$ (both known), unknown reward distributions $\\mathcal{D}_a$.} \\For{$t = 1, 2, \\ldots$} \\State Explore phase: try each arm $N$ times. \\State Exploit phase: determine $\\tilde{a}$ with the highest average reward; then play $\\tilde{a}$ in all remaining rounds. \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\nWe wish to bound the probability of getting good approximations on all arms, which we start by bounding each arm inidvidually. That is, we want to bound the deviation of its true mean \\(\\mu(a)\\) from its approximation \\(\\hat{\\mu}(a)\\) after that arm has been pulled \\(N\\) times. Let \\(\\beta_N = \\sqrt{\\frac{2 \\log T}{N}}\\), by Hoeffding’s inequality, \\[\n\\mathrm{P}(|\\hat{\\mu}(a) - \\mu(a)| \\leq \\beta_N) \\geq 1 - 2 / T^4\\ .\n\\tag{8}\\]\nWe define a good event \\(\\mathcal{E}\\) as the above being true for all arms, meaning that by the union bound \\[\n\\mathrm{P}(\\mathcal{E}) = 1 - \\mathrm{P}(\\bar{\\mathcal{E}}) \\geq\n1 - \\sum_{a=1}^K \\mathrm{P}(|\\bar{\\mu}(a) - \\mu(a)| \\geq \\beta_N) \\geq 1 - 2 K / T^4\\ .\n\\tag{9}\\]\nEven if this good event occurs, we may still choose a suboptimal arm \\(\\bar{a}\\) in the exploit phase. In that case, the estimate errors for both \\(\\bar{a}\\) and the optimal arm \\(a^*\\) are bounded by \\(\\beta_N\\). As such, \\(\\mu(a^*) - \\mu(\\bar{a}) \\leq 2 \\beta_N\\). Adding with a max regret of \\(1\\) for \\(KN\\) round of exploration, the total incurred regret in the good case is at most \\[\n\\mathcal{R}_T \\leq K N + (T - KN) \\cdot 2 \\beta_N \\leq KN + 2 T \\beta_N = KN + 2 T \\sqrt{\\frac{2 \\log T}{N}}\\ .\n\\tag{10}\\]\nThis also lets us derive a formula for \\(N\\) which we do by setting the resulting terms equal. In particular, \\(N = 2 \\sqrt[3]{T^2 \\log T / K^2}\\), which leads to a good case regret of \\(\\mathrm{O}(T^{2/3} (K \\log T)^{1/3})\\). The bad event happens with probability at most \\(2 K / T^4\\), which when multiplied by a max per-round regret of \\(1\\) gets absorbed by the big O.\nFor implementations of this and the following algorithms, we make a function play that returns an arm a, and a function feedback that records the resulting reward r. Both functions are supplied with other contextual parameters for convenience.\n\n\nCode for Explore-then-exploit\nfrom math import log\nfrom numpy import argmax\n\nclass ExploreExploit:\n    def __init__(self, K, T):\n        self.K = K\n        self.N = round(2 * (T**2 * log(T) / K**2)**(1/3))\n        self.totals = [0] * K\n    \n    def __str__(self):\n        return \"Explore-then-exploit\"\n\n    def play(self, t):\n        '''Choose an arm to play.'''\n        if t &lt;= self.K*self.N:\n            # Play each arm N times\n            return t % self.K\n        else:\n            # Play best arm\n            return self.best\n    \n    def feedback(self, t, a, r):\n        '''Receive a reward.'''\n        if t &lt;= self.K*self.N:\n            # Record reward\n            self.totals[a] += r\n        if t == self.K*self.N:\n            # Compute best arm\n            self.best = argmax(self.totals)\n\n\n\n\nEpsilon-greedy\nInstead of doing exploration all at once, we may spread it out over all rounds, making it less frequent over time (Ji 2022, Lecture 14). You’ll see this strategy used quite frequently in reinforcement learning, as it is pretty simple to implement.\n\n\n\\begin{algorithm} \\caption{Epsilon-greedy} \\begin{algorithmic} \\Require{$K$ and $T$ (both known), unknown reward distributions $\\mathcal{D}_a$.} \\For{$t = 1, 2, \\ldots$} \\State Toss a coin with success rate $\\epsilon_t$. \\If{success} \\State Explore: choose an arm uniformly at random. \\Else \\State Exploit: choose an arm with the highest average reward so far. \\EndIf \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\nWith \\(\\epsilon_t = t^{-1/3} (K \\log t)^{1/3}\\), Epsilon-greedy achieves the same regret bound, \\(\\mathrm{O}(t^{2/3} (K \\log t)^{1/3})\\). We won’t show it here for brevity, but it uses the same techniques as shown, similarly relying on Hoeffding’s inequality. Note in addition that Algorithm 3 does not rely on \\(T\\), which makes it an anytime algorithm.\n\n\nCode for Epsilon-greedy\nfrom math import log\nfrom random import random, randrange\nfrom numpy import argmax, divide, errstate, inf, nan\n\nclass EpsilonGreedy:\n    def __init__(self, K, _):\n        self.K = K\n        self.totals = [0] * K\n        self.counts = [0] * K\n    \n    def __str__(self):\n        return \"Epsilon-greedy\"\n\n    def _eps(self, t):\n        return (self.K*log(t)/t)**(1/3)\n\n    def _best(self):\n        with errstate(divide='ignore', invalid='ignore'):\n            avg = divide(self.totals, self.counts)\n        avg[avg == inf] = -inf\n        avg[avg == nan] = -inf\n        return argmax(avg)\n\n    def play(self, t):\n        '''Choose an arm to play.'''\n        if random() &lt; self._eps(t):\n            # Play random arm\n            return randrange(self.K)\n        else:\n            # Play best arm\n            return self._best()\n    \n    def feedback(self, t, a, r):\n        '''Receive a reward.'''\n        self.totals[a] += r\n        self.counts[a] += 1\n\n\n\n\nSuccessive elimination\nWe can further incorporate the concept of means and confidence bounds into the algorithm by considering their values at any \\(t \\in [T]\\). In particular, \\[\n\\mathrm{P} \\left( |\\hat{\\mu}_t(a) - \\mu(a)| \\leq \\beta_t(a) \\right) \\geq \\sqrt{\\frac{\\log (2 K T / \\delta)}{2 N_t (a)}}\\ .\n\\tag{11}\\]\nThis feels very similar to Hoeffding’s inequality, but actually requires a version generalized to martingales, known as the Azuma-Hoeffding inequality.\nFor convenience, we define (Ji 2022, Lecture 14) \\[\n\\begin{align}\n    \\mathrm{UCB}_t(a) &= \\hat{\\mu}_t(a) + \\beta_t(a)\\ ,\\\\\n    \\mathrm{LCB}_t(a) &= \\hat{\\mu}_t(a) - \\beta_t(a)\\ .\n\\end{align}\n\\tag{12}\\]\n\n\n\\begin{algorithm} \\caption{Successive Elimination} \\begin{algorithmic} \\Require{$K$ and $T$ (both known), unknown reward distributions $\\mathcal{D}_a$.} \\State Initialize active arm set $\\mathcal{A}_1 = [K]$. \\For{$p = 1, 2, \\ldots$} \\State Play each arm in $\\mathcal{A}_p$ once. \\State Let $t$ be the time at the end of the current phase $p$. \\State $\\mathcal{A}_{p+1} = \\{ a \\in \\mathcal{A}_p : \\mathrm{UCB}_t (a) \\geq \\max_{a' \\in \\mathcal{A}_p} \\mathrm{LCB}_t (a) \\}$ \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\nWe define the good event as before, and assume that it holds. If we at some point eliminate \\(a^*\\), it means \\(\\mathrm{UCB}_t(a^*) &lt; \\mathrm{LCB}_t(a')\\) for some other arm \\(a'\\), implying \\(\\mu(a^*) &lt; \\mu(a')\\) and thus a contradiction. Then, assume there is some arm \\(a \\in \\mathcal{A}_{p+1}\\) during phase \\(p\\) ending at time \\(t\\) such that \\(\\mu(a^*) - \\mu(a) &gt; 4 \\beta_t(a)\\). Then, \\[\n\\mathrm{LCB}_t(a) \\leq \\mu(a) + 2 \\beta_t(a) &lt; \\mu(a^*) - 2 \\beta_t(a^*) \\leq \\mathrm{UCB}_t(a^*)\\ ,\n\\tag{13}\\] implying that this arm should have been eliminated. Thus, the regret contributed by arm \\(a\\) is \\[\n\\mathcal{R}_{a,t} \\leq 4 N_t(a) \\sqrt{\\frac{\\log(2 K T / \\delta)}{2 N_t(a)}} = \\mathrm{O}(\\sqrt{N_t(a) \\log(K T / \\delta)})\\ .\n\\tag{14}\\]\nUsing the Cauchy-Schwarz inequality, \\(\\sum_a \\sqrt{N_t(a)} \\leq \\sqrt{\\sum_a N_t (a) \\cdot \\sum_a 1} = \\sqrt{t K}\\), meaning \\[\n\\mathcal{R}_t = \\sum_{a \\in [K]} \\mathcal{R}_{a,t} = \\mathrm{O}(\\sqrt{\\log(K T / \\delta)}) \\cdot \\sum_a \\sqrt{N_t(a)} \\leq \\mathrm{O}(\\sqrt{K t \\log (K T / \\delta)})\\ .\n\\tag{15}\\]\nWe thus have that \\(\\mathcal{R}_T = \\mathrm{O}(\\sqrt{K T \\log (K T / \\delta)})\\).\nNote that the above bounds have a configurable value \\(\\delta \\in (0, 1]\\), which makes them take effect with probability at least \\(1 - \\delta\\). In practice, we need to set delta to a ridiculously high value for any arms to actually become eliminated within a reasonable amount of time.\n\n\nCode for Successive elimination\nfrom random import random, randrange\nfrom numpy import argmax, divide, errstate, inf, nan, log, sqrt\nimport numpy as np\n\nclass SuccessiveElimination:\n    def __init__(self, K, T, delta=1500):\n        self.num = log(2 * K * T / delta) / 2\n        assert self.num &gt; 0, \"delta too large\"\n        self.totals = [0] * K\n        self.counts = [0] * K\n        self.A = list(range(K))\n        self.idx = 0\n    \n    def __str__(self):\n        return \"Successive elimination\"\n\n    def _mu(self):\n        with errstate(divide='ignore', invalid='ignore'):\n            mu = divide(self.totals, self.counts)\n        mu[mu == inf] = 0\n        mu[mu == nan] = 0\n        return mu\n\n    def _beta(self):\n        with errstate(divide='ignore'):\n            beta = sqrt(divide(self.num, self.counts))\n        return beta\n\n    def play(self, t):\n        '''Choose an arm to play.'''\n        if self.idx == len(self.A):\n            # Update arm set A\n            u, b = self._mu(), self._beta()\n            lcb, ucb = u-b, u+b\n            self.A = [a for a in self.A if ucb[a] &gt;= np.max(lcb)]\n            self.idx = 0\n        # Play each arm in A\n        a = self.A[self.idx]\n        self.idx += 1\n        return a\n    \n    def feedback(self, t, a, r):\n        '''Receive a reward.'''\n        self.totals[a] += r\n        self.counts[a] += 1\n\n\n\n\nUCB\nSince the event of eliminating an arm may be quite rare, we may instead just sample the arm with the highest \\(\\mathrm{UCB}_t(a)\\). This strategy demonstrates a principle known as optimism in the face of uncertainty Ji (2022, Lecture 15) - that is, both arms that have high sample mean and arms with fewer samples will have a larger UCB and thus more likely to get chosen.\n\n\n\\begin{algorithm} \\caption{UCB} \\begin{algorithmic} \\Require{$K$ and $T$ (both known), unknown reward distributions $\\mathcal{D}_a$.} \\For{$t = 1, 2, \\ldots$} \\State $a_t = \\arg\\max_{a \\in [K]} \\mathrm{UCB}_t (a)$ \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\nSimilar to before, \\(\\mu(a^*) - \\mu(a_t) \\leq 2 \\beta_t (a_t)\\) in the good event. We thus have \\[\n\\mathcal{R}_T = \\sum_{t=1}^T r_t \\leq c \\sqrt{\\log (2 K T / \\delta)} \\sum_{t=1}^T \\sqrt{\\frac{1}{N_t(a_t)}}\\ ,\n\\tag{16}\\]\nwhere the latter summation is bounded by \\[\n\\sum_{t=1}^T \\sqrt{\\frac{1}{N_t(a_t)}} = \\sum_{a=1}^K \\sum_{m=1}^{N_T(a)} \\sqrt{1/m} \\leq c' \\sum_{a=1}^K \\sqrt{N_T(a)} = \\mathrm{O}(\\sqrt{K T})\\ .\n\\tag{17}\\]\nThus, the regret of UCB is \\(\\mathrm{O}(\\sqrt{K T \\log (K T / \\delta)})\\).\n\n\nCode for UCB\nfrom random import random, randrange\nfrom numpy import argmax, divide, errstate, inf, nan, log, sqrt\nimport numpy as np\n\nclass UCB:\n    def __init__(self, K, T, delta=1):\n        self.num = log(2 * K * T / delta) / 2\n        assert self.num &gt; 0, \"delta too large\"\n        self.totals = [0] * K\n        self.counts = [0] * K\n    \n    def __str__(self):\n        return \"UCB\"\n\n    def _mu(self):\n        with errstate(divide='ignore', invalid='ignore'):\n            mu = divide(self.totals, self.counts)\n        mu[mu == inf] = 0\n        mu[mu == nan] = 0\n        return mu\n\n    def _beta(self):\n        with errstate(divide='ignore'):\n            beta = sqrt(divide(self.num, self.counts))\n        return beta\n\n    def play(self, t):\n        '''Choose an arm to play.'''\n        ucb = self._mu() + self._beta()\n        return argmax(ucb)\n    \n    def feedback(self, t, a, r):\n        '''Receive a reward.'''\n        self.totals[a] += r\n        self.counts[a] += 1\n\n\n\n\nSummary\nHere is a compilation of the derived bounds so far:\n\nSummary of theoretic regret bounds.\n\n\nAlgorithm\nRegret\nAnytime\n\n\n\n\nExplore-then-exploit\n\\(\\mathrm{O}(T^{2/3} (K \\log T)^{1/3})\\)\n❌\n\n\nEpsilon-greedy\n\\(\\mathrm{O}(t^{2/3} (K \\log t)^{1/3})\\)\n✅\n\n\nSuccessive elimination\n\\(\\mathrm{O}(\\sqrt{K T \\log (K T / \\delta)})\\)\n❌\n\n\nUCB\n\\(\\mathrm{O}(\\sqrt{K T \\log (K T / \\delta)})\\)\n❌\n\n\n\nOn a final note, the above bounds are all problem-independent. We can instead derive problem-dependent bounds, such as \\[\n\\mathcal{R}_T = \\sum_{a : \\Delta(a) &gt; 0} \\mathrm{O} \\left( \\frac{\\log(K T / \\delta)}{\\delta(a)} \\right)\\ ,\n\\tag{18}\\]\nfor UCB, where \\(\\Delta(a) \\coloneqq \\mu(a^*) - \\mu(a)\\).\n\n\nSimulation\nNow, it’s time to put the algorithms to the test!\n\nTechnical note\nBut first, a small note on the simulation details. As the rewards (and thus performance) are highly stochastic, we will need to run many trials to get an accurate result. Even when results are recorded once every F steps, we still run into situations where the times for storing and churning data may compete with those of running the algorithms. We have several possible approaches:\n\nStore them all: pre-allocate a huge array, and process it once afterwards. This understandably causes slowdowns.\nAggregate last: i.e., iterate over algorithms and timesteps first, then trials. This may seem like a perfect solution at first, but it runs into a caveat of the algorithms storing internal parameters. That is, algorithms from some runs will have more information about certain arms than others, so we can’t just re-sample the same instance many times. The way to address this would involve storing an array of algorithms proportional to the number of trials, which is again inefficient.\nOnline aggregation: iterate over trials first, and store a statistic for each algorithm/timestep. The way we avoid having a separate dimension for trials is via an approximation algorithm that aggregates statistics as they come in - Welford’s online algorithm.\n\nWe will use the latter approach, which is what lets us compute 1000 samples for each data point.\n\n\nSeed random number generators\nfrom random import seed as py_seed\nfrom numpy.random import seed as np_seed\npy_seed(5805)\nnp_seed(5805)\n\n\nIn particular, we will simulate having to choose between \\(K=2\\) arms:\n\nA uniform distribution on \\([0,1]\\), i.e., mean \\(0.5\\) and standard deviation \\(\\sqrt{1/12} \\approx 0.29\\).\nA normal distribution with mean \\(0.6\\) and standard deviation \\(0.2\\).\n\nThe above statistics indicate that arm 2 is more optimal, but arm 1 has a wider distribution of rewards.\n\n\nSimulation code\nfrom random import random\nfrom numpy.random import normal\nfrom numpy import zeros, sqrt\n\n# Simulation parameters\nT = 1000    # Number of rounds\nF = 10      # Logging frequency\nN = 1000    # Number of trials\n\n# Arms to explore\nmeans = [0.5, 0.6]\nbest = max(means)\narms = [random, lambda: normal(means[1], 0.2, 1).item()]\nK = len(arms)\n\n# Algorithms to use\ncs = [ExploreExploit, EpsilonGreedy, SuccessiveElimination, UCB]\nA = len(cs)\n\n# Evaluate algorithms\nx = zeros((T//F+1, A))\ns = zeros((T//F+1, A))\nfor n in range(1, N+1):\n    for i, c in enumerate(cs):\n        alg = c(K, T)\n        rgt = 0\n        for t in range(1, T+1):\n            a = alg.play(t)\n            r = arms[a]()\n            alg.feedback(t, a, r)\n            rgt += best - r\n            if t % F == 0:\n                # Welford's online algorithm\n                xn1 = x[t//F, i]\n                x[t//F, i] = ((n-1)*xn1 + rgt) / n\n                s[t//F, i] += (rgt - xn1) * (rgt - x[t//F, i])\ns = 0.2*sqrt(s / (N-1))\n\n\nWe now plot the mean regret \\(\\mathcal{R}_t\\) of each algorithm, as well as the \\(\\pm 0.2\\) sample standard deviation margins.\n\n\nCode for plotting regret\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nt = np.arange(1, T+2, F)\nplt.plot(t, x)\nfor i in range(A):\n    plt.fill_between(t, x[:,i]-s[:,i], x[:,i]+s[:,i], alpha=.15)\nplt.xlabel(\"Timestep\")\nplt.ylabel(\"Regret\")\nplt.legend([c(K, T) for c in cs]);\n\n\n\n\n\nFigure 1: Performance of MAB algorithms\n\n\n\n\nThe above performance roughly matches what we should expect. Most notably, Explore-then-exploit accumulates linear regret until a certain time, where it sharply switches to exploitation and plateus. The other algorithms gently bend down towards it instead, allowing them to achieve lower final regrets.\nNote that the above shouldn’t be used as a definitive guide for how the algorithms compare, as it’s specific to the problem instance and algorithm parameters. In particular, the performance of Successive elimination is highly dependent on its artificial parameter \\(\\delta = 1500\\), as it determines how soon the trend starts bending toward a plateu. Still, it may give an indication as to why the UCB algorithm and its varians are so common in practice."
  },
  {
    "objectID": "posts/1-probability.html#footnotes",
    "href": "posts/1-probability.html#footnotes",
    "title": "Probability theory and random variables",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese \\(X_i\\) can either be thought of as fixed values or i.i.d. random variables. In coming examples, we will use the latter.↩︎\nIn practice, it often ends up more convenient to bound the pseudo-regret \\(\\bar{\\mathcal{R}}_T\\), which swaps the order of \\(\\mathbb{E}\\) and \\(\\min\\), turning it into a \\(\\max\\). By Jensen’s inequality, \\(\\mathbb{E}[\\mathcal{R}_T] \\geq \\bar{\\mathcal{R}}_T\\).↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Alice Didenkova",
    "section": "",
    "text": "A graduate student at Virginia Tech."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Alice Didenkova",
    "section": "Education",
    "text": "Education\nVirginia Tech | Blacksburg, VA\nPh.D. Candidate in Computer Science | 2022 - present\nVirginia Tech | Blacksburg, VA\nB.S. in Computer Science, Mathematics | 2019 - 2022"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Alice Didenkova",
    "section": "Experience",
    "text": "Experience\nFujitsu Research of America | Research Intern in Quantum Computing | Summer 2023\nCS 3214: Computer Systems | Teaching Assistant | 2021 - present"
  },
  {
    "objectID": "about.html#topics",
    "href": "about.html#topics",
    "title": "Alice Didenkova",
    "section": "Topics",
    "text": "Topics\nMy interests and current research topics include:\n\n⚛️ Quantum computing\n🤖 Reinforcement learning\n\nDisclaimer: this blog has been created to fulfill the Final Project requirements of CS 5805: Machine Learning."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Probability theory and random variables\n\n\nConcentration bounds and multi-armed bandits\n\n\n\n\nProbability theory\n\n\nRandom variables\n\n\nConcentration bounds\n\n\nMulti-armed bandits\n\n\nOnline learning\n\n\n\n\nIn which we introduce random variables and describe the theory behind bounding them via concentration inequalities. We then introduce several multi-armed bandit algorithms to see how such inequalities may be used to evaluate their performance.\n\n\n\n\n\n\nDec 1, 2023\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\nSparse subspace clustering\n\n\n\n\nClustering\n\n\nUnsupervised learning\n\n\nEigenvalues\n\n\nConvex optimization\n\n\n\n\nIn which we overview common tasks and models for clustering. Includes an exploration of spectral clustering and sparse subspace clustering, and an implementation of the latter.\n\n\n\n\n\n\nDec 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nLinear and nonlinear regression\n\n\n\n\n\n\n\nTODO\n\n\n\n\nTODO\n\n\n\n\n\n\nDec 3, 2023\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\nTODO\n\n\n\n\nTODO\n\n\n\n\n\n\nDec 4, 2023\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly/outlier detection\n\n\nChangepoint detection\n\n\n\n\nTODO\n\n\n\n\nTODO\n\n\n\n\n\n\nDec 5, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2-clustering.html",
    "href": "posts/2-clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Falling within the realm of unsupervised learning, this task deals with unlabeled but adequately distinct data. While both it and classification involve modeling such differences, clustering in particular starts with no prior informaiton about data labels. In cases where partial label information is known, the task is instead considered to be semi-supervised.\nThe common goal of clustering is to separate data into clusters - groups such that data within them is similar and between them is different. What this means precisely depends on the measure of similarity used; for raw Euclidean distances it would mean that clusters are far apart literally, whereas for feature embedding spaces they would be apart semantically/conceptually.\nClustering serves a variety of purposes:\n\nUnderstanding: separating data into distinct groups may reveal patterns and highlight important features. Additionally, clustering may be viewed as a form of divide-and-conquer where the clusters may be easier to analyze in isolation.\nPreprocessing: similar to the above, the benefit of being able to analyze data separately can be extended to programmatic solutions. That is, cluster assignment can be used to split data and train models separately for each.\nAnomaly detection: the above would additionally get rid of outliers as they wouldn’t belong to any cluster. This makes clustering applicable to anomaly detection as it can identify points that are distant from all other data.\n\nClustering in itself isn’t a method but a task. A plethora of algorithms exist to perform it, thanks to various relevant considerations about the type of clustered data:\n\nShape: while we may typically think of clusters as circular-like objects, they may instead be stretched, spread out, stretched out, bent, etc. For the stretched out type, it additionally becomes important whether data exhibits linear relationships or otherwise lies in nonlinear manifolds.\nData type: the same considerations from other tasks in machine learning apply: is the data numerical or categorical, are any transforms applicable, is some of the data images/audio/text/etc?\nDimensionality: for more complex data types, preprocessing or embedding may be in order. High-dimensional data in general poses a challenge due to the curse of dimensionality, a phenomenon where regular measures of distance break down and lose meaning. In some circumstances it thus becomes crucial to use methods specifically designed around high-dimensional data.\n\n\n\nTo illustrate the former consideration particularly, we can take a look at some distinct sets of artificial data and the performances of various clustering algorithms. Here’s a pretty nice visualization of some common algorithms:\n\n\nClustering scatterplot matrix code by sklearn\nimport time\nimport warnings\nfrom itertools import cycle, islice\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import cluster, datasets, mixture\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.preprocessing import StandardScaler\n\nfrom ssc import linear, SparseSubspaceClustering\n\n# ============\n# Generate datasets. We choose the size big enough to see the scalability\n# of the algorithms, but not too big to avoid too long running times\n# ============\nn_samples = 200\nseed = 5805\nnoisy_circles = datasets.make_circles(\n    n_samples=n_samples, factor=0.5, noise=0.05, random_state=seed\n)\nnoisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=seed)\nblobs = datasets.make_blobs(n_samples=n_samples, random_state=seed)\nrng = np.random.RandomState(seed)\nno_structure = rng.rand(n_samples, 2), None\n\n# Anisotropicly distributed data\nrandom_state = 5805\nX, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\ntransformation = [[0.6, -0.6], [-0.4, 0.8]]\nX_aniso = np.dot(X, transformation)\naniso = (X_aniso, y)\n\n# blobs with varied variances\nvaried = datasets.make_blobs(\n    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n)\n\n# ============\n# Set up cluster parameters\n# ============\nplt.figure(figsize=(10.5, 6.5))\nplt.subplots_adjust(\n    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.01, hspace=0.01\n)\n\nplot_num = 1\n\ndefault_base = {\n    \"quantile\": 0.3,\n    \"eps\": 0.3,\n    \"damping\": 0.9,\n    \"preference\": -200,\n    \"n_neighbors\": 3,\n    \"n_clusters\": 3,\n    \"min_samples\": 7,\n    \"xi\": 0.05,\n    \"min_cluster_size\": 0.1,\n    \"allow_single_cluster\": True,\n    \"hdbscan_min_cluster_size\": 15,\n    \"hdbscan_min_samples\": 3,\n    \"random_state\": 42,\n}\n\ndatasets = [\n    (\n        noisy_circles,\n        {\n            \"damping\": 0.77,\n            \"preference\": -240,\n            \"quantile\": 0.2,\n            \"n_clusters\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.08,\n        },\n    ),\n    (\n        noisy_moons,\n        {\n            \"damping\": 0.75,\n            \"preference\": -220,\n            \"n_clusters\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.1,\n        },\n    ),\n    (\n        varied,\n        {\n            \"eps\": 0.18,\n            \"n_neighbors\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.01,\n            \"min_cluster_size\": 0.2,\n        },\n    ),\n    (\n        aniso,\n        {\n            \"eps\": 0.15,\n            \"n_neighbors\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.1,\n            \"min_cluster_size\": 0.2,\n        },\n    ),\n    (blobs, {\"min_samples\": 7, \"xi\": 0.1, \"min_cluster_size\": 0.2}),\n    (linear, {}),\n    (no_structure, {}),\n]\n\nfor i_dataset, (dataset, algo_params) in enumerate(datasets):\n    # update parameters with dataset-specific values\n    params = default_base.copy()\n    params.update(algo_params)\n\n    X, y = dataset\n\n    # normalize dataset for easier parameter selection\n    X = StandardScaler().fit_transform(X)\n\n    # estimate bandwidth for mean shift\n    bandwidth = cluster.estimate_bandwidth(X, quantile=params[\"quantile\"])\n\n    # connectivity matrix for structured Ward\n    connectivity = kneighbors_graph(\n        X, n_neighbors=params[\"n_neighbors\"], include_self=False\n    )\n    # make connectivity symmetric\n    connectivity = 0.5 * (connectivity + connectivity.T)\n\n    # ============\n    # Create cluster objects\n    # ============\n    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n    two_means = cluster.MiniBatchKMeans(\n        n_clusters=params[\"n_clusters\"],\n        n_init=\"auto\",\n        random_state=params[\"random_state\"],\n    )\n    ward = cluster.AgglomerativeClustering(\n        n_clusters=params[\"n_clusters\"], linkage=\"ward\", connectivity=connectivity\n    )\n    spectral = cluster.SpectralClustering(\n        n_clusters=params[\"n_clusters\"],\n        eigen_solver=\"arpack\",\n        affinity=\"nearest_neighbors\",\n        random_state=params[\"random_state\"],\n    )\n    dbscan = cluster.DBSCAN(eps=params[\"eps\"])\n    hdbscan = cluster.HDBSCAN(\n        min_samples=params[\"hdbscan_min_samples\"],\n        min_cluster_size=params[\"hdbscan_min_cluster_size\"],\n        allow_single_cluster=params[\"allow_single_cluster\"],\n    )\n    optics = cluster.OPTICS(\n        min_samples=params[\"min_samples\"],\n        xi=params[\"xi\"],\n        min_cluster_size=params[\"min_cluster_size\"],\n    )\n    affinity_propagation = cluster.AffinityPropagation(\n        damping=params[\"damping\"],\n        preference=params[\"preference\"],\n        random_state=params[\"random_state\"],\n    )\n    average_linkage = cluster.AgglomerativeClustering(\n        linkage=\"average\",\n        metric=\"cityblock\",\n        n_clusters=params[\"n_clusters\"],\n        connectivity=connectivity,\n    )\n    birch = cluster.Birch(n_clusters=params[\"n_clusters\"])\n    gmm = mixture.GaussianMixture(\n        n_components=params[\"n_clusters\"],\n        covariance_type=\"full\",\n        random_state=params[\"random_state\"],\n    )\n    ssc = SparseSubspaceClustering(\n        n_clusters=params[\"n_clusters\"],\n    )\n\n    clustering_algorithms = (\n        (\"MiniBatch\\nKMeans\", two_means),\n        (\"Affinity\\nPropagation\", affinity_propagation),\n        (\"MeanShift\", ms),\n        (\"Spectral\\nClustering\", spectral),\n        (\"Ward\", ward),\n        (\"Agglomerative\\nClustering\", average_linkage),\n        (\"DBSCAN\", dbscan),\n        (\"HDBSCAN\", hdbscan),\n        (\"OPTICS\", optics),\n        (\"BIRCH\", birch),\n        (\"Gaussian\\nMixture\", gmm),\n        (\"Sparse\\nSubspace\\nClustering\", ssc),\n    )\n\n    for name, algorithm in clustering_algorithms:\n        t0 = time.time()\n\n        # catch warnings related to kneighbors_graph, subspace clustering\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"the number of connected components of the \"\n                + \"connectivity matrix is [0-9]{1,2}\"\n                + \" &gt; 1. Completing it to avoid stopping the tree early.\",\n                category=UserWarning,\n            )\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"Graph is not fully connected, spectral embedding\"\n                + \" may not work as expected.\",\n                category=UserWarning,\n            )\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"Solution may be inaccurate. Try another solver,\"\n                + \" adjusting the solver settings, or solve with\"\n                + \" verbose=True for more information.\",\n                category=UserWarning,\n            )\n            algorithm.fit(X)\n\n        t1 = time.time()\n        if hasattr(algorithm, \"labels_\"):\n            y_pred = algorithm.labels_.astype(int)\n        else:\n            y_pred = algorithm.predict(X)\n\n        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            plt.title(name, size=9)\n\n        colors = np.array(\n            list(\n                islice(\n                    cycle(\n                        [\n                            \"#377eb8\",\n                            \"#ff7f00\",\n                            \"#4daf4a\",\n                            \"#f781bf\",\n                            \"#a65628\",\n                            \"#984ea3\",\n                            \"#999999\",\n                            \"#e41a1c\",\n                            \"#dede00\",\n                        ]\n                    ),\n                    int(max(y_pred) + 1),\n                )\n            )\n        )\n        # add black color for outliers (if any)\n        colors = np.append(colors, [\"#000000\"])\n        plt.scatter(X[:, 0], X[:, 1], s=5, color=colors[y_pred])\n\n        plt.xlim(-2.5, 2.5)\n        plt.ylim(-2.5, 2.5)\n        plt.xticks(())\n        plt.yticks(())\n        plt.text(\n            0.99,\n            0.01,\n            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n            transform=plt.gca().transAxes,\n            size=7.5,\n            horizontalalignment=\"right\",\n        )\n        plot_num += 1\n\nplt.show()\n\n\n\n\n\nFigure 1: A comparison of various clustering methods.\n\n\n\n\nWhoa, that’s a lot of them! If you’ve seen this image before and are particularly observant, you may notice that I threw in an extra algorithm (rightmost column) and a dataset (second-bottom row) - more on those later.\nEach of the above algorithms is deserving of a post of its own. Here, I will just briefly focus on one, which happens to be the only showcased algorithm that properly clusters the top 5 datasets:\n\n\n\nYes, spectral as in spectrum - i.e., eigenvalues. I found out about this algorithm back when I was taking intro to linear algebra, and it really stuck with me precisely because it gives a pretty neat interpretation of eigenvalues (which at the time I had a really hard time exemplifying).\nWe start with a similarity matrix \\(S\\), where the \\(ij\\)-th value denotes the similarity of data points indexed \\(i\\) and \\(j\\). What constitutes as being similar may vary; the example in Figure 1 simply uses Euclidean distance. We then use \\(S\\) to construct a similarity graph, a weighted graph with weights \\(w_{i,j} = s_{i,j}\\) if they are past a certain threshold and \\(0\\) (no edge) otherwise. This graph’s weighted adjacency matrix \\(W\\) is essentially \\(S\\) with all sub-threshold values zeroed out. Assuming data from \\(k\\) sufficiently distinct clusters is ordered by cluster, the adjacency matrix may look something like \\[\nW = \\begin{bmatrix} W_1 & & & \\\\ & W_2 & & \\\\ & & \\ddots & \\\\ & & & W_k \\end{bmatrix}\\ ,\n\\tag{1}\\]\nwhich you may recognize as a block diagonal matrix.\nSo, what role do eigenvalues play here? Turns out, if \\(v\\) is an eigenvalue of \\(W_1\\), then the same \\(v\\) padded by a bunch of zeros at the bottom is an eigenvalue of \\(W\\). You can verify this via Equation 1; \\(W_1 v = v \\lambda\\) necessarily implies \\(W \\begin{bmatrix} v \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} v \\\\ 0 \\end{bmatrix} \\lambda\\). The same applies for eigenvectors of any cluster, they just need to placed appropriately to act on their cluster’s similarity block \\(W_i\\). As such, the eigenvalues of \\(W\\) will be grouped according to cluster, having zero values in all other locations. Finding these eigenvalues would then reveal the clusters.\nWell… sort of. In reality, we still have some problems: the eigenvalues can have small/zero entries, and finding all \\(n\\) eigenvalues is just too difficult. Instead, we define \\(D\\) to be the diagonal degree matrix, its diagonal entries being the sum of the corresponding rows/columns of \\(W\\). We then obtain the graph Laplacian \\(L = D - W\\), another diagonal block matrix with an additional property that its rows/columns sum to zero. This makes it so multiplying each block \\(L_i\\) by a vector with all entries being \\(1\\) produces the zero vector, making it an eigenvector of \\(L_i\\) with a zero eigenvalue. Additionally, the only vectors satisfying this property are just scalar multiples of that identity vector. As such, the Laplacian \\(L\\) will have its zero eigenspace spanned by precisely \\(k\\) such identity vectors. This adresses both of our previous problems, making it so we only have to find \\(k\\) out of \\(n\\) eigenvectors. Using inverse iteration will obtain these eigenvectors only, and do so very efficiently. Putting the eigenvectors together as a matrix would then make its rows indicators for which cluster a particular point belongs to. In case there is still any ambiguity, the rows can be grouped together using a simpler clustering algorithm like \\(k\\)-means. For more details about spectral clustering, see Luxburg (2007).\n\n\nA pretty cool interpretation of this Laplacian comes from thinking of the nodes in the similarity graph as being connected by springs - the higher the weight between two nodes, the stronger their connecting spring. What happens if we move a single node by some amount? The nodes connected to it would be pulled in the same direction according to the strengths of the springs that connect them to the displaced node, the latter being pulled back with the sum of those strengths. Note that this force is precisely described by the (negative of) the column of \\(L\\) corresponding to the displaced node. If \\(x\\) is the vector representing the displacement of each node, the corresponding force (in the other direction) is given by \\(Lx\\). In the case where the displacement is an eigenvector of \\(L\\), we have \\[\n\\sum F = - L v = - \\lambda v\\ ,\n\\tag{2}\\]\nwhich is precisely Hooke’s law. Thus given an initial displacement \\(v\\), the graph will undergo simple harmonic motion with frequency \\(\\lambda^2\\). In other words, the eigenpairs \\((\\lambda, v)\\) of \\(L\\) actually describe the resonant frequencies \\(\\lambda^2\\) and corresponding vibration modes of its graph. It is for this reason that eigenvalues are crucial to analyzing mechanical resonance - where the graph represents connected parts of a physical object. This could be applied to, for intance, designing buildings that are resistant to strong winds and earthquakes.\nIn our case we saw that the similarity graph had some zero eigenvalues. This corresponds to zero frequency - i.e., once we displace the nodes of a single cluster, it experiences no force from the others and just stays in the same place.\n\n\n\n\nSo, that’s all well and good when similarity relates to Euclidean distance - but as noted before, this becomes less true for high-dimensional data. Instead, data may lie in linear subspaces, such as when representing the same scene illuminated from different angles, or objects moving at different speeds Elhamifar and Vidal (2013). A simplified image of what such data could look like is seen in Figure 2 below.\n\n\n\nFigure 2: Three linear subspaces. Figure by Elhamifar and Vidal (2013).\n\n\nThe point where the lines and plane intersect is the origin. You may note that this matches the mathematical definiton of linear subspaces, i.e., non-empty and closed under addition and scalar multiplication. Alternatively, we may pick any number of vectors and have their span be a linear subspaces. Any point in the subspaces would then be a linear combination of such vectors, which are themselves points in the subspace.\nYou may note that the new dataset is somewhat like a flattened version of Figure 2, with the lines \\(\\mathcal{S}_2\\) and \\(\\mathcal{S}_3\\) being the diagonals making up the ‘X’ shape, and the plane \\(\\mathcal{S}_1\\) being the remaining points in the background square.\n# The 'X'-looking dataset\n\nlinear = np.concatenate((\n        rng.rand(n_samples//2, 2),                              # □ Square\n        rng.rand(n_samples//4, 2) * 0.05\n            + np.repeat(rng.rand(n_samples//4, 1), 2, axis=1),  # / Diagonal\n        rng.rand(n_samples//4, 2) * 0.05\n            + np.repeat(rng.rand(n_samples//4, 1), 2, axis=1)   # \\ Diagonal\n            * [1, -1] + [0, 1]\n    ), axis=0) - [0.5, 0.5], None\nOur goal is then to separate the two diagonals and the square."
  },
  {
    "objectID": "posts/2-clustering.html#clustering",
    "href": "posts/2-clustering.html#clustering",
    "title": "Clustering",
    "section": "",
    "text": "Falling within the realm of unsupervised learning, this task deals with unlabeled but adequately distinct data. While both it and classification involve modeling such differences, clustering in particular starts with no prior informaiton about data labels. In cases where partial label information is known, the task is instead considered to be semi-supervised.\nThe common goal of clustering is to separate data into clusters - groups such that data within them is similar and between them is different. What this means precisely depends on the measure of similarity used; for raw Euclidean distances it would mean that clusters are far apart literally, whereas for feature embedding spaces they would be apart semantically/conceptually.\nClustering serves a variety of purposes:\n\nUnderstanding: separating data into distinct groups may reveal patterns and highlight important features. Additionally, clustering may be viewed as a form of divide-and-conquer where the clusters may be easier to analyze in isolation.\nPreprocessing: similar to the above, the benefit of being able to analyze data separately can be extended to programmatic solutions. That is, cluster assignment can be used to split data and train models separately for each.\nAnomaly detection: the above would additionally get rid of outliers as they wouldn’t belong to any cluster. This makes clustering applicable to anomaly detection as it can identify points that are distant from all other data.\n\nClustering in itself isn’t a method but a task. A plethora of algorithms exist to perform it, thanks to various relevant considerations about the type of clustered data:\n\nShape: while we may typically think of clusters as circular-like objects, they may instead be stretched, spread out, stretched out, bent, etc. For the stretched out type, it additionally becomes important whether data exhibits linear relationships or otherwise lies in nonlinear manifolds.\nData type: the same considerations from other tasks in machine learning apply: is the data numerical or categorical, are any transforms applicable, is some of the data images/audio/text/etc?\nDimensionality: for more complex data types, preprocessing or embedding may be in order. High-dimensional data in general poses a challenge due to the curse of dimensionality, a phenomenon where regular measures of distance break down and lose meaning. In some circumstances it thus becomes crucial to use methods specifically designed around high-dimensional data.\n\n\n\nTo illustrate the former consideration particularly, we can take a look at some distinct sets of artificial data and the performances of various clustering algorithms. Here’s a pretty nice visualization of some common algorithms:\n\n\nClustering scatterplot matrix code by sklearn\nimport time\nimport warnings\nfrom itertools import cycle, islice\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import cluster, datasets, mixture\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.preprocessing import StandardScaler\n\nfrom ssc import linear, SparseSubspaceClustering\n\n# ============\n# Generate datasets. We choose the size big enough to see the scalability\n# of the algorithms, but not too big to avoid too long running times\n# ============\nn_samples = 200\nseed = 5805\nnoisy_circles = datasets.make_circles(\n    n_samples=n_samples, factor=0.5, noise=0.05, random_state=seed\n)\nnoisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=seed)\nblobs = datasets.make_blobs(n_samples=n_samples, random_state=seed)\nrng = np.random.RandomState(seed)\nno_structure = rng.rand(n_samples, 2), None\n\n# Anisotropicly distributed data\nrandom_state = 5805\nX, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\ntransformation = [[0.6, -0.6], [-0.4, 0.8]]\nX_aniso = np.dot(X, transformation)\naniso = (X_aniso, y)\n\n# blobs with varied variances\nvaried = datasets.make_blobs(\n    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n)\n\n# ============\n# Set up cluster parameters\n# ============\nplt.figure(figsize=(10.5, 6.5))\nplt.subplots_adjust(\n    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.01, hspace=0.01\n)\n\nplot_num = 1\n\ndefault_base = {\n    \"quantile\": 0.3,\n    \"eps\": 0.3,\n    \"damping\": 0.9,\n    \"preference\": -200,\n    \"n_neighbors\": 3,\n    \"n_clusters\": 3,\n    \"min_samples\": 7,\n    \"xi\": 0.05,\n    \"min_cluster_size\": 0.1,\n    \"allow_single_cluster\": True,\n    \"hdbscan_min_cluster_size\": 15,\n    \"hdbscan_min_samples\": 3,\n    \"random_state\": 42,\n}\n\ndatasets = [\n    (\n        noisy_circles,\n        {\n            \"damping\": 0.77,\n            \"preference\": -240,\n            \"quantile\": 0.2,\n            \"n_clusters\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.08,\n        },\n    ),\n    (\n        noisy_moons,\n        {\n            \"damping\": 0.75,\n            \"preference\": -220,\n            \"n_clusters\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.1,\n        },\n    ),\n    (\n        varied,\n        {\n            \"eps\": 0.18,\n            \"n_neighbors\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.01,\n            \"min_cluster_size\": 0.2,\n        },\n    ),\n    (\n        aniso,\n        {\n            \"eps\": 0.15,\n            \"n_neighbors\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.1,\n            \"min_cluster_size\": 0.2,\n        },\n    ),\n    (blobs, {\"min_samples\": 7, \"xi\": 0.1, \"min_cluster_size\": 0.2}),\n    (linear, {}),\n    (no_structure, {}),\n]\n\nfor i_dataset, (dataset, algo_params) in enumerate(datasets):\n    # update parameters with dataset-specific values\n    params = default_base.copy()\n    params.update(algo_params)\n\n    X, y = dataset\n\n    # normalize dataset for easier parameter selection\n    X = StandardScaler().fit_transform(X)\n\n    # estimate bandwidth for mean shift\n    bandwidth = cluster.estimate_bandwidth(X, quantile=params[\"quantile\"])\n\n    # connectivity matrix for structured Ward\n    connectivity = kneighbors_graph(\n        X, n_neighbors=params[\"n_neighbors\"], include_self=False\n    )\n    # make connectivity symmetric\n    connectivity = 0.5 * (connectivity + connectivity.T)\n\n    # ============\n    # Create cluster objects\n    # ============\n    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n    two_means = cluster.MiniBatchKMeans(\n        n_clusters=params[\"n_clusters\"],\n        n_init=\"auto\",\n        random_state=params[\"random_state\"],\n    )\n    ward = cluster.AgglomerativeClustering(\n        n_clusters=params[\"n_clusters\"], linkage=\"ward\", connectivity=connectivity\n    )\n    spectral = cluster.SpectralClustering(\n        n_clusters=params[\"n_clusters\"],\n        eigen_solver=\"arpack\",\n        affinity=\"nearest_neighbors\",\n        random_state=params[\"random_state\"],\n    )\n    dbscan = cluster.DBSCAN(eps=params[\"eps\"])\n    hdbscan = cluster.HDBSCAN(\n        min_samples=params[\"hdbscan_min_samples\"],\n        min_cluster_size=params[\"hdbscan_min_cluster_size\"],\n        allow_single_cluster=params[\"allow_single_cluster\"],\n    )\n    optics = cluster.OPTICS(\n        min_samples=params[\"min_samples\"],\n        xi=params[\"xi\"],\n        min_cluster_size=params[\"min_cluster_size\"],\n    )\n    affinity_propagation = cluster.AffinityPropagation(\n        damping=params[\"damping\"],\n        preference=params[\"preference\"],\n        random_state=params[\"random_state\"],\n    )\n    average_linkage = cluster.AgglomerativeClustering(\n        linkage=\"average\",\n        metric=\"cityblock\",\n        n_clusters=params[\"n_clusters\"],\n        connectivity=connectivity,\n    )\n    birch = cluster.Birch(n_clusters=params[\"n_clusters\"])\n    gmm = mixture.GaussianMixture(\n        n_components=params[\"n_clusters\"],\n        covariance_type=\"full\",\n        random_state=params[\"random_state\"],\n    )\n    ssc = SparseSubspaceClustering(\n        n_clusters=params[\"n_clusters\"],\n    )\n\n    clustering_algorithms = (\n        (\"MiniBatch\\nKMeans\", two_means),\n        (\"Affinity\\nPropagation\", affinity_propagation),\n        (\"MeanShift\", ms),\n        (\"Spectral\\nClustering\", spectral),\n        (\"Ward\", ward),\n        (\"Agglomerative\\nClustering\", average_linkage),\n        (\"DBSCAN\", dbscan),\n        (\"HDBSCAN\", hdbscan),\n        (\"OPTICS\", optics),\n        (\"BIRCH\", birch),\n        (\"Gaussian\\nMixture\", gmm),\n        (\"Sparse\\nSubspace\\nClustering\", ssc),\n    )\n\n    for name, algorithm in clustering_algorithms:\n        t0 = time.time()\n\n        # catch warnings related to kneighbors_graph, subspace clustering\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"the number of connected components of the \"\n                + \"connectivity matrix is [0-9]{1,2}\"\n                + \" &gt; 1. Completing it to avoid stopping the tree early.\",\n                category=UserWarning,\n            )\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"Graph is not fully connected, spectral embedding\"\n                + \" may not work as expected.\",\n                category=UserWarning,\n            )\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"Solution may be inaccurate. Try another solver,\"\n                + \" adjusting the solver settings, or solve with\"\n                + \" verbose=True for more information.\",\n                category=UserWarning,\n            )\n            algorithm.fit(X)\n\n        t1 = time.time()\n        if hasattr(algorithm, \"labels_\"):\n            y_pred = algorithm.labels_.astype(int)\n        else:\n            y_pred = algorithm.predict(X)\n\n        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            plt.title(name, size=9)\n\n        colors = np.array(\n            list(\n                islice(\n                    cycle(\n                        [\n                            \"#377eb8\",\n                            \"#ff7f00\",\n                            \"#4daf4a\",\n                            \"#f781bf\",\n                            \"#a65628\",\n                            \"#984ea3\",\n                            \"#999999\",\n                            \"#e41a1c\",\n                            \"#dede00\",\n                        ]\n                    ),\n                    int(max(y_pred) + 1),\n                )\n            )\n        )\n        # add black color for outliers (if any)\n        colors = np.append(colors, [\"#000000\"])\n        plt.scatter(X[:, 0], X[:, 1], s=5, color=colors[y_pred])\n\n        plt.xlim(-2.5, 2.5)\n        plt.ylim(-2.5, 2.5)\n        plt.xticks(())\n        plt.yticks(())\n        plt.text(\n            0.99,\n            0.01,\n            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n            transform=plt.gca().transAxes,\n            size=7.5,\n            horizontalalignment=\"right\",\n        )\n        plot_num += 1\n\nplt.show()\n\n\n\n\n\nFigure 1: A comparison of various clustering methods.\n\n\n\n\nWhoa, that’s a lot of them! If you’ve seen this image before and are particularly observant, you may notice that I threw in an extra algorithm (rightmost column) and a dataset (second-bottom row) - more on those later.\nEach of the above algorithms is deserving of a post of its own. Here, I will just briefly focus on one, which happens to be the only showcased algorithm that properly clusters the top 5 datasets:\n\n\n\nYes, spectral as in spectrum - i.e., eigenvalues. I found out about this algorithm back when I was taking intro to linear algebra, and it really stuck with me precisely because it gives a pretty neat interpretation of eigenvalues (which at the time I had a really hard time exemplifying).\nWe start with a similarity matrix \\(S\\), where the \\(ij\\)-th value denotes the similarity of data points indexed \\(i\\) and \\(j\\). What constitutes as being similar may vary; the example in Figure 1 simply uses Euclidean distance. We then use \\(S\\) to construct a similarity graph, a weighted graph with weights \\(w_{i,j} = s_{i,j}\\) if they are past a certain threshold and \\(0\\) (no edge) otherwise. This graph’s weighted adjacency matrix \\(W\\) is essentially \\(S\\) with all sub-threshold values zeroed out. Assuming data from \\(k\\) sufficiently distinct clusters is ordered by cluster, the adjacency matrix may look something like \\[\nW = \\begin{bmatrix} W_1 & & & \\\\ & W_2 & & \\\\ & & \\ddots & \\\\ & & & W_k \\end{bmatrix}\\ ,\n\\tag{1}\\]\nwhich you may recognize as a block diagonal matrix.\nSo, what role do eigenvalues play here? Turns out, if \\(v\\) is an eigenvalue of \\(W_1\\), then the same \\(v\\) padded by a bunch of zeros at the bottom is an eigenvalue of \\(W\\). You can verify this via Equation 1; \\(W_1 v = v \\lambda\\) necessarily implies \\(W \\begin{bmatrix} v \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} v \\\\ 0 \\end{bmatrix} \\lambda\\). The same applies for eigenvectors of any cluster, they just need to placed appropriately to act on their cluster’s similarity block \\(W_i\\). As such, the eigenvalues of \\(W\\) will be grouped according to cluster, having zero values in all other locations. Finding these eigenvalues would then reveal the clusters.\nWell… sort of. In reality, we still have some problems: the eigenvalues can have small/zero entries, and finding all \\(n\\) eigenvalues is just too difficult. Instead, we define \\(D\\) to be the diagonal degree matrix, its diagonal entries being the sum of the corresponding rows/columns of \\(W\\). We then obtain the graph Laplacian \\(L = D - W\\), another diagonal block matrix with an additional property that its rows/columns sum to zero. This makes it so multiplying each block \\(L_i\\) by a vector with all entries being \\(1\\) produces the zero vector, making it an eigenvector of \\(L_i\\) with a zero eigenvalue. Additionally, the only vectors satisfying this property are just scalar multiples of that identity vector. As such, the Laplacian \\(L\\) will have its zero eigenspace spanned by precisely \\(k\\) such identity vectors. This adresses both of our previous problems, making it so we only have to find \\(k\\) out of \\(n\\) eigenvectors. Using inverse iteration will obtain these eigenvectors only, and do so very efficiently. Putting the eigenvectors together as a matrix would then make its rows indicators for which cluster a particular point belongs to. In case there is still any ambiguity, the rows can be grouped together using a simpler clustering algorithm like \\(k\\)-means. For more details about spectral clustering, see Luxburg (2007).\n\n\nA pretty cool interpretation of this Laplacian comes from thinking of the nodes in the similarity graph as being connected by springs - the higher the weight between two nodes, the stronger their connecting spring. What happens if we move a single node by some amount? The nodes connected to it would be pulled in the same direction according to the strengths of the springs that connect them to the displaced node, the latter being pulled back with the sum of those strengths. Note that this force is precisely described by the (negative of) the column of \\(L\\) corresponding to the displaced node. If \\(x\\) is the vector representing the displacement of each node, the corresponding force (in the other direction) is given by \\(Lx\\). In the case where the displacement is an eigenvector of \\(L\\), we have \\[\n\\sum F = - L v = - \\lambda v\\ ,\n\\tag{2}\\]\nwhich is precisely Hooke’s law. Thus given an initial displacement \\(v\\), the graph will undergo simple harmonic motion with frequency \\(\\lambda^2\\). In other words, the eigenpairs \\((\\lambda, v)\\) of \\(L\\) actually describe the resonant frequencies \\(\\lambda^2\\) and corresponding vibration modes of its graph. It is for this reason that eigenvalues are crucial to analyzing mechanical resonance - where the graph represents connected parts of a physical object. This could be applied to, for intance, designing buildings that are resistant to strong winds and earthquakes.\nIn our case we saw that the similarity graph had some zero eigenvalues. This corresponds to zero frequency - i.e., once we displace the nodes of a single cluster, it experiences no force from the others and just stays in the same place.\n\n\n\n\nSo, that’s all well and good when similarity relates to Euclidean distance - but as noted before, this becomes less true for high-dimensional data. Instead, data may lie in linear subspaces, such as when representing the same scene illuminated from different angles, or objects moving at different speeds Elhamifar and Vidal (2013). A simplified image of what such data could look like is seen in Figure 2 below.\n\n\n\nFigure 2: Three linear subspaces. Figure by Elhamifar and Vidal (2013).\n\n\nThe point where the lines and plane intersect is the origin. You may note that this matches the mathematical definiton of linear subspaces, i.e., non-empty and closed under addition and scalar multiplication. Alternatively, we may pick any number of vectors and have their span be a linear subspaces. Any point in the subspaces would then be a linear combination of such vectors, which are themselves points in the subspace.\nYou may note that the new dataset is somewhat like a flattened version of Figure 2, with the lines \\(\\mathcal{S}_2\\) and \\(\\mathcal{S}_3\\) being the diagonals making up the ‘X’ shape, and the plane \\(\\mathcal{S}_1\\) being the remaining points in the background square.\n# The 'X'-looking dataset\n\nlinear = np.concatenate((\n        rng.rand(n_samples//2, 2),                              # □ Square\n        rng.rand(n_samples//4, 2) * 0.05\n            + np.repeat(rng.rand(n_samples//4, 1), 2, axis=1),  # / Diagonal\n        rng.rand(n_samples//4, 2) * 0.05\n            + np.repeat(rng.rand(n_samples//4, 1), 2, axis=1)   # \\ Diagonal\n            * [1, -1] + [0, 1]\n    ), axis=0) - [0.5, 0.5], None\nOur goal is then to separate the two diagonals and the square."
  },
  {
    "objectID": "posts/2-clustering.html#sparse-subspace-clustering",
    "href": "posts/2-clustering.html#sparse-subspace-clustering",
    "title": "Clustering",
    "section": "Sparse Subspace Clustering",
    "text": "Sparse Subspace Clustering\nTo this end, we will analyze an algorithm based on spectral clustering, proposed by Elhamifar and Vidal (2013). This algorithm relies on the self-expressiveness property described above, i.e., that points in a subspace can be described by a linear combination of other points from the same subspace. Specifically, let \\(Y = \\begin{bmatrix} y_1 & y_2 & \\dots & y_n \\end{bmatrix}\\) be a \\(d\\)-by-\\(n\\) matrix of data. Then, with every data point \\(y_i\\) we can associate a coefficient vector \\(c_i\\) such that \\[\ny_i = Y c_i\\ ,\\qquad\nc_{ii} = 0\\ .\n\\tag{3}\\]\nAs the above has infinitely many solutions, we will additionally \\(\\text{minimize} ||c_i||_q\\). Using smaller \\(q\\) will make more coefficients zero, and for \\(q=1\\) will generally only have nonzero coefficients for points within the same subspace.1\nLetting \\(C = \\begin{bmatrix} c_1 & c_2 & \\dots & c_n \\end{bmatrix}\\), we can rewrite the problem in matrix form as \\[\n\\begin{align}\n    \\text{minimize} \\quad& ||C||_1 \\\\\n    \\text{such that} \\quad& Y = Y C\\ ,\\\\\n    & \\mathrm{diag}(C) = 0\\ .\n\\end{align}\n\\tag{4}\\]\nNote that the objective is convex and the constraints affine, so this can be solved efficiently using\n\nConvex optimization\nA framework for problems with certain nice properties, convex optimization can be applied to a multitude of tasks in various fields. While I first encountered it in exactly this context of sparse subspace clustering, I now use a variant of it (semidefinite programming) all the time for quantum-related stuff.\nLet’s solve an instance of Equation 4 in practice to see why this matrix \\(C\\) could be useful. First, let’s construct a simplified version of the dataset shown above, containing fewer points and only the diagonals.\n\n\nGenerate a simpler dataset\nn = 20\nY = np.concatenate((\n    rng.rand(2, n//2) * 0.01\n        + np.repeat(rng.rand(1, n//2), 2, axis=0),  # / Diagonal\n    rng.rand(2, n//2) * 0.01\n        + np.repeat(rng.rand(1, n//2), 2, axis=0)   # \\ Diagonal\n        * [[1], [-1]] + [[0], [1]]\n), axis=1) - [[0.5], [0.5]]\n\n\nTo construct and solve the convex optimization problem, we will use the python package cvxpy. We essentially just need to declare \\(C\\) as a variable, then specify our objective and constraints. The rest is handled by the solver - everything from verifying that the problem is convex to returning its solution.\n\n\nSolve and visualize a convex optimization problem\nfrom cvxpy import Variable, Minimize, norm, diag, Problem, ECOS\nimport plotly.express as px\n\n# Variable matrix\nC = Variable((n, n))\n\n# Minimize ||C||_1\nobjective = Minimize(norm(C,1))\n\n# Subject to Y = YC, diag(C) = 0\nconstraints = [Y == Y @ C, diag(C) == 0]\nprob = Problem(objective, constraints)\nprob.solve(solver=ECOS)\n\n# Normalize coefficient matrix\nC = C.value / np.max(np.abs(C.value), axis=0)\npx.imshow(C)\n\n\n\n\n                                                \nFigure 3: A heatmap of the obtained coefficient matrix \\(C\\).\n\n\n\n?@fig-C above shows what the (normalized) solution looks like - you may note that it’s a block diagonal matrix much like Equation 1. This is due to the self-expressiveness property; points from a subspace will only have nonzero coefficients corresponding to other points from the same subspace. As such, we may essentially use this \\(C\\) matrix as similarity for (spectral clustering)[#spectral-clustering].\nMore specifically, you may note that there are two rows with quite large coefficients whereas the others are quite small. This comes from minimizing the norm of \\(C\\), leading to us to prioritize points with larger distances from the origin. In the case of one-dimensional subspaces like this, we end up picking the farthest point from the origin and expressing the remaining points as scalar multiples of it. To better approximate the block structures and make the similarity graph undirected, we thus symmetrize the matrix via \\(W = |C| + |C|^\\top\\).\n\n\nImplementation\nAs one last detail, Elhamifar and Vidal (2013) proposes a modified version of Equation 4 that deals with noise \\[\n\\begin{align}\n    \\text{minimize} \\quad& ||C||_1 + \\lambda_e ||E||_1 + \\frac{\\lambda_z}{2} ||Z||_F^2 \\\\\n    \\text{such that} \\quad& Y = Y C + E + Z\\ ,\\\\\n    & \\mathrm{diag}(C) = 0\\ .\n\\end{align}\n\\tag{5}\\]\nThe \\(E\\) matrix is meant to account for outliers and the \\(Z\\) matrix for other noise. The reference material suggests setting \\(\\lambda_e = \\alpha_e / \\mu_e\\) and \\(\\lambda_e = \\alpha_e / \\mu_e\\) with \\[\n\\mu_e = \\min_i \\max_{j \\neq i} ||y_j||_1\\ ,\\qquad\n\\mu_z = \\min_i \\max_{j \\neq i} |y_i^\\top y_j|\\ .\n\\tag{6}\\]\nPutting everything together, the algorithm looks like this:\ndef fit(self, Y):\n    Y = Y.T\n    d, n = Y.shape\n    assert d &lt; n\n\n    ## 1. Solve sparse optimization problem\n\n    # Starting form, equation (5)\n    C = Variable((n, n))\n    objective = norm(C,1)\n    constraint = Y @ C\n\n    # Account for outliers\n    if self.use_E:\n        mu_e = np.partition(np.sum(np.abs(Y), axis=0), -2)[-2]\n        l_e = 20 / mu_e\n        E = Variable((d, n))\n        objective += l_e * norm(E,1)\n        constraint += E\n\n    # Account for noise\n    if self.use_Z:\n        G = np.abs(Y.T @ Y)\n        mu_z = np.min(np.max(G - np.diag(np.diag(G)), axis=1))\n        l_z = 800 / mu_z\n        Z = Variable((d, n))\n        objective += l_z/2 * norm(Z,\"fro\")**2\n        constraint += Z\n\n    constraints = [Y == constraint, diag(C) == 0]\n    prob = Problem(Minimize(objective), constraints)\n    try:\n        prob.solve(solver=ECOS)\n\n        ## 2. Normalize the columns of C\n        C = C.value / np.max(np.abs(C.value), axis=0)\n\n        ## 3. Form the weights of a similarity graph\n        W = np.abs(C)\n        W = W + W.T\n\n        ## 4. Apply spectral clustering on the graph\n        self.labels_ = SpectralClustering(\n                n_clusters=self.n_clusters,\n                affinity='precomputed'\n            ).fit_predict(W)\n    except SolverError:\n        self.labels_ = np.zeros(n)"
  },
  {
    "objectID": "posts/2-clustering.html#footnotes",
    "href": "posts/2-clustering.html#footnotes",
    "title": "Clustering",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that we could also solve for \\(q=0\\), but this is an \\(\\mathsf{NP}\\)-hard problem.↩︎"
  },
  {
    "objectID": "posts/4-classification.html",
    "href": "posts/4-classification.html",
    "title": "Classification",
    "section": "",
    "text": "Image credit: sklearn"
  },
  {
    "objectID": "posts/4-classification.html#simple-dataset",
    "href": "posts/4-classification.html#simple-dataset",
    "title": "Classification",
    "section": "Simple dataset",
    "text": "Simple dataset\n\nIris\n\n\n\nImage credit: Sebastian Raschka\n\n\n\n\n\nImage credit: Gaurav Chauhan\n\n\n\n\n2D scatterplot of iris\nimport plotly.express as px\nfrom sklearn.preprocessing import MinMaxScaler\n\niris = px.data.iris().drop('species_id', axis=1)\nfig = px.scatter(iris, x=\"sepal_width\", y=\"sepal_length\", size=\"petal_length\",\n    color=\"species\", marginal_y=\"violin\", marginal_x=\"violin\",\n    template=\"simple_white\", hover_data=['petal_width'])\nop = MinMaxScaler(feature_range=(0.5, 1.0)).fit_transform(\n    iris['petal_width'].values[:, None])\nfig.update_traces(marker=dict(opacity=op), selector=dict(mode='markers'))\nfig.show();\n\n\n\n                                                \n\n\n\n\n3D scatterplot of iris\nimport plotly.express as px\n\nfig = px.scatter_3d(iris, x=\"sepal_width\", y=\"sepal_length\", z=\"petal_width\",\n    size=\"petal_length\", color=\"species\", template=\"simple_white\")\nfig.show();\n\n\n\n                                                \n\n\n\n\nNaive Bayes\n\n\nNaive bayes classifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, precision_score\n\n# Fit and predict\nX, y = iris.drop('species', axis=1), iris['species']\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.5, random_state=5805)\nnb = GaussianNB().fit(X_train, y_train)\niris['predict'] = nb.predict(X)\ny_pred = nb.predict(X_test)\nprint(f\"Accuracy:  {accuracy_score(y_test, y_pred):.4f}\\n\"\n      f\"Precision: {precision_score(y_test, y_pred, average='macro'):.4f}\")\n\n\nAccuracy:  0.9467\nPrecision: 0.9444\n\n\n\n\nPerformance\n\n\nCode for plotting confusion matrix\nimport seaborn as sn\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\n\nnames = iris['species'].unique()\nconf = confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(conf, index=names, columns=names)\nplt.figure(figsize = (4,3))\nsn.heatmap(df_cm, annot=True, fmt='d');\n\n\n\n\n\n\n\n\n\n\n\n2D scatterplot of iris & predictions\nimport numpy as np\n\nfix = {\"circle\": \"#1F77B4\", \"diamond\": \"#FF7F0E\", \"square\": \"#2CA02C\"}\nfig = px.scatter(iris, x=\"sepal_width\", y=\"sepal_length\", size=\"petal_length\",\n    color=\"species\", symbol=\"predict\",\n    template=\"simple_white\", hover_data=['petal_width'])\nfig.update_traces(marker=dict(opacity=op), selector=dict(mode='markers'))\nfor symbol, color in fix.items():\n    fig.update_traces(marker=dict(line=dict(width=3, color=color),\n                      symbol=\"circle\"),\n                      selector=dict(mode='markers', marker_symbol=symbol))\nfig.show();\n\n\n\n                                                \n\n\n\n\n3D scatterplot of iris & predictions\nimport plotly.express as px\n\nfig = px.scatter_3d(iris, x=\"sepal_width\", y=\"sepal_length\", z=\"petal_width\",\n    size=\"petal_length\", color=\"species\", symbol=\"predict\",\n    template=\"simple_white\")\nfor symbol, color in fix.items():\n    fig.update_traces(marker=dict(line=dict(color=color)),\n                      selector=dict(mode='markers', marker_symbol=symbol))\nfig.show();"
  },
  {
    "objectID": "posts/4-classification.html#complex-dataset",
    "href": "posts/4-classification.html#complex-dataset",
    "title": "Classification",
    "section": "Complex dataset",
    "text": "Complex dataset\n\nMNIST\n\n\n\nImage credit: Orhan G. Yalçın\n\n\n\n\nNeural network\npytorch tutorial\n\n\nImport pytorch\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\nimport torchvision.transforms as transforms\nimport torch.optim as optim\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n\nUsing device: cpu\n\n\n\n\nLoad and preprocess data\ntransform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5), (0.5))\n    ])\nbatch_size = 4\n\ntrainset = MNIST(root='./data', train=True, download=True, transform=transform)\ntrainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n\ntestset = MNIST(root='./data', train=False, download=True, transform=transform)\ntestloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n\nnames = list(range(10))\n\n\n\n\nCreate a neural network\nclass NeuralNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 3, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(3, 6, 5)\n        self.fc1 = nn.Linear(6 * 4 * 4, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        return x\n\nmodel = NeuralNet()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\n\n\n\nTrain a neural network\n# Do one pass over the dataset\nfor i, data in enumerate(trainloader, 0):\n    inputs, labels = data\n\n    # Resent the gradients\n    optimizer.zero_grad()\n\n    # Forward/backward pass\n    outputs = model(inputs)\n    loss = criterion(outputs, labels)\n    loss.backward()\n    optimizer.step()\n\n# Evaluate network\ny_test, y_pred = [], []\nwith torch.no_grad():\n    for data in testloader:\n        inputs, labels = data\n        outputs = model(inputs)\n        _, pred = torch.max(outputs.data, 1)\n        y_test.extend(labels)\n        y_pred.extend(pred)\nprint(f\"Accuracy:  {accuracy_score(y_test, y_pred):.4f}\\n\"\n      f\"Precision: {precision_score(y_test, y_pred, average='macro'):.4f}\")\n\n\nAccuracy:  0.8889\nPrecision: 0.8931\n\n\n\n\nPerformance\n\n\nCode for plotting confusion matrix\nconf = confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(conf, index=names, columns=names)\nplt.figure(figsize = (8.5,7))\nsn.heatmap(df_cm, annot=True, fmt='d');\n\n\n\n\n\n\n\n\n\n\n\nShow test samples with mistakes\nimport torchvision\nimport numpy as np\n\ndef imshow(img):\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n\n# Get samples where mistakes were made\ndef wrong(dataloader):\n    for i, (images, labels) in enumerate(testloader):\n        for j, (image, label) in enumerate(zip(images, labels)):\n            label, pred = label.item(), y_pred[batch_size*i+j].item()\n            if pred != label:\n                yield (image, label, pred)\n\n\n# Plot several examples\nloader = wrong(testloader)\nfor _ in range(4):\n    image, label, pred = next(loader)\n    img = np.transpose(image, (1, 2, 0))\n    plt.imshow(img, cmap='Greys')\n    plt.title(f\"Actual: {label}\\nPredicted: {pred}\", size=42)\n    plt.xticks([],[])\n    plt.yticks([],[])\n    plt.show()"
  }
]