[
  {
    "objectID": "posts/5-anomaly.html",
    "href": "posts/5-anomaly.html",
    "title": "Anomaly/outlier detection",
    "section": "",
    "text": "Image credit: Ajitesh Kumar\n\n\n\n\nRun linear regression on uncorrelated data\nfrom pandas import DataFrame\nfrom numpy.random import normal, random, seed\nimport plotly.express as px\nseed(5805)\n\nn = 100\ndf = DataFrame({'x': normal(size=n-1) + random(n-1), 'y': normal(size=n-1)})\nfig = px.scatter(df, x='x', y='y', trendline='ols').show();\n\n\n\n\n                                                \nFigure 1: Uncorrelated data\n\n\n\n\n\nRun linear regression on data with an outlier\ndf.loc[n-1] = {'x': 10, 'y': 10, 'outlier': True}\nfig = px.scatter(df, x='x', y='y', trendline='ols')\nfig.update_traces(marker=dict(color=[\"#636efa\"] * (n-1) + [\"#EF553B\"]))\nfig.show();\n\n\n\n\n                                                \nFigure 2: Regular OLS\n\n\n\n\\[\nL_\\delta(a) = \\begin{cases}\n    \\frac{1}{2} a^2 & \\text{if } |a| \\leq \\delta\\ ,\\\\\n    \\delta(|a| - \\frac{1}{2} \\delta) & \\text{otherwise}\\ .\n\\end{cases}\n\\tag{1}\\]\n\n\n\nImage credit: Wikipedia\n\n\n\n\nRun linear regression with Huber loss\nfrom numpy import linspace\nimport plotly.graph_objects as go\nfrom sklearn.linear_model import HuberRegressor\n\nfig = px.scatter(df, x='x', y='y')\nfig.update_traces(marker=dict(color=[\"#636efa\"] * (n-1) + [\"#EF553B\"]))\nX = df['x'].values.reshape(-1, 1)\nx_range = linspace(X.min(), X.max(), 100).reshape(-1, 1)\ny = df['y']\n\nmodel = HuberRegressor(epsilon=1)\nmodel.fit(X, y)\ny_fit = model.predict(x_range)\nfig.add_traces(go.Scatter(x=x_range.squeeze(), y=y_fit,\n    showlegend=False, line=dict(color=\"#636efa\"))).show();\n\n\n\n\n                                                \nFigure 3: OLS with Huber loss"
  },
  {
    "objectID": "posts/5-anomaly.html#outliers",
    "href": "posts/5-anomaly.html#outliers",
    "title": "Anomaly/outlier detection",
    "section": "",
    "text": "Image credit: Ajitesh Kumar\n\n\n\n\nRun linear regression on uncorrelated data\nfrom pandas import DataFrame\nfrom numpy.random import normal, random, seed\nimport plotly.express as px\nseed(5805)\n\nn = 100\ndf = DataFrame({'x': normal(size=n-1) + random(n-1), 'y': normal(size=n-1)})\nfig = px.scatter(df, x='x', y='y', trendline='ols').show();\n\n\n\n\n                                                \nFigure 1: Uncorrelated data\n\n\n\n\n\nRun linear regression on data with an outlier\ndf.loc[n-1] = {'x': 10, 'y': 10, 'outlier': True}\nfig = px.scatter(df, x='x', y='y', trendline='ols')\nfig.update_traces(marker=dict(color=[\"#636efa\"] * (n-1) + [\"#EF553B\"]))\nfig.show();\n\n\n\n\n                                                \nFigure 2: Regular OLS\n\n\n\n\\[\nL_\\delta(a) = \\begin{cases}\n    \\frac{1}{2} a^2 & \\text{if } |a| \\leq \\delta\\ ,\\\\\n    \\delta(|a| - \\frac{1}{2} \\delta) & \\text{otherwise}\\ .\n\\end{cases}\n\\tag{1}\\]\n\n\n\nImage credit: Wikipedia\n\n\n\n\nRun linear regression with Huber loss\nfrom numpy import linspace\nimport plotly.graph_objects as go\nfrom sklearn.linear_model import HuberRegressor\n\nfig = px.scatter(df, x='x', y='y')\nfig.update_traces(marker=dict(color=[\"#636efa\"] * (n-1) + [\"#EF553B\"]))\nX = df['x'].values.reshape(-1, 1)\nx_range = linspace(X.min(), X.max(), 100).reshape(-1, 1)\ny = df['y']\n\nmodel = HuberRegressor(epsilon=1)\nmodel.fit(X, y)\ny_fit = model.predict(x_range)\nfig.add_traces(go.Scatter(x=x_range.squeeze(), y=y_fit,\n    showlegend=False, line=dict(color=\"#636efa\"))).show();\n\n\n\n\n                                                \nFigure 3: OLS with Huber loss"
  },
  {
    "objectID": "posts/5-anomaly.html#changepoint-detection",
    "href": "posts/5-anomaly.html#changepoint-detection",
    "title": "Anomaly/outlier detection",
    "section": "Changepoint detection",
    "text": "Changepoint detection\n\n\nLoad and display the Nile dataset\nimport statsmodels.api as sm\nimport plotly.express as px\n\n# Time series data\nnile = sm.datasets.get_rdataset(\"Nile\").data\nfig = px.line(nile, x=\"time\", y=\"value\")\n\n# Ground truth change point\nfig.add_vline(x=1899, line_width=2, line_dash=\"dash\", line_color=\"black\")\n\n\n\n\n                                                \nFigure 4: Yearly volume of the Nile river at Aswan. A dam was built in 1898.\n\n\n\n\nCUSUM\n@page_continuous_1954\nMATLAB has a built-in cusum function\n\n\nA visual example of changepoint detection\nfrom numpy import concatenate, cumsum, mean\n\n# Simulate coin flipping\nn, t = [75, 25], 50\np0, p1 = 0.45, 0.75\ncoin = concatenate((random((n[0], t)) &lt; p0, random((n[1], t)) &lt; p1))\ncumul = mean(cumsum(2*coin-1, axis=0), axis=1)\n\n# Display accumulated counts\navg0 = (2*p0 - 1) * n[0]\navg1 = (2*p1 - 1) * n[1] + avg0\nfig = px.scatter(y=cumul, labels={'x': 'Flip count', 'y': \"Accumulated (Heads - Tails)\"})\nfig.add_traces(go.Scatter(x=[n[0],n[0]+n[1]], y=[avg0, avg1],\n    mode=\"lines\", name=\"Changed\", line=dict(dash='dash')))\nfig.add_traces(go.Scatter(x=[0,n[0]], y=[0, avg0], mode=\"lines\", name=\"Original\"))\n\n\n\n\n                                                \nFigure 5: Cumulative outcomes of flipping a coin with changing probabilities.\n\n\n\n\\(Z_n = (X_n - \\bar{x}) / s\\)\n\\[\nH_n = \\max(0, H_{n-1} + Z_n - \\omega)\\ ,\n\\tag{2}\\] \\[\nL_n = \\min(0, L_{n-1} + Z_n + \\omega)\\ ,\n\\tag{3}\\] where we set \\(H_0 = L_0 = 0\\).\n\n\nImplementation of the CUSUM algorithm\nimport numpy as np\n\ndef cusum(X, lim=5, w=0, mean=None, std=None, plot=True):\n    '''Detect changes in mean using the CUSUM test.\n\n    ARGUMENTS:\n    lim: number of standard deviations in drift to be detected.\n    w: number of standard devitaions as damping coefficient.\n    mean: expected mean or None to compute from 25 leading samples.\n    std: expected standard deviation or None to compute from 25 leading samples.\n    plot: whether to display a visualization.\n\n    RETURNS:\n    index of the first detected change or None.\n    '''\n    # Estimate mean and standard deviation\n    if mean is None:\n        mean = np.mean(X[:25])\n    if std is None:\n        std = np.mean(X[:25])\n    Z = (X - mean) / std\n\n    n = len(Z)\n    H = np.zeros(n)\n    L = np.zeros(n)\n    for i, z in enumerate(Z):\n        H[i] = max(0, H[i-1] + z - w)\n        L[i] = min(0, L[i-1] + z + w)\n\n    idx_H = np.argmax(np.append(H, lim) &gt;= lim)\n    idx_L = np.argmax(np.append(L, -lim) &lt;= -lim)\n    idx = min(idx_H, idx_L)\n\n    if plot:\n        fig = go.Figure()\n        fig.add_traces(go.Scatter(y=H, name=\"Upper cumulative sum\"))\n        fig.add_traces(go.Scatter(y=L, name=\"Lower cumulative sum\"))\n        fig.add_hline(y=lim, line_dash='dash', line_color=\"#636efa\")\n        fig.add_hline(y=-lim, line_dash='dash', line_color=\"#EF553B\")\n        if idx &lt; n:\n            HL = H if idx_H &lt;= idx_L else L\n            fig.add_traces(go.Scatter(x=[idx], y=[HL[idx]],\n                mode=\"markers\", marker_symbol='circle-open', name=\"Changepoint\",\n                marker=dict(size=12, line_width=4, color='black')))\n        fig.show()\n\n    return idx if idx &lt; n else None\n\nchange = cusum(nile['value'], lim=2)\nprint(f\"Detected changepoint at year {nile['time'][change]}.\")\n\n\n\n\n                                                \nFigure 6: CUSUM control chart.\n\n\n\nDetected changepoint at year 1905."
  },
  {
    "objectID": "posts/3-regression.html",
    "href": "posts/3-regression.html",
    "title": "Linear and nonlinear regression",
    "section": "",
    "text": "glm(y ~ x, ...) or glm(y ~ ., ...)\n\n\nCreate artificial data\nimport plotly.express as px\nfrom numpy.random import rand, normal, seed\nfrom numpy import repeat, concatenate, power\nfrom pandas import DataFrame\nseed(5805)\n\n# Create artificial data\nn, r, s, b = 100, 100, 5, 10\nlin = repeat(rand(n, 1) * r, 2, axis=1) + normal(scale=s, size=(n, 2))\nexp = power(b, lin[:,1:2] / r) * r / b\nexp = concatenate((lin[:,0:1], exp), axis=1)\n\n# Plot data\nlin_df = DataFrame({'x': lin[:,0], 'y': lin[:,1]})\nexp_df = DataFrame({'x': exp[:,0], 'y': exp[:,1]})\npx.scatter(lin_df, x='x', y='y').show()\npx.scatter(exp_df, x='x', y='y').show();\n\n\n\n\n\n\n\n                                                \n(a) Linear growth\n\n\n\n\n\n                                                \n(b) Exponential growth\n\n\n\nFigure 1: A couple of artificial datasets"
  },
  {
    "objectID": "posts/3-regression.html#regression",
    "href": "posts/3-regression.html#regression",
    "title": "Linear and nonlinear regression",
    "section": "",
    "text": "glm(y ~ x, ...) or glm(y ~ ., ...)\n\n\nCreate artificial data\nimport plotly.express as px\nfrom numpy.random import rand, normal, seed\nfrom numpy import repeat, concatenate, power\nfrom pandas import DataFrame\nseed(5805)\n\n# Create artificial data\nn, r, s, b = 100, 100, 5, 10\nlin = repeat(rand(n, 1) * r, 2, axis=1) + normal(scale=s, size=(n, 2))\nexp = power(b, lin[:,1:2] / r) * r / b\nexp = concatenate((lin[:,0:1], exp), axis=1)\n\n# Plot data\nlin_df = DataFrame({'x': lin[:,0], 'y': lin[:,1]})\nexp_df = DataFrame({'x': exp[:,0], 'y': exp[:,1]})\npx.scatter(lin_df, x='x', y='y').show()\npx.scatter(exp_df, x='x', y='y').show();\n\n\n\n\n\n\n\n                                                \n(a) Linear growth\n\n\n\n\n\n                                                \n(b) Exponential growth\n\n\n\nFigure 1: A couple of artificial datasets"
  },
  {
    "objectID": "posts/3-regression.html#linear-regression",
    "href": "posts/3-regression.html#linear-regression",
    "title": "Linear and nonlinear regression",
    "section": "Linear regression",
    "text": "Linear regression\n\n\n\nImage credit: Wikipedia\n\n\n\n\nCode for polynomial OLS\n# Taken from https://plotly.com/python/ml-regression/\nfrom numpy import linspace\nimport plotly.graph_objects as go\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\ndef plot(df, trend, **kwargs):\n    fig = px.scatter(df, x='x', y='y')\n    X = df['x'].values.reshape(-1, 1)\n    x_range = linspace(X.min(), X.max(), 100).reshape(-1, 1)\n    y = df['y']\n\n    y_fit = trend(X, x_range, y, **kwargs)\n    fig.add_traces(go.Scatter(x=x_range.squeeze(), y=y_fit,\n        showlegend=False, line=dict(color='black')))\n    return fig\n\ndef poly(X, x_range, y, degree=2):\n    poly = PolynomialFeatures(degree)\n    poly.fit(X)\n    X_poly = poly.transform(X)\n    x_range_poly = poly.transform(x_range)\n\n    model = LinearRegression(fit_intercept=False)\n    model.fit(X_poly, y)\n    return model.predict(x_range_poly)\n\ndef show(fig):\n    fig.update_layout(\n            margin=dict(l=0, r=20, t=20, b=20),\n            height=380\n        ).show()\n\n\n\n\nFit trendlines via ordinary least squares\nshow(px.scatter(lin_df, x='x', y='y', trendline='ols',\n    trendline_color_override='black'))\nshow(plot(exp_df, poly, degree=2))\nshow(px.scatter(exp_df, x='x', y='y', trendline='ols',\n    trendline_options=dict(log_y=True),\n    trendline_color_override='black'))\n\n\n\n\n\n\n\n                                                \n(a) Ordinary least squares\n\n\n\n\n\n                                                \n(b) OLS of y ~ x^2 + x\n\n\n\n\n\n                                                \n(c) OLS of log(y) ~ x\n\n\n\nFigure 2: Trendlines via ordinary least squares\n\n\n\n\n\nCode for KNN regression\n# Taken from https://plotly.com/python/ml-regression/\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsRegressor\n\ndef knn(X, x_range, y, k=20, weights='uniform'):\n    knn_uni = KNeighborsRegressor(10, weights=weights)\n    knn_uni.fit(X, y)\n    return knn_uni.predict(x_range.reshape(-1, 1))\n\n\n\n\nFit trendlines via miscellaneous methods\nfrom scipy.optimize import curve_fit\n\n# Nonlinear regression\nf = lambda x, a, b, c: a * x / (b + x) + c\ndef nonlinear(X, x_range, y, f, init=None):\n    args, _ = curve_fit(f, exp_df['x'], exp_df['y'], p0=init)\n    return f(x_range, *args).squeeze()\nshow(plot(exp_df, nonlinear, f=f, init=[-90, -200, 10]))\n\n# Non-parametric regression\nshow(plot(exp_df, knn, k=20))\nshow(px.scatter(exp_df, x='x', y='y', trendline='lowess',\n   trendline_color_override='black'))\n\n\n\n\n\n\n\n                                                \n(a) Nonlinear least squares\n\n\n\n\n\n                                                \n(b) KNN Regression\n\n\n\n\n\n                                                \n(c) LOWESS\n\n\n\nFigure 3: Trendlines via other regression methods\n\n\n\n\n\nFit a multiratiate regression model\n# Taken from https://plotly.com/python/ml-regression/\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom sklearn.svm import SVR\n\nmesh_size = .02\nmargin = 0\n\ndf = px.data.iris()\n\nX = df[['sepal_width', 'sepal_length']]\ny = df['petal_width']\n\n# Condition the model on sepal width and length, predict the petal width\nmodel = SVR(C=1.)\nmodel.fit(X, y)\n\n# Create a mesh grid on which we will run our model\nx_min, x_max = X.sepal_width.min() - margin, X.sepal_width.max() + margin\ny_min, y_max = X.sepal_length.min() - margin, X.sepal_length.max() + margin\nxrange = np.arange(x_min, x_max, mesh_size)\nyrange = np.arange(y_min, y_max, mesh_size)\nxx, yy = np.meshgrid(xrange, yrange)\n\n# Run model\npred = model.predict(np.c_[xx.ravel(), yy.ravel()])\npred = pred.reshape(xx.shape)\n\n# Generate the plot\nfig = px.scatter_3d(df, x='sepal_width', y='sepal_length', z='petal_width')\nfig.update_traces(marker=dict(size=5))\nfig.add_traces(go.Surface(x=xrange, y=yrange, z=pred, name='pred_surface', opacity=0.75))\nfig.show()\n\n\n\n\n                                                \nFigure 4: Support Vector Regression on the Iris dataset"
  },
  {
    "objectID": "posts/1-probability.html",
    "href": "posts/1-probability.html",
    "title": "Probability theory and random variables",
    "section": "",
    "text": "Here’s the definition; taken from Wikipedia, which in order takes it from Fristedt and Gray (1996, 11).\n\nDefinition 1 (Random variable). Let \\((\\Omega, \\mathcal{F}, P)\\) be a probability space and \\((E, \\mathcal{E})\\) a measurable space. Then an \\((E, \\mathcal{E})\\)-valued random variable is a measurable function \\(X : \\Omega \\to E\\), which measures that, for every subset \\(B \\in \\mathcal{E}\\), its preimage is \\(\\mathcal{F}\\)-measurable; \\(X^{-1}(B) \\in \\mathcal{F}\\), where \\(X^{-1}(B) = \\{\\omega : X(\\omega) \\in B\\}\\)."
  },
  {
    "objectID": "posts/1-probability.html#random-variables",
    "href": "posts/1-probability.html#random-variables",
    "title": "Probability theory and random variables",
    "section": "",
    "text": "Here’s the definition; taken from Wikipedia, which in order takes it from Fristedt and Gray (1996, 11).\n\nDefinition 1 (Random variable). Let \\((\\Omega, \\mathcal{F}, P)\\) be a probability space and \\((E, \\mathcal{E})\\) a measurable space. Then an \\((E, \\mathcal{E})\\)-valued random variable is a measurable function \\(X : \\Omega \\to E\\), which measures that, for every subset \\(B \\in \\mathcal{E}\\), its preimage is \\(\\mathcal{F}\\)-measurable; \\(X^{-1}(B) \\in \\mathcal{F}\\), where \\(X^{-1}(B) = \\{\\omega : X(\\omega) \\in B\\}\\)."
  },
  {
    "objectID": "posts/1-probability.html#concentration-bounds",
    "href": "posts/1-probability.html#concentration-bounds",
    "title": "Probability theory and random variables",
    "section": "Concentration bounds",
    "text": "Concentration bounds\n\nHoeffding’s inequality\n\n\nExample"
  },
  {
    "objectID": "posts/1-probability.html#multi-armed-bandits",
    "href": "posts/1-probability.html#multi-armed-bandits",
    "title": "Probability theory and random variables",
    "section": "Multi-armed bandits",
    "text": "Multi-armed bandits\nBased on Slivkins (2022)\n\n\n\\begin{algorithm} \\caption{Stochastic MAB (Framework)} \\begin{algorithmic} \\Require{$K$ and $T$ (both known), unknown reward distributions $\\mathcal{D}_a$.} \\For{$t = 1, 2, \\ldots$} \\State Choose an action $a_t \\in [K]$. \\State Suffer loss $z_t[a_t]$ and also only observe $z_t[a_t]$. \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\nHere, \\(z_t\\) is a random vector sampled from the unknown distributions, i.e., \\(z_t[a] \\sim \\mathcal{D}_a\\).\n\nExplore-then-exploit\nJi (2022, Lecture 14)\n\n\n\\begin{algorithm} \\caption{Explore-then-exploit} \\begin{algorithmic} \\Require{$K$ and $T$ (both known), unknown reward distributions $\\mathcal{D}_a$.} \\For{$t = 1, 2, \\ldots$} \\State Explore phase: try each arm $N$ times. \\State Exploit phase: determine $\\tilde{a}$ with the highest average reward; then play $\\tilde{a}$ in all remaining rounds. \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\n\\(N = 2 \\sqrt[3]{T^2 \\log T / K^2}\\)\n\n\nCode for Explore-then-exploit\nfrom math import log\nfrom numpy import argmax\n\nclass ExploreExploit:\n    def __init__(self, K, T):\n        self.K = K\n        self.N = round(2 * (T**2 * log(T) / K**2)**(1/3))\n        self.totals = [0] * K\n    \n    def __str__(self):\n        return \"Explore-then-exploit\"\n\n    def play(self, t):\n        '''Choose an arm to play.'''\n        if t &lt;= self.K*self.N:\n            # Play each arm N times\n            return t % self.K\n        else:\n            # Play best arm\n            return self.best\n    \n    def feedback(self, t, a, r):\n        '''Receive a reward.'''\n        if t &lt;= self.K*self.N:\n            # Record reward\n            self.totals[a] += r\n        if t == self.K*self.N:\n            # Compute best arm\n            self.best = argmax(self.totals)\n\n\n\n\nEpsilon-greedy\nJi (2022, Lecture 14)\n\n\n\\begin{algorithm} \\caption{Epsilon-greedy} \\begin{algorithmic} \\Require{$K$ and $T$ (both known), unknown reward distributions $\\mathcal{D}_a$.} \\For{$t = 1, 2, \\ldots$} \\State Toss a coin with success rate $\\epsilon_t$. \\If{success} \\State Explore: choose an arm uniformly at random. \\Else \\State Exploit: choose an arm with the highest average reward so far. \\EndIf \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\n\n\nCode for Epsilon-greedy\nfrom math import log\nfrom random import random, randrange\nfrom numpy import argmax, divide, errstate, inf, nan\n\nclass EpsilonGreedy:\n    def __init__(self, K, _):\n        self.K = K\n        self.totals = [0] * K\n        self.counts = [0] * K\n    \n    def __str__(self):\n        return \"Epsilon-greedy\"\n\n    def _eps(self, t):\n        return (self.K*log(t)/t)**(1/3)\n\n    def _best(self):\n        with errstate(divide='ignore', invalid='ignore'):\n            avg = divide(self.totals, self.counts)\n        avg[avg == inf] = -inf\n        avg[avg == nan] = -inf\n        return argmax(avg)\n\n    def play(self, t):\n        '''Choose an arm to play.'''\n        if random() &lt; self._eps(t):\n            # Play random arm\n            return randrange(self.K)\n        else:\n            # Play best arm\n            return self._best()\n    \n    def feedback(self, t, a, r):\n        '''Receive a reward.'''\n        self.totals[a] += r\n        self.counts[a] += 1\n\n\n\n\nSuccessive elimination\nJi (2022, Lecture 14)\n\n\n\\begin{algorithm} \\caption{Successive Elimination} \\begin{algorithmic} \\Require{$K$ and $T$ (both known), unknown reward distributions $\\mathcal{D}_a$.} \\State Initialize active arm set $\\mathcal{A}_1 = [K]$. \\For{$p = 1, 2, \\ldots$} \\State Play each arm in $\\mathcal{A}_p$ once. \\State Let $t$ be the time at the end of the current phase $p$. \\State $\\mathcal{A}_{p+1} = \\{ a \\in \\mathcal{A}_p : \\mathrm{UCB}_t (a) \\geq \\max_{a' \\in \\mathcal{A}_p} \\mathrm{LCB}_t (a) \\}$ \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\n\n\nCode for Successive elimination\nfrom random import random, randrange\nfrom numpy import argmax, divide, errstate, inf, nan, log, sqrt\nimport numpy as np\n\nclass SuccessiveElimination:\n    def __init__(self, K, T, delta=1500):\n        self.num = log(2 * K * T / delta) / 2\n        assert self.num &gt; 0, \"delta too large\"\n        self.totals = [0] * K\n        self.counts = [0] * K\n        self.A = list(range(K))\n        self.idx = 0\n    \n    def __str__(self):\n        return \"Successive elimination\"\n\n    def _mu(self):\n        with errstate(divide='ignore', invalid='ignore'):\n            mu = divide(self.totals, self.counts)\n        mu[mu == inf] = 0\n        mu[mu == nan] = 0\n        return mu\n\n    def _beta(self):\n        with errstate(divide='ignore'):\n            beta = sqrt(divide(self.num, self.counts))\n        return beta\n\n    def play(self, t):\n        '''Choose an arm to play.'''\n        if self.idx == len(self.A):\n            # Update arm set A\n            u, b = self._mu(), self._beta()\n            lcb, ucb = u-b, u+b\n            self.A = [a for a in self.A if ucb[a] &gt;= np.max(lcb)]\n            self.idx = 0\n        # Play each arm in A\n        a = self.A[self.idx]\n        self.idx += 1\n        return a\n    \n    def feedback(self, t, a, r):\n        '''Receive a reward.'''\n        self.totals[a] += r\n        self.counts[a] += 1\n\n\n\n\nUCB\nJi (2022, Lecture 15)\n\n\n\\begin{algorithm} \\caption{UCB} \\begin{algorithmic} \\Require{$K$ and $T$ (both known), unknown reward distributions $\\mathcal{D}_a$.} \\For{$t = 1, 2, \\ldots$} \\State $a_t = \\arg\\max_{a \\in [K]} \\mathrm{UCB}_t (a)$ \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\n\n\nCode for UCB\nfrom random import random, randrange\nfrom numpy import argmax, divide, errstate, inf, nan, log, sqrt\nimport numpy as np\n\nclass UCB:\n    def __init__(self, K, T, delta=1):\n        self.num = log(2 * K * T / delta) / 2\n        assert self.num &gt; 0, \"delta too large\"\n        self.totals = [0] * K\n        self.counts = [0] * K\n    \n    def __str__(self):\n        return \"UCB\"\n\n    def _mu(self):\n        with errstate(divide='ignore', invalid='ignore'):\n            mu = divide(self.totals, self.counts)\n        mu[mu == inf] = 0\n        mu[mu == nan] = 0\n        return mu\n\n    def _beta(self):\n        with errstate(divide='ignore'):\n            beta = sqrt(divide(self.num, self.counts))\n        return beta\n\n    def play(self, t):\n        '''Choose an arm to play.'''\n        ucb = self._mu() + self._beta()\n        return argmax(ucb)\n    \n    def feedback(self, t, a, r):\n        '''Receive a reward.'''\n        self.totals[a] += r\n        self.counts[a] += 1\n\n\n\n\nSimulation\n\nTechnical note\n\n\nSeed random number generators\nfrom random import seed as py_seed\nfrom numpy.random import seed as np_seed\npy_seed(5805)\nnp_seed(5805)\n\n\nWelford’s online algorithm\n\n\nSimulation code\nfrom random import random\nfrom numpy.random import normal\nfrom numpy import zeros, sqrt\n\n# Simulation parameters\nT = 1000    # Number of rounds\nF = 10      # Logging frequency\nN = 100     # Number of trials\n\n# Arms to explore\nmeans = [0.5, 0.6]\nbest = max(means)\narms = [random, lambda: normal(means[1], 0.2, 1).item()]\nK = len(arms)\n\n# Algorithms to use\ncs = [ExploreExploit, EpsilonGreedy, SuccessiveElimination, UCB]\nA = len(cs)\n\n# Evaluate algorithms\nx = zeros((T//F+1, A))\ns = zeros((T//F+1, A))\nfor n in range(1, N+1):\n    for i, c in enumerate(cs):\n        alg = c(K, T)\n        rgt = 0\n        for t in range(1, T+1):\n            a = alg.play(t)\n            r = arms[a]()\n            alg.feedback(t, a, r)\n            rgt += best - r\n            if t % F == 0:\n                # Welford's online algorithm\n                xn1 = x[t//F, i]\n                x[t//F, i] = ((n-1)*xn1 + rgt) / n\n                s[t//F, i] += (rgt - xn1) * (rgt - x[t//F, i])\ns = 0.2*sqrt(s / (N-1))\n\n\n\n\nCode for plotting regret\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nt = np.arange(1, T+2, F)\nplt.plot(t, x)\nfor i in range(A):\n    plt.fill_between(t, x[:,i]-s[:,i], x[:,i]+s[:,i], alpha=.15)\nplt.xlabel(\"Timestep\")\nplt.ylabel(\"Regret\")\nplt.legend([c(K, T) for c in cs]);\n\n\n\n\n\nFigure 1: Performance of MAB algorithms"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Alice Didenkova",
    "section": "",
    "text": "A graduate student at Virginia Tech."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Alice Didenkova",
    "section": "Education",
    "text": "Education\nVirginia Tech | Blacksburg, VA\nPh.D. Candidate in Computer Science | 2022 - present\nVirginia Tech | Blacksburg, VA\nB.S. in Computer Science, Mathematics | 2019 - 2022"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Alice Didenkova",
    "section": "Experience",
    "text": "Experience\nFujitsu Research of America | Research Intern in Quantum Computing | Summer 2023\nCS 3214: Computer Systems | Teaching Assistant | 2021 - present"
  },
  {
    "objectID": "about.html#topics",
    "href": "about.html#topics",
    "title": "Alice Didenkova",
    "section": "Topics",
    "text": "Topics\nMy interests and current research topics include:\n\n⚛️ Quantum computing\n🤖 Reinforcement learning\n\nDisclaimer: this blog has been created to fulfill the Final Project requirements of CS 5805: Machine Learning."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Probability theory and random variables\n\n\nConcentration bounds and multi-armed bandits\n\n\n\n\nTODO\n\n\n\n\nTODO\n\n\n\n\n\n\nDec 1, 2023\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\nSparse subspace clustering\n\n\n\n\nTODO\n\n\n\n\nTODO\n\n\n\n\n\n\nDec 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nLinear and nonlinear regression\n\n\n\n\n\n\n\nTODO\n\n\n\n\nTODO\n\n\n\n\n\n\nDec 3, 2023\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\nTODO\n\n\n\n\nTODO\n\n\n\n\n\n\nDec 4, 2023\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly/outlier detection\n\n\nChangepoint detection\n\n\n\n\nTODO\n\n\n\n\nTODO\n\n\n\n\n\n\nDec 5, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2-clustering.html",
    "href": "posts/2-clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Scatterplot matrix code by sklearn\n# Directly taken from:\n# https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html\n\nimport time\nimport warnings\nfrom itertools import cycle, islice\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import cluster, datasets, mixture\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.preprocessing import StandardScaler\n\nfrom ssc import linear, SparseSubspaceClustering\n\n# ============\n# Generate datasets. We choose the size big enough to see the scalability\n# of the algorithms, but not too big to avoid too long running times\n# ============\nn_samples = 200\nseed = 5805\nnoisy_circles = datasets.make_circles(\n    n_samples=n_samples, factor=0.5, noise=0.05, random_state=seed\n)\nnoisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=seed)\nblobs = datasets.make_blobs(n_samples=n_samples, random_state=seed)\nrng = np.random.RandomState(seed)\nno_structure = rng.rand(n_samples, 2), None\n\n# Anisotropicly distributed data\nrandom_state = 5805\nX, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\ntransformation = [[0.6, -0.6], [-0.4, 0.8]]\nX_aniso = np.dot(X, transformation)\naniso = (X_aniso, y)\n\n# blobs with varied variances\nvaried = datasets.make_blobs(\n    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n)\n\n# ============\n# Set up cluster parameters\n# ============\nplt.figure(figsize=(10.5, 6.5))\nplt.subplots_adjust(\n    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.01, hspace=0.01\n)\n\nplot_num = 1\n\ndefault_base = {\n    \"quantile\": 0.3,\n    \"eps\": 0.3,\n    \"damping\": 0.9,\n    \"preference\": -200,\n    \"n_neighbors\": 3,\n    \"n_clusters\": 3,\n    \"min_samples\": 7,\n    \"xi\": 0.05,\n    \"min_cluster_size\": 0.1,\n    \"allow_single_cluster\": True,\n    \"hdbscan_min_cluster_size\": 15,\n    \"hdbscan_min_samples\": 3,\n    \"random_state\": 42,\n}\n\ndatasets = [\n    (\n        noisy_circles,\n        {\n            \"damping\": 0.77,\n            \"preference\": -240,\n            \"quantile\": 0.2,\n            \"n_clusters\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.08,\n        },\n    ),\n    (\n        noisy_moons,\n        {\n            \"damping\": 0.75,\n            \"preference\": -220,\n            \"n_clusters\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.1,\n        },\n    ),\n    (\n        varied,\n        {\n            \"eps\": 0.18,\n            \"n_neighbors\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.01,\n            \"min_cluster_size\": 0.2,\n        },\n    ),\n    (\n        aniso,\n        {\n            \"eps\": 0.15,\n            \"n_neighbors\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.1,\n            \"min_cluster_size\": 0.2,\n        },\n    ),\n    (blobs, {\"min_samples\": 7, \"xi\": 0.1, \"min_cluster_size\": 0.2}),\n    (linear, {}),\n    (no_structure, {}),\n]\n\nfor i_dataset, (dataset, algo_params) in enumerate(datasets):\n    # update parameters with dataset-specific values\n    params = default_base.copy()\n    params.update(algo_params)\n\n    X, y = dataset\n\n    # normalize dataset for easier parameter selection\n    X = StandardScaler().fit_transform(X)\n\n    # estimate bandwidth for mean shift\n    bandwidth = cluster.estimate_bandwidth(X, quantile=params[\"quantile\"])\n\n    # connectivity matrix for structured Ward\n    connectivity = kneighbors_graph(\n        X, n_neighbors=params[\"n_neighbors\"], include_self=False\n    )\n    # make connectivity symmetric\n    connectivity = 0.5 * (connectivity + connectivity.T)\n\n    # ============\n    # Create cluster objects\n    # ============\n    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n    two_means = cluster.MiniBatchKMeans(\n        n_clusters=params[\"n_clusters\"],\n        n_init=\"auto\",\n        random_state=params[\"random_state\"],\n    )\n    ward = cluster.AgglomerativeClustering(\n        n_clusters=params[\"n_clusters\"], linkage=\"ward\", connectivity=connectivity\n    )\n    spectral = cluster.SpectralClustering(\n        n_clusters=params[\"n_clusters\"],\n        eigen_solver=\"arpack\",\n        affinity=\"nearest_neighbors\",\n        random_state=params[\"random_state\"],\n    )\n    dbscan = cluster.DBSCAN(eps=params[\"eps\"])\n    hdbscan = cluster.HDBSCAN(\n        min_samples=params[\"hdbscan_min_samples\"],\n        min_cluster_size=params[\"hdbscan_min_cluster_size\"],\n        allow_single_cluster=params[\"allow_single_cluster\"],\n    )\n    optics = cluster.OPTICS(\n        min_samples=params[\"min_samples\"],\n        xi=params[\"xi\"],\n        min_cluster_size=params[\"min_cluster_size\"],\n    )\n    affinity_propagation = cluster.AffinityPropagation(\n        damping=params[\"damping\"],\n        preference=params[\"preference\"],\n        random_state=params[\"random_state\"],\n    )\n    average_linkage = cluster.AgglomerativeClustering(\n        linkage=\"average\",\n        metric=\"cityblock\",\n        n_clusters=params[\"n_clusters\"],\n        connectivity=connectivity,\n    )\n    birch = cluster.Birch(n_clusters=params[\"n_clusters\"])\n    gmm = mixture.GaussianMixture(\n        n_components=params[\"n_clusters\"],\n        covariance_type=\"full\",\n        random_state=params[\"random_state\"],\n    )\n    ssc = SparseSubspaceClustering(\n        n_clusters=params[\"n_clusters\"],\n    )\n\n    clustering_algorithms = (\n        (\"MiniBatch\\nKMeans\", two_means),\n        (\"Affinity\\nPropagation\", affinity_propagation),\n        (\"MeanShift\", ms),\n        (\"Spectral\\nClustering\", spectral),\n        (\"Ward\", ward),\n        (\"Agglomerative\\nClustering\", average_linkage),\n        (\"DBSCAN\", dbscan),\n        (\"HDBSCAN\", hdbscan),\n        (\"OPTICS\", optics),\n        (\"BIRCH\", birch),\n        (\"Gaussian\\nMixture\", gmm),\n        (\"Sparse\\nSubspace\\nClustering\", ssc),\n    )\n\n    for name, algorithm in clustering_algorithms:\n        t0 = time.time()\n\n        # catch warnings related to kneighbors_graph, subspace clustering\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"the number of connected components of the \"\n                + \"connectivity matrix is [0-9]{1,2}\"\n                + \" &gt; 1. Completing it to avoid stopping the tree early.\",\n                category=UserWarning,\n            )\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"Graph is not fully connected, spectral embedding\"\n                + \" may not work as expected.\",\n                category=UserWarning,\n            )\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"Solution may be inaccurate. Try another solver,\"\n                + \" adjusting the solver settings, or solve with\"\n                + \" verbose=True for more information.\",\n                category=UserWarning,\n            )\n            algorithm.fit(X)\n\n        t1 = time.time()\n        if hasattr(algorithm, \"labels_\"):\n            y_pred = algorithm.labels_.astype(int)\n        else:\n            y_pred = algorithm.predict(X)\n\n        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            plt.title(name, size=9)\n\n        colors = np.array(\n            list(\n                islice(\n                    cycle(\n                        [\n                            \"#377eb8\",\n                            \"#ff7f00\",\n                            \"#4daf4a\",\n                            \"#f781bf\",\n                            \"#a65628\",\n                            \"#984ea3\",\n                            \"#999999\",\n                            \"#e41a1c\",\n                            \"#dede00\",\n                        ]\n                    ),\n                    int(max(y_pred) + 1),\n                )\n            )\n        )\n        # add black color for outliers (if any)\n        colors = np.append(colors, [\"#000000\"])\n        plt.scatter(X[:, 0], X[:, 1], s=5, color=colors[y_pred])\n\n        plt.xlim(-2.5, 2.5)\n        plt.ylim(-2.5, 2.5)\n        plt.xticks(())\n        plt.yticks(())\n        plt.text(\n            0.99,\n            0.01,\n            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n            transform=plt.gca().transAxes,\n            size=7.5,\n            horizontalalignment=\"right\",\n        )\n        plot_num += 1\n\nplt.show()\n\n\n\n\n\nA comparison of various clustering methods.\n\n\n\n\n# The 'X'-looking dataset\n\nlinear = np.concatenate((\n        rng.rand(n_samples//2, 2),                              # □ Square\n        rng.rand(n_samples//4, 2) * 0.05\n            + np.repeat(rng.rand(n_samples//4, 1), 2, axis=1),  # / Diagonal\n        rng.rand(n_samples//4, 2) * 0.05\n            + np.repeat(rng.rand(n_samples//4, 1), 2, axis=1)   # \\ Diagonal\n            * [1, -1] + [0, 1]\n    ), axis=0) - [0.5, 0.5], None\n\n\nGenerate a simpler dataset\nn = 20\nY = np.concatenate((\n    rng.rand(2, n//2) * 0.01\n        + np.repeat(rng.rand(1, n//2), 2, axis=0),  # / Diagonal\n    rng.rand(2, n//2) * 0.01\n        + np.repeat(rng.rand(1, n//2), 2, axis=0)   # \\ Diagonal\n        * [[1], [-1]] + [[0], [1]]\n), axis=1) - [[0.5], [0.5]]\n\n\n\n\nSolve and visualize a convex optimization problem\nfrom cvxpy import Variable, Minimize, norm, diag, Problem, ECOS\nimport plotly.express as px\n\n# Variable matrix\nC = Variable((n, n))\n\n# Minimize ||C||_1\nobjective = Minimize(norm(C,1))\n\n# Subject to Y = YC, diag(C) = 0\nconstraints = [Y == Y @ C, diag(C) == 0]\nprob = Problem(objective, constraints)\nprob.solve(solver=ECOS)\n\nC = C.value / np.max(np.abs(C.value), axis=0)\npx.imshow(C)\n\n\n\n                                                \n\n\ndef fit(self, Y):\n    Y = Y.T\n    d, n = Y.shape\n    assert d &lt; n\n\n    ## 1. Solve sparse optimization problem\n\n    # Starting form, equation (5)\n    C = Variable((n, n))\n    objective = norm(C,1)\n    constraint = Y @ C\n\n    # Account for outliers\n    if self.use_E:\n        mu_e = np.partition(np.sum(np.abs(Y), axis=0), -2)[-2]\n        l_e = 20 / mu_e\n        E = Variable((d, n))\n        objective += l_e * norm(E,1)\n        constraint += E\n\n    # Account for noise\n    if self.use_Z:\n        G = np.abs(Y.T @ Y)\n        mu_z = np.min(np.max(G - np.diag(np.diag(G)), axis=1))\n        l_z = 800 / mu_z\n        Z = Variable((d, n))\n        objective += l_z/2 * norm(Z,\"fro\")**2\n        constraint += Z\n\n    constraints = [Y == constraint, diag(C) == 0]\n    prob = Problem(Minimize(objective), constraints)\n    try:\n        prob.solve(solver=ECOS)\n\n        ## 2. Normalize the columns of C\n        C = C.value / np.max(np.abs(C.value), axis=0)\n\n        ## 3. Form the weights of a similarity graph\n        W = np.abs(C)\n        W = W + W.T\n\n        ## 4. Apply spectral clustering on the graph\n        self.labels_ = SpectralClustering(\n                n_clusters=self.n_clusters,\n                affinity='precomputed'\n            ).fit_predict(W)\n    except SolverError:\n        self.labels_ = np.zeros(n)"
  },
  {
    "objectID": "posts/4-classification.html",
    "href": "posts/4-classification.html",
    "title": "Classification",
    "section": "",
    "text": "Image credit: sklearn"
  },
  {
    "objectID": "posts/4-classification.html#simple-dataset",
    "href": "posts/4-classification.html#simple-dataset",
    "title": "Classification",
    "section": "Simple dataset",
    "text": "Simple dataset\n\nIris\n\n\n\nImage credit: Sebastian Raschka\n\n\n\n\n\nImage credit: Gaurav Chauhan\n\n\n\n\n2D scatterplot of iris\nimport plotly.express as px\nfrom sklearn.preprocessing import MinMaxScaler\n\niris = px.data.iris().drop('species_id', axis=1)\nfig = px.scatter(iris, x=\"sepal_width\", y=\"sepal_length\", size=\"petal_length\",\n    color=\"species\", marginal_y=\"violin\", marginal_x=\"violin\",\n    template=\"simple_white\", hover_data=['petal_width'])\nop = MinMaxScaler(feature_range=(0.5, 1.0)).fit_transform(\n    iris['petal_width'].values[:, None])\nfig.update_traces(marker=dict(opacity=op), selector=dict(mode='markers'))\nfig.show();\n\n\n\n                                                \n\n\n\n\n3D scatterplot of iris\nimport plotly.express as px\n\nfig = px.scatter_3d(iris, x=\"sepal_width\", y=\"sepal_length\", z=\"petal_width\",\n    size=\"petal_length\", color=\"species\", template=\"simple_white\")\nfig.show();\n\n\n\n                                                \n\n\n\n\nNaive Bayes\n\n\nNaive bayes classifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, precision_score\n\n# Fit and predict\nX, y = iris.drop('species', axis=1), iris['species']\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.5, random_state=5805)\nnb = GaussianNB().fit(X_train, y_train)\niris['predict'] = nb.predict(X)\ny_pred = nb.predict(X_test)\nprint(f\"Accuracy:  {accuracy_score(y_test, y_pred):.4f}\\n\"\n      f\"Precision: {precision_score(y_test, y_pred, average='macro'):.4f}\")\n\n\nAccuracy:  0.9467\nPrecision: 0.9444\n\n\n\n\nPerformance\n\n\nCode for plotting confusion matrix\nimport seaborn as sn\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\n\nnames = iris['species'].unique()\nconf = confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(conf, index=names, columns=names)\nplt.figure(figsize = (4,3))\nsn.heatmap(df_cm, annot=True, fmt='d');\n\n\n\n\n\n\n\n\n\n\n\n2D scatterplot of iris & predictions\nimport numpy as np\n\nfix = {\"circle\": \"#1F77B4\", \"diamond\": \"#FF7F0E\", \"square\": \"#2CA02C\"}\nfig = px.scatter(iris, x=\"sepal_width\", y=\"sepal_length\", size=\"petal_length\",\n    color=\"species\", symbol=\"predict\",\n    template=\"simple_white\", hover_data=['petal_width'])\nfig.update_traces(marker=dict(opacity=op), selector=dict(mode='markers'))\nfor symbol, color in fix.items():\n    fig.update_traces(marker=dict(line=dict(width=3, color=color),\n                      symbol=\"circle\"),\n                      selector=dict(mode='markers', marker_symbol=symbol))\nfig.show();\n\n\n\n                                                \n\n\n\n\n3D scatterplot of iris & predictions\nimport plotly.express as px\n\nfig = px.scatter_3d(iris, x=\"sepal_width\", y=\"sepal_length\", z=\"petal_width\",\n    size=\"petal_length\", color=\"species\", symbol=\"predict\",\n    template=\"simple_white\")\nfor symbol, color in fix.items():\n    fig.update_traces(marker=dict(line=dict(color=color)),\n                      selector=dict(mode='markers', marker_symbol=symbol))\nfig.show();"
  },
  {
    "objectID": "posts/4-classification.html#complex-dataset",
    "href": "posts/4-classification.html#complex-dataset",
    "title": "Classification",
    "section": "Complex dataset",
    "text": "Complex dataset\n\nMNIST\n\n\n\nImage credit: Orhan G. Yalçın\n\n\n\n\nNeural network\npytorch tutorial\n\n\nImport pytorch\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\nimport torchvision.transforms as transforms\nimport torch.optim as optim\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n\nUsing device: cpu\n\n\n\n\n0.3%0.7%1.0%1.3%1.7%2.0%2.3%2.6%3.0%3.3%3.6%4.0%4.3%4.6%5.0%5.3%5.6%6.0%6.3%6.6%6.9%7.3%7.6%7.9%8.3%8.6%8.9%9.3%9.6%9.9%10.2%10.6%10.9%11.2%11.6%11.9%12.2%12.6%12.9%13.2%13.6%13.9%14.2%14.5%14.9%15.2%15.5%15.9%16.2%16.5%16.9%17.2%17.5%17.9%18.2%18.5%18.8%19.2%19.5%19.8%20.2%20.5%20.8%21.2%21.5%21.8%22.1%22.5%22.8%23.1%23.5%23.8%24.1%24.5%24.8%25.1%25.5%25.8%26.1%26.4%26.8%27.1%27.4%27.8%28.1%28.4%28.8%29.1%29.4%29.8%30.1%30.4%30.7%31.1%31.4%31.7%32.1%32.4%32.7%33.1%33.4%33.7%34.0%34.4%34.7%35.0%35.4%35.7%36.0%36.4%36.7%37.0%37.4%37.7%38.0%38.3%38.7%39.0%39.3%39.7%40.0%40.3%40.7%41.0%41.3%41.7%42.0%42.3%42.6%43.0%43.3%43.6%44.0%44.3%44.6%45.0%45.3%45.6%45.9%46.3%46.6%46.9%47.3%47.6%47.9%48.3%48.6%48.9%49.3%49.6%49.9%50.2%50.6%50.9%51.2%51.6%51.9%52.2%52.6%52.9%53.2%53.6%53.9%54.2%54.5%54.9%55.2%55.5%55.9%56.2%56.5%56.9%57.2%57.5%57.9%58.2%58.5%58.8%59.2%59.5%59.8%60.2%60.5%60.8%61.2%61.5%61.8%62.1%62.5%62.8%63.1%63.5%63.8%64.1%64.5%64.8%65.1%65.5%65.8%66.1%66.4%66.8%67.1%67.4%67.8%68.1%68.4%68.8%69.1%69.4%69.8%70.1%70.4%70.7%71.1%71.4%71.7%72.1%72.4%72.7%73.1%73.4%73.7%74.0%74.4%74.7%75.0%75.4%75.7%76.0%76.4%76.7%77.0%77.4%77.7%78.0%78.3%78.7%79.0%79.3%79.7%80.0%80.3%80.7%81.0%81.3%81.7%82.0%82.3%82.6%83.0%83.3%83.6%84.0%84.3%84.6%85.0%85.3%85.6%85.9%86.3%86.6%86.9%87.3%87.6%87.9%88.3%88.6%88.9%89.3%89.6%89.9%90.2%90.6%90.9%91.2%91.6%91.9%92.2%92.6%92.9%93.2%93.6%93.9%94.2%94.5%94.9%95.2%95.5%95.9%96.2%96.5%96.9%97.2%97.5%97.9%98.2%98.5%98.8%99.2%99.5%99.8%100.0%\n100.0%\n2.0%4.0%6.0%7.9%9.9%11.9%13.9%15.9%17.9%19.9%21.9%23.8%25.8%27.8%29.8%31.8%33.8%35.8%37.8%39.7%41.7%43.7%45.7%47.7%49.7%51.7%53.7%55.6%57.6%59.6%61.6%63.6%65.6%67.6%69.6%71.5%73.5%75.5%77.5%79.5%81.5%83.5%85.5%87.4%89.4%91.4%93.4%95.4%97.4%99.4%100.0%\n100.0%\n\n\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\nExtracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\nExtracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\nExtracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\nExtracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\n\n\n\n\nCreate a neural network\nclass NeuralNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 3, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(3, 6, 5)\n        self.fc1 = nn.Linear(6 * 4 * 4, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        return x\n\nmodel = NeuralNet()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\n\n\n\nTrain a neural network\n# Do one pass over the dataset\nfor i, data in enumerate(trainloader, 0):\n    inputs, labels = data\n\n    # Resent the gradients\n    optimizer.zero_grad()\n\n    # Forward/backward pass\n    outputs = model(inputs)\n    loss = criterion(outputs, labels)\n    loss.backward()\n    optimizer.step()\n\n# Evaluate network\ny_test, y_pred = [], []\nwith torch.no_grad():\n    for data in testloader:\n        inputs, labels = data\n        outputs = model(inputs)\n        _, pred = torch.max(outputs.data, 1)\n        y_test.extend(labels)\n        y_pred.extend(pred)\nprint(f\"Accuracy:  {accuracy_score(y_test, y_pred):.4f}\\n\"\n      f\"Precision: {precision_score(y_test, y_pred, average='macro'):.4f}\")\n\n\nAccuracy:  0.8951\nPrecision: 0.8975\n\n\n\n\nPerformance\n\n\nCode for plotting confusion matrix\nconf = confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(conf, index=names, columns=names)\nplt.figure(figsize = (8.5,7))\nsn.heatmap(df_cm, annot=True, fmt='d');\n\n\n\n\n\n\n\n\n\n\n\nShow test samples with mistakes\nimport torchvision\nimport numpy as np\n\ndef imshow(img):\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n\n# Get samples where mistakes were made\ndef wrong(dataloader):\n    for i, (images, labels) in enumerate(testloader):\n        for j, (image, label) in enumerate(zip(images, labels)):\n            label, pred = label.item(), y_pred[batch_size*i+j].item()\n            if pred != label:\n                yield (image, label, pred)\n\n\n# Plot several examples\nloader = wrong(testloader)\nfor _ in range(4):\n    image, label, pred = next(loader)\n    img = np.transpose(image, (1, 2, 0))\n    plt.imshow(img, cmap='Greys')\n    plt.title(f\"Actual: {label}\\nPredicted: {pred}\", size=42)\n    plt.xticks([],[])\n    plt.yticks([],[])\n    plt.show()"
  }
]