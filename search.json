[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Probability theory and random variables\nClustering\nLinear and nonlinear regression\nClassification\nAnomaly/outlier detection"
  },
  {
    "objectID": "1-probability.html",
    "href": "1-probability.html",
    "title": "Probability theory and random variables",
    "section": "",
    "text": "Here’s the definition; taken from Wikipedia, which in order takes it from Fristedt and Gray (1996, 11).\n\nDefinition 1 (Random variable). Let \\((\\Omega, \\mathcal{F}, P)\\) be a probability space and \\((E, \\mathcal{E})\\) a measurable space. Then an \\((E, \\mathcal{E})\\)-valued random variable is a measurable function \\(X : \\Omega \\to E\\), which measures that, for every subset \\(B \\in \\mathcal{E}\\), its preimage is \\(\\mathcal{F}\\)-measurable; \\(X^{-1}(B) \\in \\mathcal{F}\\), where \\(X^{-1}(B) = \\{\\omega : X(\\omega) \\in B\\}\\).\n\nJi (2022, Lecture 14)\n\n\n\\begin{algorithm} \\caption{Explore-then-exploit} \\begin{algorithmic} \\Require{$K$ and $T$ (both known), unknown reward distributions $\\mathcal{D}_a$.} \\For{$t = 1, 2, \\ldots$} \\State Explore phase: try each arm $N$ times. \\State Exploit phase: determine $\\tilde{a}$ with the highest average reward; then play $\\tilde{a}$ in all remaining rounds. \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\n\\(N = 2 \\sqrt[3]{T^2 \\log T / K^2}\\)\n\nfrom math import log\nfrom numpy import argmax\n\nclass ExploreExploit:\n    def __init__(self, K, T):\n        self.K = K\n        self.N = round(2 * (T**2 * log(T) / K**2)**(1/3))\n        self.totals = [0] * K\n    \n    def __str__(self):\n        return \"Explore-then-exploit\"\n\n    def play(self, t):\n        '''Choose an arm to play.'''\n        if t &lt;= self.K*self.N:\n            # Play each arm N times\n            return t % self.K\n        else:\n            # Play best arm\n            return self.best\n    \n    def feedback(self, t, a, r):\n        '''Receive a reward.'''\n        if t &lt;= self.K*self.N:\n            # Record reward\n            self.totals[a] += r\n        if t == self.K*self.N:\n            # Compute best arm\n            self.best = argmax(self.totals)\n\n\nfrom math import log\nfrom random import random, randrange\nfrom numpy import argmax, divide, errstate, inf, nan\n\nclass EpsilonGreedy:\n    def __init__(self, K, T):\n        self.K = K\n        self.totals = [0] * K\n        self.counts = [0] * K\n    \n    def __str__(self):\n        return \"Epsilon-greedy\"\n\n    def _eps(self, t):\n        return (self.K*log(t)/t)**(1/3)\n\n    def _best(self):\n        with errstate(divide='ignore', invalid='ignore'):\n            avg = divide(self.totals, self.counts)\n        avg[avg == inf] = -inf\n        avg[avg == nan] = -inf\n        return argmax(avg)\n\n    def play(self, t):\n        '''Choose an arm to play.'''\n        if random() &lt; self._eps(t):\n            # Play random arm\n            return randrange(self.K)\n        else:\n            # Play best arm\n            return self._best()\n    \n    def feedback(self, t, a, r):\n        '''Receive a reward.'''\n        # Record reward\n        self.totals[a] += r\n        self.counts[a] += 1\n\n\nfrom random import random\nfrom numpy.random import normal\nfrom numpy import zeros, cumsum\n\n# Simulation parameters\nT = 1000    # Number of rounds\nF = 10      # Logging frequency\nS = 100     # Number of trials\n\n# Arms to explore\nmeans = [0.5, 0.55]\narms = [random, lambda: normal(means[1], 0.2, 1).item()]\nK = len(arms)\n\n# Algorithms to use\nalgs = [ExploreExploit(K, T), EpsilonGreedy(K, T)]\nA = len(algs)\n\n# Evaluate algorithms\nrewards = zeros((S, T//F+1, A))\nfor s in range(S):\n    for t in range(1, T+1):\n        for i, alg in enumerate(algs):\n            a = alg.play(t)\n            r = arms[a]()\n            alg.feedback(t, a, r)\n            if t % F == 0:\n                rewards[s, t//F, i] = r\nregret = cumsum(max(means)-rewards, axis=1)\n\n\n\nCode for plotting regret\nimport seaborn as sns\nimport pandas as pd\n\n# Transform to a format expected by seaborn\ndf = pd.DataFrame({\n        \"Timestep\": [F*((t//A)%(T//F+1)) for t in range(S*(T//F+1)*A)],\n        \"Cumulative regret\": regret.flatten(),\n        \"Algorithm\": S*(T//F+1)*[str(x) for x in algs],\n    })\n\n# Plot results\nsns.lineplot(data=df, x=\"Timestep\", y=\"Cumulative regret\", hue=\"Algorithm\");\n\n\n\n\n\nFigure 1: Performance of MAB algorithms\n\n\n\n\nJi (2022, Lecture 14)\n\n\n\\begin{algorithm} \\caption{Epsilon-greedy} \\begin{algorithmic} \\Require{$K$ and $T$ (both known), unknown reward distributions $\\mathcal{D}_a$.} \\For{$t = 1, 2, \\ldots$} \\State Toss a coin with success rate $\\epsilon_t$. \\If{success} \\State Explore: choose an arm uniformly at random. \\Else \\State Exploit: choose an arm with the highest average reward so far. \\EndIf \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\nJi (2022, Lecture 15)\n\n\n\\begin{algorithm} \\caption{UCB} \\begin{algorithmic} \\Require{$K$ and $T$ (both known), unknown reward distributions $\\mathcal{D}_a$.} \\For{$t = 1, 2, \\ldots$} \\State $a_t = \\arg\\max_{a \\in [K]} \\mathrm{UCB}_t (a)$ \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\n\n\nReferences\n\nFristedt, Bert E., and Lawrence F. Gray. 1996. A Modern Approach to Probability Theory. Springer Science & Business Media.\n\n\nJi, Bo. 2022. “Online Learning and Sequential Decision Making.” CS6104 Advanced Topics in Theory of Computation."
  },
  {
    "objectID": "2-clustering.html",
    "href": "2-clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Scatterplot matrix code by sklearn\n# Directly taken from:\n# https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html\n\nimport time\nimport warnings\nfrom itertools import cycle, islice\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import cluster, datasets, mixture\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.preprocessing import StandardScaler\n\nfrom ssc import linear, SparseSubspaceClustering\n\n# ============\n# Generate datasets. We choose the size big enough to see the scalability\n# of the algorithms, but not too big to avoid too long running times\n# ============\nn_samples = 200\nseed = 5805\nnoisy_circles = datasets.make_circles(\n    n_samples=n_samples, factor=0.5, noise=0.05, random_state=seed\n)\nnoisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=seed)\nblobs = datasets.make_blobs(n_samples=n_samples, random_state=seed)\nrng = np.random.RandomState(seed)\nno_structure = rng.rand(n_samples, 2), None\n\n# Anisotropicly distributed data\nrandom_state = 5805\nX, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\ntransformation = [[0.6, -0.6], [-0.4, 0.8]]\nX_aniso = np.dot(X, transformation)\naniso = (X_aniso, y)\n\n# blobs with varied variances\nvaried = datasets.make_blobs(\n    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n)\n\n# ============\n# Set up cluster parameters\n# ============\nplt.figure(figsize=(10.5, 6.5))\nplt.subplots_adjust(\n    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.01, hspace=0.01\n)\n\nplot_num = 1\n\ndefault_base = {\n    \"quantile\": 0.3,\n    \"eps\": 0.3,\n    \"damping\": 0.9,\n    \"preference\": -200,\n    \"n_neighbors\": 3,\n    \"n_clusters\": 3,\n    \"min_samples\": 7,\n    \"xi\": 0.05,\n    \"min_cluster_size\": 0.1,\n    \"allow_single_cluster\": True,\n    \"hdbscan_min_cluster_size\": 15,\n    \"hdbscan_min_samples\": 3,\n    \"random_state\": 42,\n}\n\ndatasets = [\n    (\n        noisy_circles,\n        {\n            \"damping\": 0.77,\n            \"preference\": -240,\n            \"quantile\": 0.2,\n            \"n_clusters\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.08,\n        },\n    ),\n    (\n        noisy_moons,\n        {\n            \"damping\": 0.75,\n            \"preference\": -220,\n            \"n_clusters\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.1,\n        },\n    ),\n    (\n        varied,\n        {\n            \"eps\": 0.18,\n            \"n_neighbors\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.01,\n            \"min_cluster_size\": 0.2,\n        },\n    ),\n    (\n        aniso,\n        {\n            \"eps\": 0.15,\n            \"n_neighbors\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.1,\n            \"min_cluster_size\": 0.2,\n        },\n    ),\n    (blobs, {\"min_samples\": 7, \"xi\": 0.1, \"min_cluster_size\": 0.2}),\n    (linear, {}),\n    (no_structure, {}),\n]\n\nfor i_dataset, (dataset, algo_params) in enumerate(datasets):\n    # update parameters with dataset-specific values\n    params = default_base.copy()\n    params.update(algo_params)\n\n    X, y = dataset\n\n    # normalize dataset for easier parameter selection\n    X = StandardScaler().fit_transform(X)\n\n    # estimate bandwidth for mean shift\n    bandwidth = cluster.estimate_bandwidth(X, quantile=params[\"quantile\"])\n\n    # connectivity matrix for structured Ward\n    connectivity = kneighbors_graph(\n        X, n_neighbors=params[\"n_neighbors\"], include_self=False\n    )\n    # make connectivity symmetric\n    connectivity = 0.5 * (connectivity + connectivity.T)\n\n    # ============\n    # Create cluster objects\n    # ============\n    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n    two_means = cluster.MiniBatchKMeans(\n        n_clusters=params[\"n_clusters\"],\n        n_init=\"auto\",\n        random_state=params[\"random_state\"],\n    )\n    ward = cluster.AgglomerativeClustering(\n        n_clusters=params[\"n_clusters\"], linkage=\"ward\", connectivity=connectivity\n    )\n    spectral = cluster.SpectralClustering(\n        n_clusters=params[\"n_clusters\"],\n        eigen_solver=\"arpack\",\n        affinity=\"nearest_neighbors\",\n        random_state=params[\"random_state\"],\n    )\n    dbscan = cluster.DBSCAN(eps=params[\"eps\"])\n    hdbscan = cluster.HDBSCAN(\n        min_samples=params[\"hdbscan_min_samples\"],\n        min_cluster_size=params[\"hdbscan_min_cluster_size\"],\n        allow_single_cluster=params[\"allow_single_cluster\"],\n    )\n    optics = cluster.OPTICS(\n        min_samples=params[\"min_samples\"],\n        xi=params[\"xi\"],\n        min_cluster_size=params[\"min_cluster_size\"],\n    )\n    affinity_propagation = cluster.AffinityPropagation(\n        damping=params[\"damping\"],\n        preference=params[\"preference\"],\n        random_state=params[\"random_state\"],\n    )\n    average_linkage = cluster.AgglomerativeClustering(\n        linkage=\"average\",\n        metric=\"cityblock\",\n        n_clusters=params[\"n_clusters\"],\n        connectivity=connectivity,\n    )\n    birch = cluster.Birch(n_clusters=params[\"n_clusters\"])\n    gmm = mixture.GaussianMixture(\n        n_components=params[\"n_clusters\"],\n        covariance_type=\"full\",\n        random_state=params[\"random_state\"],\n    )\n    ssc = SparseSubspaceClustering(\n        n_clusters=params[\"n_clusters\"],\n    )\n\n    clustering_algorithms = (\n        (\"MiniBatch\\nKMeans\", two_means),\n        (\"Affinity\\nPropagation\", affinity_propagation),\n        (\"MeanShift\", ms),\n        (\"Spectral\\nClustering\", spectral),\n        (\"Ward\", ward),\n        (\"Agglomerative\\nClustering\", average_linkage),\n        (\"DBSCAN\", dbscan),\n        (\"HDBSCAN\", hdbscan),\n        (\"OPTICS\", optics),\n        (\"BIRCH\", birch),\n        (\"Gaussian\\nMixture\", gmm),\n        (\"Sparse\\nSubspace\\nClustering\", ssc),\n    )\n\n    for name, algorithm in clustering_algorithms:\n        t0 = time.time()\n\n        # catch warnings related to kneighbors_graph, subspace clustering\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"the number of connected components of the \"\n                + \"connectivity matrix is [0-9]{1,2}\"\n                + \" &gt; 1. Completing it to avoid stopping the tree early.\",\n                category=UserWarning,\n            )\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"Graph is not fully connected, spectral embedding\"\n                + \" may not work as expected.\",\n                category=UserWarning,\n            )\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"Solution may be inaccurate. Try another solver,\"\n                + \" adjusting the solver settings, or solve with\"\n                + \" verbose=True for more information.\",\n                category=UserWarning,\n            )\n            algorithm.fit(X)\n\n        t1 = time.time()\n        if hasattr(algorithm, \"labels_\"):\n            y_pred = algorithm.labels_.astype(int)\n        else:\n            y_pred = algorithm.predict(X)\n\n        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            plt.title(name, size=9)\n\n        colors = np.array(\n            list(\n                islice(\n                    cycle(\n                        [\n                            \"#377eb8\",\n                            \"#ff7f00\",\n                            \"#4daf4a\",\n                            \"#f781bf\",\n                            \"#a65628\",\n                            \"#984ea3\",\n                            \"#999999\",\n                            \"#e41a1c\",\n                            \"#dede00\",\n                        ]\n                    ),\n                    int(max(y_pred) + 1),\n                )\n            )\n        )\n        # add black color for outliers (if any)\n        colors = np.append(colors, [\"#000000\"])\n        plt.scatter(X[:, 0], X[:, 1], s=5, color=colors[y_pred])\n\n        plt.xlim(-2.5, 2.5)\n        plt.ylim(-2.5, 2.5)\n        plt.xticks(())\n        plt.yticks(())\n        plt.text(\n            0.99,\n            0.01,\n            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n            transform=plt.gca().transAxes,\n            size=7.5,\n            horizontalalignment=\"right\",\n        )\n        plot_num += 1\n\nplt.show()\n\n\n\n\n\nA comparison of various clustering methods.\n\n\n\n\n# The 'X'-looking dataset\n\nlinear = np.concatenate((\n        rng.rand(n_samples//2, 2),                              # □ Square\n        rng.rand(n_samples//4, 2) * 0.05\n            + np.repeat(rng.rand(n_samples//4, 1), 2, axis=1),  # / Diagonal\n        rng.rand(n_samples//4, 2) * 0.05\n            + np.repeat(rng.rand(n_samples//4, 1), 2, axis=1)   # \\ Diagonal\n            * [1, -1] + [0, 1]\n    ), axis=0) - [0.5, 0.5], None\ndef fit(self, Y):\n    Y = Y.T\n    d, n = Y.shape\n    assert d &lt; n\n\n    ## 1. Solve sparse optimization problem\n\n    # Starting form, equation (5)\n    C = Variable((n, n))\n    objective = norm(C,1)\n    constraint = Y @ C\n\n    # Account for outliers\n    if self.use_E:\n        mu_e = np.partition(np.sum(np.abs(Y), axis=0), -2)[-2]\n        l_e = 20 / mu_e\n        E = Variable((d, n))\n        objective += l_e * norm(E,1)\n        constraint += E\n\n    # Account for noise\n    if self.use_Z:\n        G = np.abs(Y.T @ Y)\n        mu_z = np.min(np.max(G - np.diag(np.diag(G)), axis=1))\n        l_z = 800 / mu_z\n        Z = Variable((d, n))\n        objective += l_z/2 * norm(Z,\"fro\")**2\n        constraint += Z\n\n    constraints = [Y == constraint, diag(C) == 0]\n    prob = Problem(Minimize(objective), constraints)\n    try:\n        prob.solve(solver=ECOS)\n\n        ## 2. Normalize the columns of C\n        C = C.value / np.max(np.abs(C.value), axis=0)\n\n        ## 3. Form the weights of a similarity graph\n        W = np.abs(C)\n        W = W + W.T\n\n        ## 4. Apply spectral clustering on the graph\n        self.labels_ = SpectralClustering(\n                n_clusters=self.n_clusters,\n                affinity='precomputed'\n            ).fit_predict(W)\n    except SolverError:\n        self.labels_ = np.zeros(n)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]