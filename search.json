[
  {
    "objectID": "posts/5-anomaly.html",
    "href": "posts/5-anomaly.html",
    "title": "Anomaly/outlier detection",
    "section": "",
    "text": "Image credit: Ajitesh Kumar\n\n\n\n\nRun linear regression on uncorrelated data\nfrom pandas import DataFrame\nfrom numpy.random import normal, random, seed\nimport plotly.express as px\nseed(5805)\n\nn = 100\ndf = DataFrame({'x': normal(size=n-1) + random(n-1), 'y': normal(size=n-1)})\nfig = px.scatter(df, x='x', y='y', trendline='ols').show();\n\n\n\n\n                                                \nFigure 1: Uncorrelated data\n\n\n\n\n\nRun linear regression on data with an outlier\ndf.loc[n-1] = {'x': 10, 'y': 10, 'outlier': True}\nfig = px.scatter(df, x='x', y='y', trendline='ols')\nfig.update_traces(marker=dict(color=[\"#636efa\"] * (n-1) + [\"#EF553B\"]))\nfig.show();\n\n\n\n\n                                                \nFigure 2: Regular OLS\n\n\n\n\\[\nL_\\delta(a) = \\begin{cases}\n    \\frac{1}{2} a^2 & \\text{if } |a| \\leq \\delta\\ ,\\\\\n    \\delta(|a| - \\frac{1}{2} \\delta) & \\text{otherwise}\\ .\n\\end{cases}\n\\tag{1}\\]\n\n\n\nImage credit: Wikipedia\n\n\n\n\nRun linear regression with Huber loss\nfrom numpy import linspace\nimport plotly.graph_objects as go\nfrom sklearn.linear_model import HuberRegressor\n\nfig = px.scatter(df, x='x', y='y')\nfig.update_traces(marker=dict(color=[\"#636efa\"] * (n-1) + [\"#EF553B\"]))\nX = df['x'].values.reshape(-1, 1)\nx_range = linspace(X.min(), X.max(), 100).reshape(-1, 1)\ny = df['y']\n\nmodel = HuberRegressor(epsilon=1)\nmodel.fit(X, y)\ny_fit = model.predict(x_range)\nfig.add_traces(go.Scatter(x=x_range.squeeze(), y=y_fit,\n    showlegend=False, line=dict(color=\"#636efa\"))).show();\n\n\n\n\n                                                \nFigure 3: OLS with Huber loss"
  },
  {
    "objectID": "posts/5-anomaly.html#outliers",
    "href": "posts/5-anomaly.html#outliers",
    "title": "Anomaly/outlier detection",
    "section": "",
    "text": "Image credit: Ajitesh Kumar\n\n\n\n\nRun linear regression on uncorrelated data\nfrom pandas import DataFrame\nfrom numpy.random import normal, random, seed\nimport plotly.express as px\nseed(5805)\n\nn = 100\ndf = DataFrame({'x': normal(size=n-1) + random(n-1), 'y': normal(size=n-1)})\nfig = px.scatter(df, x='x', y='y', trendline='ols').show();\n\n\n\n\n                                                \nFigure 1: Uncorrelated data\n\n\n\n\n\nRun linear regression on data with an outlier\ndf.loc[n-1] = {'x': 10, 'y': 10, 'outlier': True}\nfig = px.scatter(df, x='x', y='y', trendline='ols')\nfig.update_traces(marker=dict(color=[\"#636efa\"] * (n-1) + [\"#EF553B\"]))\nfig.show();\n\n\n\n\n                                                \nFigure 2: Regular OLS\n\n\n\n\\[\nL_\\delta(a) = \\begin{cases}\n    \\frac{1}{2} a^2 & \\text{if } |a| \\leq \\delta\\ ,\\\\\n    \\delta(|a| - \\frac{1}{2} \\delta) & \\text{otherwise}\\ .\n\\end{cases}\n\\tag{1}\\]\n\n\n\nImage credit: Wikipedia\n\n\n\n\nRun linear regression with Huber loss\nfrom numpy import linspace\nimport plotly.graph_objects as go\nfrom sklearn.linear_model import HuberRegressor\n\nfig = px.scatter(df, x='x', y='y')\nfig.update_traces(marker=dict(color=[\"#636efa\"] * (n-1) + [\"#EF553B\"]))\nX = df['x'].values.reshape(-1, 1)\nx_range = linspace(X.min(), X.max(), 100).reshape(-1, 1)\ny = df['y']\n\nmodel = HuberRegressor(epsilon=1)\nmodel.fit(X, y)\ny_fit = model.predict(x_range)\nfig.add_traces(go.Scatter(x=x_range.squeeze(), y=y_fit,\n    showlegend=False, line=dict(color=\"#636efa\"))).show();\n\n\n\n\n                                                \nFigure 3: OLS with Huber loss"
  },
  {
    "objectID": "posts/5-anomaly.html#changepoint-detection",
    "href": "posts/5-anomaly.html#changepoint-detection",
    "title": "Anomaly/outlier detection",
    "section": "Changepoint detection",
    "text": "Changepoint detection\n\n\nLoad and display the Nile dataset\nimport statsmodels.api as sm\nimport plotly.express as px\n\n# Time series data\nnile = sm.datasets.get_rdataset(\"Nile\").data\nfig = px.line(nile, x=\"time\", y=\"value\")\n\n# Ground truth change point\nfig.add_vline(x=1899, line_width=2, line_dash=\"dash\", line_color=\"black\")\n\n\n\n\n                                                \nFigure 4: Yearly volume of the Nile river at Aswan. A dam was built in 1898.\n\n\n\n\nCUSUM\nPage (1954)\nMATLAB has a built-in cusum function\n\n\nA visual example of changepoint detection\nfrom numpy import concatenate, cumsum, mean\n\n# Simulate coin flipping\nn, t = [75, 25], 50\np0, p1 = 0.45, 0.75\ncoin = concatenate((random((n[0], t)) &lt; p0, random((n[1], t)) &lt; p1))\ncumul = mean(cumsum(2*coin-1, axis=0), axis=1)\n\n# Display accumulated counts\navg0 = (2*p0 - 1) * n[0]\navg1 = (2*p1 - 1) * n[1] + avg0\nfig = px.scatter(y=cumul, labels={'x': 'Flip count', 'y': \"Accumulated (Heads - Tails)\"})\nfig.add_traces(go.Scatter(x=[n[0],n[0]+n[1]], y=[avg0, avg1],\n    mode=\"lines\", name=\"Changed\", line=dict(dash='dash')))\nfig.add_traces(go.Scatter(x=[0,n[0]], y=[0, avg0], mode=\"lines\", name=\"Original\"))\n\n\n\n\n                                                \nFigure 5: Cumulative outcomes of flipping a coin with changing probabilities.\n\n\n\n\\(Z_n = (X_n - \\bar{x}) / s\\)\n\\[\n\\begin{align}\n    H_n = \\max(0, H_{n-1} + Z_n - \\omega)\\ ,\\\\\n    L_n = \\min(0, L_{n-1} + Z_n + \\omega)\\ ,\n\\end{align}\n\\tag{2}\\]\nwhere we set \\(H_0 = L_0 = 0\\).\n\n\nImplementation of the CUSUM algorithm\nimport numpy as np\n\ndef cusum(X, lim=5, w=0, mean=None, std=None, plot=True):\n    '''Detect changes in mean using the CUSUM test.\n\n    ARGUMENTS:\n    lim: number of standard deviations in drift to be detected.\n    w: number of standard devitaions as damping coefficient.\n    mean: expected mean or None to compute from 25 leading samples.\n    std: expected standard deviation or None to compute from 25 leading samples.\n    plot: whether to display a visualization.\n\n    RETURNS:\n    index of the first detected change or None.\n    '''\n    # Estimate mean and standard deviation\n    if mean is None:\n        mean = np.mean(X[:25])\n    if std is None:\n        std = np.mean(X[:25])\n    Z = (X - mean) / std\n\n    n = len(Z)\n    H = np.zeros(n)\n    L = np.zeros(n)\n    for i, z in enumerate(Z):\n        H[i] = max(0, H[i-1] + z - w)\n        L[i] = min(0, L[i-1] + z + w)\n\n    idx_H = np.argmax(np.append(H, lim) &gt;= lim)\n    idx_L = np.argmax(np.append(L, -lim) &lt;= -lim)\n    idx = min(idx_H, idx_L)\n\n    if plot:\n        fig = go.Figure()\n        fig.add_traces(go.Scatter(y=H, name=\"Upper cumulative sum\"))\n        fig.add_traces(go.Scatter(y=L, name=\"Lower cumulative sum\"))\n        fig.add_hline(y=lim, line_dash='dash', line_color=\"#636efa\")\n        fig.add_hline(y=-lim, line_dash='dash', line_color=\"#EF553B\")\n        if idx &lt; n:\n            HL = H if idx_H &lt;= idx_L else L\n            fig.add_traces(go.Scatter(x=[idx], y=[HL[idx]],\n                mode=\"markers\", marker_symbol='circle-open', name=\"Changepoint\",\n                marker=dict(size=12, line_width=4, color='black')))\n        fig.show()\n\n    return idx if idx &lt; n else None\n\nchange = cusum(nile['value'], lim=2)\nprint(f\"Detected changepoint at year {nile['time'][change]}.\")\n\n\n\n\n                                                \nFigure 6: CUSUM control chart.\n\n\n\nDetected changepoint at year 1905."
  },
  {
    "objectID": "posts/3-regression.html",
    "href": "posts/3-regression.html",
    "title": "Linear and nonlinear regression",
    "section": "",
    "text": "glm(y ~ x, ...) or glm(y ~ ., ...)\n\n\nCreate artificial data\nimport plotly.express as px\nfrom numpy.random import rand, normal, seed\nfrom numpy import repeat, concatenate, power\nfrom pandas import DataFrame\nseed(5805)\n\n# Create artificial data\nn, r, s, b = 100, 100, 5, 10\nlin = repeat(rand(n, 1) * r, 2, axis=1) + normal(scale=s, size=(n, 2))\nexp = power(b, lin[:,1:2] / r) * r / b\nexp = concatenate((lin[:,0:1], exp), axis=1)\n\n# Plot data\nlin_df = DataFrame({'x': lin[:,0], 'y': lin[:,1]})\nexp_df = DataFrame({'x': exp[:,0], 'y': exp[:,1]})\npx.scatter(lin_df, x='x', y='y').show()\npx.scatter(exp_df, x='x', y='y').show();\n\n\n\n\n\n\n\n                                                \n(a) Linear growth\n\n\n\n\n\n                                                \n(b) Exponential growth\n\n\n\nFigure 1: A couple of artificial datasets"
  },
  {
    "objectID": "posts/3-regression.html#regression",
    "href": "posts/3-regression.html#regression",
    "title": "Linear and nonlinear regression",
    "section": "",
    "text": "glm(y ~ x, ...) or glm(y ~ ., ...)\n\n\nCreate artificial data\nimport plotly.express as px\nfrom numpy.random import rand, normal, seed\nfrom numpy import repeat, concatenate, power\nfrom pandas import DataFrame\nseed(5805)\n\n# Create artificial data\nn, r, s, b = 100, 100, 5, 10\nlin = repeat(rand(n, 1) * r, 2, axis=1) + normal(scale=s, size=(n, 2))\nexp = power(b, lin[:,1:2] / r) * r / b\nexp = concatenate((lin[:,0:1], exp), axis=1)\n\n# Plot data\nlin_df = DataFrame({'x': lin[:,0], 'y': lin[:,1]})\nexp_df = DataFrame({'x': exp[:,0], 'y': exp[:,1]})\npx.scatter(lin_df, x='x', y='y').show()\npx.scatter(exp_df, x='x', y='y').show();\n\n\n\n\n\n\n\n                                                \n(a) Linear growth\n\n\n\n\n\n                                                \n(b) Exponential growth\n\n\n\nFigure 1: A couple of artificial datasets"
  },
  {
    "objectID": "posts/3-regression.html#linear-regression",
    "href": "posts/3-regression.html#linear-regression",
    "title": "Linear and nonlinear regression",
    "section": "Linear regression",
    "text": "Linear regression\n\n\n\nImage credit: Wikipedia\n\n\n\n\nCode for polynomial OLS\n# Taken from https://plotly.com/python/ml-regression/\nfrom numpy import linspace\nimport plotly.graph_objects as go\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\ndef plot(df, trend, **kwargs):\n    fig = px.scatter(df, x='x', y='y')\n    X = df['x'].values.reshape(-1, 1)\n    x_range = linspace(X.min(), X.max(), 100).reshape(-1, 1)\n    y = df['y']\n\n    y_fit = trend(X, x_range, y, **kwargs)\n    fig.add_traces(go.Scatter(x=x_range.squeeze(), y=y_fit,\n        showlegend=False, line=dict(color='black')))\n    return fig\n\ndef poly(X, x_range, y, degree=2):\n    poly = PolynomialFeatures(degree)\n    poly.fit(X)\n    X_poly = poly.transform(X)\n    x_range_poly = poly.transform(x_range)\n\n    model = LinearRegression(fit_intercept=False)\n    model.fit(X_poly, y)\n    return model.predict(x_range_poly)\n\ndef show(fig):\n    fig.update_layout(\n            margin=dict(l=0, r=20, t=20, b=20),\n            height=380\n        ).show()\n\n\n\n\nFit trendlines via ordinary least squares\nshow(px.scatter(lin_df, x='x', y='y', trendline='ols',\n    trendline_color_override='black'))\nshow(plot(exp_df, poly, degree=2))\nshow(px.scatter(exp_df, x='x', y='y', trendline='ols',\n    trendline_options=dict(log_y=True),\n    trendline_color_override='black'))\n\n\n\n\n\n\n\n                                                \n(a) Ordinary least squares\n\n\n\n\n\n                                                \n(b) OLS of y ~ x^2 + x\n\n\n\n\n\n                                                \n(c) OLS of log(y) ~ x\n\n\n\nFigure 2: Trendlines via ordinary least squares\n\n\n\n\n\nCode for KNN regression\n# Taken from https://plotly.com/python/ml-regression/\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsRegressor\n\ndef knn(X, x_range, y, k=20, weights='uniform'):\n    knn_uni = KNeighborsRegressor(10, weights=weights)\n    knn_uni.fit(X, y)\n    return knn_uni.predict(x_range.reshape(-1, 1))\n\n\n\n\nFit trendlines via miscellaneous methods\nfrom scipy.optimize import curve_fit\n\n# Nonlinear regression\nf = lambda x, a, b, c: a * x / (b + x) + c\ndef nonlinear(X, x_range, y, f, init=None):\n    args, _ = curve_fit(f, exp_df['x'], exp_df['y'], p0=init)\n    return f(x_range, *args).squeeze()\nshow(plot(exp_df, nonlinear, f=f, init=[-90, -200, 10]))\n\n# Non-parametric regression\nshow(plot(exp_df, knn, k=20))\nshow(px.scatter(exp_df, x='x', y='y', trendline='lowess',\n   trendline_color_override='black'))\n\n\n\n\n\n\n\n                                                \n(a) Nonlinear least squares\n\n\n\n\n\n                                                \n(b) KNN Regression\n\n\n\n\n\n                                                \n(c) LOWESS\n\n\n\nFigure 3: Trendlines via other regression methods\n\n\n\n\n\nFit a multiratiate regression model\n# Taken from https://plotly.com/python/ml-regression/\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom sklearn.svm import SVR\n\nmesh_size = .02\nmargin = 0\n\ndf = px.data.iris()\n\nX = df[['sepal_width', 'sepal_length']]\ny = df['petal_width']\n\n# Condition the model on sepal width and length, predict the petal width\nmodel = SVR(C=1.)\nmodel.fit(X, y)\n\n# Create a mesh grid on which we will run our model\nx_min, x_max = X.sepal_width.min() - margin, X.sepal_width.max() + margin\ny_min, y_max = X.sepal_length.min() - margin, X.sepal_length.max() + margin\nxrange = np.arange(x_min, x_max, mesh_size)\nyrange = np.arange(y_min, y_max, mesh_size)\nxx, yy = np.meshgrid(xrange, yrange)\n\n# Run model\npred = model.predict(np.c_[xx.ravel(), yy.ravel()])\npred = pred.reshape(xx.shape)\n\n# Generate the plot\nfig = px.scatter_3d(df, x='sepal_width', y='sepal_length', z='petal_width')\nfig.update_traces(marker=dict(size=5))\nfig.add_traces(go.Surface(x=xrange, y=yrange, z=pred, name='pred_surface', opacity=0.75))\nfig.show()\n\n\n\n\n                                                \nFigure 4: Support Vector Regression on the Iris dataset"
  },
  {
    "objectID": "posts/1-probability.html",
    "href": "posts/1-probability.html",
    "title": "Probability theory and random variables",
    "section": "",
    "text": "Here’s the definition; taken from Wikipedia, which in order takes it from Fristedt and Gray (1996, 11).\n\nDefinition 1 (Random variable). Let \\((\\Omega, \\mathcal{F}, P)\\) be a probability space and \\((E, \\mathcal{E})\\) a measurable space. Then an \\((E, \\mathcal{E})\\)-valued random variable is a measurable function \\(X : \\Omega \\to E\\), which measures that, for every subset \\(B \\in \\mathcal{E}\\), its preimage is \\(\\mathcal{F}\\)-measurable; \\(X^{-1}(B) \\in \\mathcal{F}\\), where \\(X^{-1}(B) = \\{\\omega : X(\\omega) \\in B\\}\\)."
  },
  {
    "objectID": "posts/1-probability.html#random-variables",
    "href": "posts/1-probability.html#random-variables",
    "title": "Probability theory and random variables",
    "section": "",
    "text": "Here’s the definition; taken from Wikipedia, which in order takes it from Fristedt and Gray (1996, 11).\n\nDefinition 1 (Random variable). Let \\((\\Omega, \\mathcal{F}, P)\\) be a probability space and \\((E, \\mathcal{E})\\) a measurable space. Then an \\((E, \\mathcal{E})\\)-valued random variable is a measurable function \\(X : \\Omega \\to E\\), which measures that, for every subset \\(B \\in \\mathcal{E}\\), its preimage is \\(\\mathcal{F}\\)-measurable; \\(X^{-1}(B) \\in \\mathcal{F}\\), where \\(X^{-1}(B) = \\{\\omega : X(\\omega) \\in B\\}\\)."
  },
  {
    "objectID": "posts/1-probability.html#concentration-bounds",
    "href": "posts/1-probability.html#concentration-bounds",
    "title": "Probability theory and random variables",
    "section": "Concentration bounds",
    "text": "Concentration bounds\n\nHoeffding’s inequality\n\n\nExample"
  },
  {
    "objectID": "posts/1-probability.html#multi-armed-bandits",
    "href": "posts/1-probability.html#multi-armed-bandits",
    "title": "Probability theory and random variables",
    "section": "Multi-armed bandits",
    "text": "Multi-armed bandits\nBased on Slivkins (2022)\n\n\n\\begin{algorithm} \\caption{Stochastic MAB (Framework)} \\begin{algorithmic} \\Require{$K$ and $T$ (both known), unknown reward distributions $\\mathcal{D}_a$.} \\For{$t = 1, 2, \\ldots$} \\State Choose an action $a_t \\in [K]$. \\State Suffer loss $z_t[a_t]$ and also only observe $z_t[a_t]$. \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\nHere, \\(z_t\\) is a random vector sampled from the unknown distributions, i.e., \\(z_t[a] \\sim \\mathcal{D}_a\\).\n\nExplore-then-exploit\nJi (2022, Lecture 14)\n\n\n\\begin{algorithm} \\caption{Explore-then-exploit} \\begin{algorithmic} \\Require{$K$ and $T$ (both known), unknown reward distributions $\\mathcal{D}_a$.} \\For{$t = 1, 2, \\ldots$} \\State Explore phase: try each arm $N$ times. \\State Exploit phase: determine $\\tilde{a}$ with the highest average reward; then play $\\tilde{a}$ in all remaining rounds. \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\n\\(N = 2 \\sqrt[3]{T^2 \\log T / K^2}\\)\n\n\nCode for Explore-then-exploit\nfrom math import log\nfrom numpy import argmax\n\nclass ExploreExploit:\n    def __init__(self, K, T):\n        self.K = K\n        self.N = round(2 * (T**2 * log(T) / K**2)**(1/3))\n        self.totals = [0] * K\n    \n    def __str__(self):\n        return \"Explore-then-exploit\"\n\n    def play(self, t):\n        '''Choose an arm to play.'''\n        if t &lt;= self.K*self.N:\n            # Play each arm N times\n            return t % self.K\n        else:\n            # Play best arm\n            return self.best\n    \n    def feedback(self, t, a, r):\n        '''Receive a reward.'''\n        if t &lt;= self.K*self.N:\n            # Record reward\n            self.totals[a] += r\n        if t == self.K*self.N:\n            # Compute best arm\n            self.best = argmax(self.totals)\n\n\n\n\nEpsilon-greedy\nJi (2022, Lecture 14)\n\n\n\\begin{algorithm} \\caption{Epsilon-greedy} \\begin{algorithmic} \\Require{$K$ and $T$ (both known), unknown reward distributions $\\mathcal{D}_a$.} \\For{$t = 1, 2, \\ldots$} \\State Toss a coin with success rate $\\epsilon_t$. \\If{success} \\State Explore: choose an arm uniformly at random. \\Else \\State Exploit: choose an arm with the highest average reward so far. \\EndIf \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\n\n\nCode for Epsilon-greedy\nfrom math import log\nfrom random import random, randrange\nfrom numpy import argmax, divide, errstate, inf, nan\n\nclass EpsilonGreedy:\n    def __init__(self, K, _):\n        self.K = K\n        self.totals = [0] * K\n        self.counts = [0] * K\n    \n    def __str__(self):\n        return \"Epsilon-greedy\"\n\n    def _eps(self, t):\n        return (self.K*log(t)/t)**(1/3)\n\n    def _best(self):\n        with errstate(divide='ignore', invalid='ignore'):\n            avg = divide(self.totals, self.counts)\n        avg[avg == inf] = -inf\n        avg[avg == nan] = -inf\n        return argmax(avg)\n\n    def play(self, t):\n        '''Choose an arm to play.'''\n        if random() &lt; self._eps(t):\n            # Play random arm\n            return randrange(self.K)\n        else:\n            # Play best arm\n            return self._best()\n    \n    def feedback(self, t, a, r):\n        '''Receive a reward.'''\n        self.totals[a] += r\n        self.counts[a] += 1\n\n\n\n\nSuccessive elimination\nJi (2022, Lecture 14)\n\n\n\\begin{algorithm} \\caption{Successive Elimination} \\begin{algorithmic} \\Require{$K$ and $T$ (both known), unknown reward distributions $\\mathcal{D}_a$.} \\State Initialize active arm set $\\mathcal{A}_1 = [K]$. \\For{$p = 1, 2, \\ldots$} \\State Play each arm in $\\mathcal{A}_p$ once. \\State Let $t$ be the time at the end of the current phase $p$. \\State $\\mathcal{A}_{p+1} = \\{ a \\in \\mathcal{A}_p : \\mathrm{UCB}_t (a) \\geq \\max_{a' \\in \\mathcal{A}_p} \\mathrm{LCB}_t (a) \\}$ \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\n\n\nCode for Successive elimination\nfrom random import random, randrange\nfrom numpy import argmax, divide, errstate, inf, nan, log, sqrt\nimport numpy as np\n\nclass SuccessiveElimination:\n    def __init__(self, K, T, delta=1500):\n        self.num = log(2 * K * T / delta) / 2\n        assert self.num &gt; 0, \"delta too large\"\n        self.totals = [0] * K\n        self.counts = [0] * K\n        self.A = list(range(K))\n        self.idx = 0\n    \n    def __str__(self):\n        return \"Successive elimination\"\n\n    def _mu(self):\n        with errstate(divide='ignore', invalid='ignore'):\n            mu = divide(self.totals, self.counts)\n        mu[mu == inf] = 0\n        mu[mu == nan] = 0\n        return mu\n\n    def _beta(self):\n        with errstate(divide='ignore'):\n            beta = sqrt(divide(self.num, self.counts))\n        return beta\n\n    def play(self, t):\n        '''Choose an arm to play.'''\n        if self.idx == len(self.A):\n            # Update arm set A\n            u, b = self._mu(), self._beta()\n            lcb, ucb = u-b, u+b\n            self.A = [a for a in self.A if ucb[a] &gt;= np.max(lcb)]\n            self.idx = 0\n        # Play each arm in A\n        a = self.A[self.idx]\n        self.idx += 1\n        return a\n    \n    def feedback(self, t, a, r):\n        '''Receive a reward.'''\n        self.totals[a] += r\n        self.counts[a] += 1\n\n\n\n\nUCB\nJi (2022, Lecture 15)\n\n\n\\begin{algorithm} \\caption{UCB} \\begin{algorithmic} \\Require{$K$ and $T$ (both known), unknown reward distributions $\\mathcal{D}_a$.} \\For{$t = 1, 2, \\ldots$} \\State $a_t = \\arg\\max_{a \\in [K]} \\mathrm{UCB}_t (a)$ \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\n\n\nCode for UCB\nfrom random import random, randrange\nfrom numpy import argmax, divide, errstate, inf, nan, log, sqrt\nimport numpy as np\n\nclass UCB:\n    def __init__(self, K, T, delta=1):\n        self.num = log(2 * K * T / delta) / 2\n        assert self.num &gt; 0, \"delta too large\"\n        self.totals = [0] * K\n        self.counts = [0] * K\n    \n    def __str__(self):\n        return \"UCB\"\n\n    def _mu(self):\n        with errstate(divide='ignore', invalid='ignore'):\n            mu = divide(self.totals, self.counts)\n        mu[mu == inf] = 0\n        mu[mu == nan] = 0\n        return mu\n\n    def _beta(self):\n        with errstate(divide='ignore'):\n            beta = sqrt(divide(self.num, self.counts))\n        return beta\n\n    def play(self, t):\n        '''Choose an arm to play.'''\n        ucb = self._mu() + self._beta()\n        return argmax(ucb)\n    \n    def feedback(self, t, a, r):\n        '''Receive a reward.'''\n        self.totals[a] += r\n        self.counts[a] += 1\n\n\n\n\nSimulation\n\nTechnical note\n\n\nSeed random number generators\nfrom random import seed as py_seed\nfrom numpy.random import seed as np_seed\npy_seed(5805)\nnp_seed(5805)\n\n\nWelford’s online algorithm\n\n\nSimulation code\nfrom random import random\nfrom numpy.random import normal\nfrom numpy import zeros, sqrt\n\n# Simulation parameters\nT = 1000    # Number of rounds\nF = 10      # Logging frequency\nN = 100     # Number of trials\n\n# Arms to explore\nmeans = [0.5, 0.6]\nbest = max(means)\narms = [random, lambda: normal(means[1], 0.2, 1).item()]\nK = len(arms)\n\n# Algorithms to use\ncs = [ExploreExploit, EpsilonGreedy, SuccessiveElimination, UCB]\nA = len(cs)\n\n# Evaluate algorithms\nx = zeros((T//F+1, A))\ns = zeros((T//F+1, A))\nfor n in range(1, N+1):\n    for i, c in enumerate(cs):\n        alg = c(K, T)\n        rgt = 0\n        for t in range(1, T+1):\n            a = alg.play(t)\n            r = arms[a]()\n            alg.feedback(t, a, r)\n            rgt += best - r\n            if t % F == 0:\n                # Welford's online algorithm\n                xn1 = x[t//F, i]\n                x[t//F, i] = ((n-1)*xn1 + rgt) / n\n                s[t//F, i] += (rgt - xn1) * (rgt - x[t//F, i])\ns = 0.2*sqrt(s / (N-1))\n\n\n\n\nCode for plotting regret\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nt = np.arange(1, T+2, F)\nplt.plot(t, x)\nfor i in range(A):\n    plt.fill_between(t, x[:,i]-s[:,i], x[:,i]+s[:,i], alpha=.15)\nplt.xlabel(\"Timestep\")\nplt.ylabel(\"Regret\")\nplt.legend([c(K, T) for c in cs]);\n\n\n\n\n\nFigure 1: Performance of MAB algorithms"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Alice Didenkova",
    "section": "",
    "text": "A graduate student at Virginia Tech."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Alice Didenkova",
    "section": "Education",
    "text": "Education\nVirginia Tech | Blacksburg, VA\nPh.D. Candidate in Computer Science | 2022 - present\nVirginia Tech | Blacksburg, VA\nB.S. in Computer Science, Mathematics | 2019 - 2022"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Alice Didenkova",
    "section": "Experience",
    "text": "Experience\nFujitsu Research of America | Research Intern in Quantum Computing | Summer 2023\nCS 3214: Computer Systems | Teaching Assistant | 2021 - present"
  },
  {
    "objectID": "about.html#topics",
    "href": "about.html#topics",
    "title": "Alice Didenkova",
    "section": "Topics",
    "text": "Topics\nMy interests and current research topics include:\n\n⚛️ Quantum computing\n🤖 Reinforcement learning\n\nDisclaimer: this blog has been created to fulfill the Final Project requirements of CS 5805: Machine Learning."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Probability theory and random variables\n\n\nConcentration bounds and multi-armed bandits\n\n\n\n\nTODO\n\n\n\n\nTODO\n\n\n\n\n\n\nDec 1, 2023\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\nSparse subspace clustering\n\n\n\n\nTODO\n\n\n\n\nTODO\n\n\n\n\n\n\nDec 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nLinear and nonlinear regression\n\n\n\n\n\n\n\nTODO\n\n\n\n\nTODO\n\n\n\n\n\n\nDec 3, 2023\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\nTODO\n\n\n\n\nTODO\n\n\n\n\n\n\nDec 4, 2023\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly/outlier detection\n\n\nChangepoint detection\n\n\n\n\nTODO\n\n\n\n\nTODO\n\n\n\n\n\n\nDec 5, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2-clustering.html",
    "href": "posts/2-clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Falling within the realm of unsupervised learning, this task deals with unlabeled but adequately distinct data. While both it and classification involve modeling such differences, clustering in particular starts with no prior informaiton about data labels. In cases where partial label information is known, the task is instead considered to be semi-supervised.\nThe common goal of clustering is to separate data into clusters - groups such that data within them is similar and between them is different. What this means precisely depends on the measure of similarity used; for raw Euclidean distances it would mean that clusters are far apart literally, whereas for feature embedding spaces they would be apart semantically/conceptually.\nClustering serves a variety of purposes:\n\nUnderstanding: separating data into distinct groups may reveal patterns and highlight important features. Additionally, clustering may be viewed as a form of divide-and-conquer where the clusters may be easier to analyze in isolation.\nPreprocessing: similar to the above, the benefit of being able to analyze data separately can be extended to programmatic solutions. That is, cluster assignment can be used to split data and train models separately for each.\nAnomaly detection: the above would additionally get rid of outliers as they wouldn’t belong to any cluster. This makes clustering applicable to anomaly detection as it can identify points that are distant from all other data.\n\nClustering in itself isn’t a method but a task. A plethora of algorithms exist to perform it, thanks to various relevant considerations about the type of clustered data:\n\nShape: while we may typically think of clusters as circular-like objects, they may instead be stretched, spread out, stretched out, bent, etc. For the stretched out type, it additionally becomes important whether data exhibits linear relationships or otherwise lies in nonlinear manifolds.\nData type: the same considerations from other tasks in machine learning apply: is the data numerical or categorical, are any transforms applicable, is some of the data images/audio/text/etc?\nDimensionality: for more complex data types, preprocessing or embedding may be in order. High-dimensional data in general poses a challenge due to the curse of dimensionality, a phenomenon where regular measures of distance break down and lose meaning. In some circumstances it thus becomes crucial to use methods specifically designed around high-dimensional data.\n\n\n\nTo illustrate the former consideration particularly, we can take a look at some distinct sets of artificial data and the performances of various clustering algorithms. Here’s a pretty nice visualization of some common algorithms:\n\n\nClustering scatterplot matrix code by sklearn\nimport time\nimport warnings\nfrom itertools import cycle, islice\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import cluster, datasets, mixture\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.preprocessing import StandardScaler\n\nfrom ssc import linear, SparseSubspaceClustering\n\n# ============\n# Generate datasets. We choose the size big enough to see the scalability\n# of the algorithms, but not too big to avoid too long running times\n# ============\nn_samples = 200\nseed = 5805\nnoisy_circles = datasets.make_circles(\n    n_samples=n_samples, factor=0.5, noise=0.05, random_state=seed\n)\nnoisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=seed)\nblobs = datasets.make_blobs(n_samples=n_samples, random_state=seed)\nrng = np.random.RandomState(seed)\nno_structure = rng.rand(n_samples, 2), None\n\n# Anisotropicly distributed data\nrandom_state = 5805\nX, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\ntransformation = [[0.6, -0.6], [-0.4, 0.8]]\nX_aniso = np.dot(X, transformation)\naniso = (X_aniso, y)\n\n# blobs with varied variances\nvaried = datasets.make_blobs(\n    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n)\n\n# ============\n# Set up cluster parameters\n# ============\nplt.figure(figsize=(10.5, 6.5))\nplt.subplots_adjust(\n    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.01, hspace=0.01\n)\n\nplot_num = 1\n\ndefault_base = {\n    \"quantile\": 0.3,\n    \"eps\": 0.3,\n    \"damping\": 0.9,\n    \"preference\": -200,\n    \"n_neighbors\": 3,\n    \"n_clusters\": 3,\n    \"min_samples\": 7,\n    \"xi\": 0.05,\n    \"min_cluster_size\": 0.1,\n    \"allow_single_cluster\": True,\n    \"hdbscan_min_cluster_size\": 15,\n    \"hdbscan_min_samples\": 3,\n    \"random_state\": 42,\n}\n\ndatasets = [\n    (\n        noisy_circles,\n        {\n            \"damping\": 0.77,\n            \"preference\": -240,\n            \"quantile\": 0.2,\n            \"n_clusters\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.08,\n        },\n    ),\n    (\n        noisy_moons,\n        {\n            \"damping\": 0.75,\n            \"preference\": -220,\n            \"n_clusters\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.1,\n        },\n    ),\n    (\n        varied,\n        {\n            \"eps\": 0.18,\n            \"n_neighbors\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.01,\n            \"min_cluster_size\": 0.2,\n        },\n    ),\n    (\n        aniso,\n        {\n            \"eps\": 0.15,\n            \"n_neighbors\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.1,\n            \"min_cluster_size\": 0.2,\n        },\n    ),\n    (blobs, {\"min_samples\": 7, \"xi\": 0.1, \"min_cluster_size\": 0.2}),\n    (linear, {}),\n    (no_structure, {}),\n]\n\nfor i_dataset, (dataset, algo_params) in enumerate(datasets):\n    # update parameters with dataset-specific values\n    params = default_base.copy()\n    params.update(algo_params)\n\n    X, y = dataset\n\n    # normalize dataset for easier parameter selection\n    X = StandardScaler().fit_transform(X)\n\n    # estimate bandwidth for mean shift\n    bandwidth = cluster.estimate_bandwidth(X, quantile=params[\"quantile\"])\n\n    # connectivity matrix for structured Ward\n    connectivity = kneighbors_graph(\n        X, n_neighbors=params[\"n_neighbors\"], include_self=False\n    )\n    # make connectivity symmetric\n    connectivity = 0.5 * (connectivity + connectivity.T)\n\n    # ============\n    # Create cluster objects\n    # ============\n    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n    two_means = cluster.MiniBatchKMeans(\n        n_clusters=params[\"n_clusters\"],\n        n_init=\"auto\",\n        random_state=params[\"random_state\"],\n    )\n    ward = cluster.AgglomerativeClustering(\n        n_clusters=params[\"n_clusters\"], linkage=\"ward\", connectivity=connectivity\n    )\n    spectral = cluster.SpectralClustering(\n        n_clusters=params[\"n_clusters\"],\n        eigen_solver=\"arpack\",\n        affinity=\"nearest_neighbors\",\n        random_state=params[\"random_state\"],\n    )\n    dbscan = cluster.DBSCAN(eps=params[\"eps\"])\n    hdbscan = cluster.HDBSCAN(\n        min_samples=params[\"hdbscan_min_samples\"],\n        min_cluster_size=params[\"hdbscan_min_cluster_size\"],\n        allow_single_cluster=params[\"allow_single_cluster\"],\n    )\n    optics = cluster.OPTICS(\n        min_samples=params[\"min_samples\"],\n        xi=params[\"xi\"],\n        min_cluster_size=params[\"min_cluster_size\"],\n    )\n    affinity_propagation = cluster.AffinityPropagation(\n        damping=params[\"damping\"],\n        preference=params[\"preference\"],\n        random_state=params[\"random_state\"],\n    )\n    average_linkage = cluster.AgglomerativeClustering(\n        linkage=\"average\",\n        metric=\"cityblock\",\n        n_clusters=params[\"n_clusters\"],\n        connectivity=connectivity,\n    )\n    birch = cluster.Birch(n_clusters=params[\"n_clusters\"])\n    gmm = mixture.GaussianMixture(\n        n_components=params[\"n_clusters\"],\n        covariance_type=\"full\",\n        random_state=params[\"random_state\"],\n    )\n    ssc = SparseSubspaceClustering(\n        n_clusters=params[\"n_clusters\"],\n    )\n\n    clustering_algorithms = (\n        (\"MiniBatch\\nKMeans\", two_means),\n        (\"Affinity\\nPropagation\", affinity_propagation),\n        (\"MeanShift\", ms),\n        (\"Spectral\\nClustering\", spectral),\n        (\"Ward\", ward),\n        (\"Agglomerative\\nClustering\", average_linkage),\n        (\"DBSCAN\", dbscan),\n        (\"HDBSCAN\", hdbscan),\n        (\"OPTICS\", optics),\n        (\"BIRCH\", birch),\n        (\"Gaussian\\nMixture\", gmm),\n        (\"Sparse\\nSubspace\\nClustering\", ssc),\n    )\n\n    for name, algorithm in clustering_algorithms:\n        t0 = time.time()\n\n        # catch warnings related to kneighbors_graph, subspace clustering\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"the number of connected components of the \"\n                + \"connectivity matrix is [0-9]{1,2}\"\n                + \" &gt; 1. Completing it to avoid stopping the tree early.\",\n                category=UserWarning,\n            )\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"Graph is not fully connected, spectral embedding\"\n                + \" may not work as expected.\",\n                category=UserWarning,\n            )\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"Solution may be inaccurate. Try another solver,\"\n                + \" adjusting the solver settings, or solve with\"\n                + \" verbose=True for more information.\",\n                category=UserWarning,\n            )\n            algorithm.fit(X)\n\n        t1 = time.time()\n        if hasattr(algorithm, \"labels_\"):\n            y_pred = algorithm.labels_.astype(int)\n        else:\n            y_pred = algorithm.predict(X)\n\n        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            plt.title(name, size=9)\n\n        colors = np.array(\n            list(\n                islice(\n                    cycle(\n                        [\n                            \"#377eb8\",\n                            \"#ff7f00\",\n                            \"#4daf4a\",\n                            \"#f781bf\",\n                            \"#a65628\",\n                            \"#984ea3\",\n                            \"#999999\",\n                            \"#e41a1c\",\n                            \"#dede00\",\n                        ]\n                    ),\n                    int(max(y_pred) + 1),\n                )\n            )\n        )\n        # add black color for outliers (if any)\n        colors = np.append(colors, [\"#000000\"])\n        plt.scatter(X[:, 0], X[:, 1], s=5, color=colors[y_pred])\n\n        plt.xlim(-2.5, 2.5)\n        plt.ylim(-2.5, 2.5)\n        plt.xticks(())\n        plt.yticks(())\n        plt.text(\n            0.99,\n            0.01,\n            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n            transform=plt.gca().transAxes,\n            size=7.5,\n            horizontalalignment=\"right\",\n        )\n        plot_num += 1\n\nplt.show()\n\n\n\n\n\nFigure 1: A comparison of various clustering methods.\n\n\n\n\nWhoa, that’s a lot of them! If you’ve seen this image before and are particularly observant, you may notice that I threw in an extra algorithm (rightmost column) and a dataset (second-bottom row) - more on those later.\nEach of the above algorithms is deserving of a post of its own. Here, I will just briefly focus on one, which happens to be the only showcased algorithm that properly clusters the top 5 datasets:\n\n\n\nYes, spectral as in spectrum - i.e., eigenvalues. I found out about this algorithm back when I was taking intro to linear algebra, and it really stuck with me precisely because it gives a pretty neat interpretation of eigenvalues (which at the time I had a really hard time exemplifying).\nWe start with a similarity matrix \\(S\\), where the \\(ij\\)-th value denotes the similarity of data points indexed \\(i\\) and \\(j\\). What constitutes as being similar may vary; the example in Figure 1 simply uses Euclidean distance. We then use \\(S\\) to construct a similarity graph, a weighted graph with weights \\(w_{i,j} = s_{i,j}\\) if they are past a certain threshold and \\(0\\) (no edge) otherwise. This graph’s weighted adjacency matrix \\(W\\) is essentially \\(S\\) with all sub-threshold values zeroed out. Assuming data from \\(k\\) sufficiently distinct clusters is ordered by cluster, the adjacency matrix may look something like \\[\nW = \\begin{bmatrix} W_1 & & & \\\\ & W_2 & & \\\\ & & \\ddots & \\\\ & & & W_k \\end{bmatrix}\\ ,\n\\tag{1}\\]\nwhich you may recognize as a block diagonal matrix.\nSo, what role do eigenvalues play here? Turns out, if \\(v\\) is an eigenvalue of \\(W_1\\), then the same \\(v\\) padded by a bunch of zeros at the bottom is an eigenvalue of \\(W\\). You can verify this via Equation 1; \\(W_1 v = v \\lambda\\) necessarily implies \\(W \\begin{bmatrix} v \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} v \\\\ 0 \\end{bmatrix} \\lambda\\). The same applies for eigenvectors of any cluster, they just need to placed appropriately to act on their cluster’s similarity block \\(W_i\\). As such, the eigenvalues of \\(W\\) will be grouped according to cluster, having zero values in all other locations. Finding these eigenvalues would then reveal the clusters.\nWell… sort of. In reality, we still have some problems: the eigenvalues can have small/zero entries, and finding all \\(n\\) eigenvalues is just too difficult. Instead, we define \\(D\\) to be the diagonal degree matrix, its diagonal entries being the sum of the corresponding rows/columns of \\(W\\). We then obtain the graph Laplacian \\(L = D - W\\), another diagonal block matrix with an additional property that its rows/columns sum to zero. This makes it so multiplying each block \\(L_i\\) by a vector with all entries being \\(1\\) produces the zero vector, making it an eigenvector of \\(L_i\\) with a zero eigenvalue. Additionally, the only vectors satisfying this property are just scalar multiples of that identity vector. As such, the Laplacian \\(L\\) will have its zero eigenspace spanned by precisely \\(k\\) such identity vectors. This adresses both of our previous problems, making it so we only have to find \\(k\\) out of \\(n\\) eigenvectors. Using inverse iteration will obtain these eigenvectors only, and do so very efficiently. Putting the eigenvectors together as a matrix would then make its rows indicators for which cluster a particular point belongs to. In case there is still any ambiguity, the rows can be grouped together using a simpler clustering algorithm like \\(k\\)-means. For more details about spectral clustering, see Luxburg (2007).\n\n\nA pretty cool interpretation of this Laplacian comes from thinking of the nodes in the similarity graph as being connected by springs - the higher the weight between two nodes, the stronger their connecting spring. What happens if we move a single node by some amount? The nodes connected to it would be pulled in the same direction according to the strengths of the springs that connect them to the displaced node, the latter being pulled back with the sum of those strengths. Note that this force is precisely described by the (negative of) the column of \\(L\\) corresponding to the displaced node. If \\(x\\) is the vector representing the displacement of each node, the corresponding force (in the other direction) is given by \\(Lx\\). In the case where the displacement is an eigenvector of \\(L\\), we have \\[\n\\sum F = - L v = - \\lambda v\\ ,\n\\tag{2}\\]\nwhich is precisely Hooke’s law. Thus given an initial displacement \\(v\\), the graph will undergo simple harmonic motion with frequency \\(\\lambda^2\\). In other words, the eigenpairs \\((\\lambda, v)\\) of \\(L\\) actually describe the resonant frequencies \\(\\lambda^2\\) and corresponding vibration modes of its graph. It is for this reason that eigenvalues are crucial to analyzing mechanical resonance - where the graph represents connected parts of a physical object. This could be applied to, for intance, designing buildings that are resistant to strong winds and earthquakes.\nIn our case we saw that the similarity graph had some zero eigenvalues. This corresponds to zero frequency - i.e., once we displace the nodes of a single cluster, it experiences no force from the others and just stays in the same place.\n\n\n\n\nSo, that’s all well and good when similarity relates to Euclidean distance - but as noted before, this becomes less true for high-dimensional data. Instead, data may lie in linear subspaces, such as when representing the same scene illuminated from different angles, or objects moving at different speeds Elhamifar and Vidal (2013). A simplified image of what such data could look like is seen in Figure 2 below.\n\n\n\nFigure 2: Three linear subspaces. Figure by Elhamifar and Vidal (2013).\n\n\nThe point where the lines and plane intersect is the origin. You may note that this matches the mathematical definiton of linear subspaces, i.e., non-empty and closed under addition and scalar multiplication. Alternatively, we may pick any number of vectors and have their span be a linear subspaces. Any point in the subspaces would then be a linear combination of such vectors, which are themselves points in the subspace.\nYou may note that the new dataset is somewhat like a flattened version of Figure 2, with the lines \\(\\mathcal{S}_2\\) and \\(\\mathcal{S}_3\\) being the diagonals making up the ‘X’ shape, and the plane \\(\\mathcal{S}_1\\) being the remaining points in the background square.\n# The 'X'-looking dataset\n\nlinear = np.concatenate((\n        rng.rand(n_samples//2, 2),                              # □ Square\n        rng.rand(n_samples//4, 2) * 0.05\n            + np.repeat(rng.rand(n_samples//4, 1), 2, axis=1),  # / Diagonal\n        rng.rand(n_samples//4, 2) * 0.05\n            + np.repeat(rng.rand(n_samples//4, 1), 2, axis=1)   # \\ Diagonal\n            * [1, -1] + [0, 1]\n    ), axis=0) - [0.5, 0.5], None\nOur goal is then to separate the two diagonals and the square."
  },
  {
    "objectID": "posts/2-clustering.html#clustering",
    "href": "posts/2-clustering.html#clustering",
    "title": "Clustering",
    "section": "",
    "text": "Falling within the realm of unsupervised learning, this task deals with unlabeled but adequately distinct data. While both it and classification involve modeling such differences, clustering in particular starts with no prior informaiton about data labels. In cases where partial label information is known, the task is instead considered to be semi-supervised.\nThe common goal of clustering is to separate data into clusters - groups such that data within them is similar and between them is different. What this means precisely depends on the measure of similarity used; for raw Euclidean distances it would mean that clusters are far apart literally, whereas for feature embedding spaces they would be apart semantically/conceptually.\nClustering serves a variety of purposes:\n\nUnderstanding: separating data into distinct groups may reveal patterns and highlight important features. Additionally, clustering may be viewed as a form of divide-and-conquer where the clusters may be easier to analyze in isolation.\nPreprocessing: similar to the above, the benefit of being able to analyze data separately can be extended to programmatic solutions. That is, cluster assignment can be used to split data and train models separately for each.\nAnomaly detection: the above would additionally get rid of outliers as they wouldn’t belong to any cluster. This makes clustering applicable to anomaly detection as it can identify points that are distant from all other data.\n\nClustering in itself isn’t a method but a task. A plethora of algorithms exist to perform it, thanks to various relevant considerations about the type of clustered data:\n\nShape: while we may typically think of clusters as circular-like objects, they may instead be stretched, spread out, stretched out, bent, etc. For the stretched out type, it additionally becomes important whether data exhibits linear relationships or otherwise lies in nonlinear manifolds.\nData type: the same considerations from other tasks in machine learning apply: is the data numerical or categorical, are any transforms applicable, is some of the data images/audio/text/etc?\nDimensionality: for more complex data types, preprocessing or embedding may be in order. High-dimensional data in general poses a challenge due to the curse of dimensionality, a phenomenon where regular measures of distance break down and lose meaning. In some circumstances it thus becomes crucial to use methods specifically designed around high-dimensional data.\n\n\n\nTo illustrate the former consideration particularly, we can take a look at some distinct sets of artificial data and the performances of various clustering algorithms. Here’s a pretty nice visualization of some common algorithms:\n\n\nClustering scatterplot matrix code by sklearn\nimport time\nimport warnings\nfrom itertools import cycle, islice\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import cluster, datasets, mixture\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.preprocessing import StandardScaler\n\nfrom ssc import linear, SparseSubspaceClustering\n\n# ============\n# Generate datasets. We choose the size big enough to see the scalability\n# of the algorithms, but not too big to avoid too long running times\n# ============\nn_samples = 200\nseed = 5805\nnoisy_circles = datasets.make_circles(\n    n_samples=n_samples, factor=0.5, noise=0.05, random_state=seed\n)\nnoisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=seed)\nblobs = datasets.make_blobs(n_samples=n_samples, random_state=seed)\nrng = np.random.RandomState(seed)\nno_structure = rng.rand(n_samples, 2), None\n\n# Anisotropicly distributed data\nrandom_state = 5805\nX, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\ntransformation = [[0.6, -0.6], [-0.4, 0.8]]\nX_aniso = np.dot(X, transformation)\naniso = (X_aniso, y)\n\n# blobs with varied variances\nvaried = datasets.make_blobs(\n    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n)\n\n# ============\n# Set up cluster parameters\n# ============\nplt.figure(figsize=(10.5, 6.5))\nplt.subplots_adjust(\n    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.01, hspace=0.01\n)\n\nplot_num = 1\n\ndefault_base = {\n    \"quantile\": 0.3,\n    \"eps\": 0.3,\n    \"damping\": 0.9,\n    \"preference\": -200,\n    \"n_neighbors\": 3,\n    \"n_clusters\": 3,\n    \"min_samples\": 7,\n    \"xi\": 0.05,\n    \"min_cluster_size\": 0.1,\n    \"allow_single_cluster\": True,\n    \"hdbscan_min_cluster_size\": 15,\n    \"hdbscan_min_samples\": 3,\n    \"random_state\": 42,\n}\n\ndatasets = [\n    (\n        noisy_circles,\n        {\n            \"damping\": 0.77,\n            \"preference\": -240,\n            \"quantile\": 0.2,\n            \"n_clusters\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.08,\n        },\n    ),\n    (\n        noisy_moons,\n        {\n            \"damping\": 0.75,\n            \"preference\": -220,\n            \"n_clusters\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.1,\n        },\n    ),\n    (\n        varied,\n        {\n            \"eps\": 0.18,\n            \"n_neighbors\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.01,\n            \"min_cluster_size\": 0.2,\n        },\n    ),\n    (\n        aniso,\n        {\n            \"eps\": 0.15,\n            \"n_neighbors\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.1,\n            \"min_cluster_size\": 0.2,\n        },\n    ),\n    (blobs, {\"min_samples\": 7, \"xi\": 0.1, \"min_cluster_size\": 0.2}),\n    (linear, {}),\n    (no_structure, {}),\n]\n\nfor i_dataset, (dataset, algo_params) in enumerate(datasets):\n    # update parameters with dataset-specific values\n    params = default_base.copy()\n    params.update(algo_params)\n\n    X, y = dataset\n\n    # normalize dataset for easier parameter selection\n    X = StandardScaler().fit_transform(X)\n\n    # estimate bandwidth for mean shift\n    bandwidth = cluster.estimate_bandwidth(X, quantile=params[\"quantile\"])\n\n    # connectivity matrix for structured Ward\n    connectivity = kneighbors_graph(\n        X, n_neighbors=params[\"n_neighbors\"], include_self=False\n    )\n    # make connectivity symmetric\n    connectivity = 0.5 * (connectivity + connectivity.T)\n\n    # ============\n    # Create cluster objects\n    # ============\n    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n    two_means = cluster.MiniBatchKMeans(\n        n_clusters=params[\"n_clusters\"],\n        n_init=\"auto\",\n        random_state=params[\"random_state\"],\n    )\n    ward = cluster.AgglomerativeClustering(\n        n_clusters=params[\"n_clusters\"], linkage=\"ward\", connectivity=connectivity\n    )\n    spectral = cluster.SpectralClustering(\n        n_clusters=params[\"n_clusters\"],\n        eigen_solver=\"arpack\",\n        affinity=\"nearest_neighbors\",\n        random_state=params[\"random_state\"],\n    )\n    dbscan = cluster.DBSCAN(eps=params[\"eps\"])\n    hdbscan = cluster.HDBSCAN(\n        min_samples=params[\"hdbscan_min_samples\"],\n        min_cluster_size=params[\"hdbscan_min_cluster_size\"],\n        allow_single_cluster=params[\"allow_single_cluster\"],\n    )\n    optics = cluster.OPTICS(\n        min_samples=params[\"min_samples\"],\n        xi=params[\"xi\"],\n        min_cluster_size=params[\"min_cluster_size\"],\n    )\n    affinity_propagation = cluster.AffinityPropagation(\n        damping=params[\"damping\"],\n        preference=params[\"preference\"],\n        random_state=params[\"random_state\"],\n    )\n    average_linkage = cluster.AgglomerativeClustering(\n        linkage=\"average\",\n        metric=\"cityblock\",\n        n_clusters=params[\"n_clusters\"],\n        connectivity=connectivity,\n    )\n    birch = cluster.Birch(n_clusters=params[\"n_clusters\"])\n    gmm = mixture.GaussianMixture(\n        n_components=params[\"n_clusters\"],\n        covariance_type=\"full\",\n        random_state=params[\"random_state\"],\n    )\n    ssc = SparseSubspaceClustering(\n        n_clusters=params[\"n_clusters\"],\n    )\n\n    clustering_algorithms = (\n        (\"MiniBatch\\nKMeans\", two_means),\n        (\"Affinity\\nPropagation\", affinity_propagation),\n        (\"MeanShift\", ms),\n        (\"Spectral\\nClustering\", spectral),\n        (\"Ward\", ward),\n        (\"Agglomerative\\nClustering\", average_linkage),\n        (\"DBSCAN\", dbscan),\n        (\"HDBSCAN\", hdbscan),\n        (\"OPTICS\", optics),\n        (\"BIRCH\", birch),\n        (\"Gaussian\\nMixture\", gmm),\n        (\"Sparse\\nSubspace\\nClustering\", ssc),\n    )\n\n    for name, algorithm in clustering_algorithms:\n        t0 = time.time()\n\n        # catch warnings related to kneighbors_graph, subspace clustering\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"the number of connected components of the \"\n                + \"connectivity matrix is [0-9]{1,2}\"\n                + \" &gt; 1. Completing it to avoid stopping the tree early.\",\n                category=UserWarning,\n            )\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"Graph is not fully connected, spectral embedding\"\n                + \" may not work as expected.\",\n                category=UserWarning,\n            )\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"Solution may be inaccurate. Try another solver,\"\n                + \" adjusting the solver settings, or solve with\"\n                + \" verbose=True for more information.\",\n                category=UserWarning,\n            )\n            algorithm.fit(X)\n\n        t1 = time.time()\n        if hasattr(algorithm, \"labels_\"):\n            y_pred = algorithm.labels_.astype(int)\n        else:\n            y_pred = algorithm.predict(X)\n\n        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            plt.title(name, size=9)\n\n        colors = np.array(\n            list(\n                islice(\n                    cycle(\n                        [\n                            \"#377eb8\",\n                            \"#ff7f00\",\n                            \"#4daf4a\",\n                            \"#f781bf\",\n                            \"#a65628\",\n                            \"#984ea3\",\n                            \"#999999\",\n                            \"#e41a1c\",\n                            \"#dede00\",\n                        ]\n                    ),\n                    int(max(y_pred) + 1),\n                )\n            )\n        )\n        # add black color for outliers (if any)\n        colors = np.append(colors, [\"#000000\"])\n        plt.scatter(X[:, 0], X[:, 1], s=5, color=colors[y_pred])\n\n        plt.xlim(-2.5, 2.5)\n        plt.ylim(-2.5, 2.5)\n        plt.xticks(())\n        plt.yticks(())\n        plt.text(\n            0.99,\n            0.01,\n            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n            transform=plt.gca().transAxes,\n            size=7.5,\n            horizontalalignment=\"right\",\n        )\n        plot_num += 1\n\nplt.show()\n\n\n\n\n\nFigure 1: A comparison of various clustering methods.\n\n\n\n\nWhoa, that’s a lot of them! If you’ve seen this image before and are particularly observant, you may notice that I threw in an extra algorithm (rightmost column) and a dataset (second-bottom row) - more on those later.\nEach of the above algorithms is deserving of a post of its own. Here, I will just briefly focus on one, which happens to be the only showcased algorithm that properly clusters the top 5 datasets:\n\n\n\nYes, spectral as in spectrum - i.e., eigenvalues. I found out about this algorithm back when I was taking intro to linear algebra, and it really stuck with me precisely because it gives a pretty neat interpretation of eigenvalues (which at the time I had a really hard time exemplifying).\nWe start with a similarity matrix \\(S\\), where the \\(ij\\)-th value denotes the similarity of data points indexed \\(i\\) and \\(j\\). What constitutes as being similar may vary; the example in Figure 1 simply uses Euclidean distance. We then use \\(S\\) to construct a similarity graph, a weighted graph with weights \\(w_{i,j} = s_{i,j}\\) if they are past a certain threshold and \\(0\\) (no edge) otherwise. This graph’s weighted adjacency matrix \\(W\\) is essentially \\(S\\) with all sub-threshold values zeroed out. Assuming data from \\(k\\) sufficiently distinct clusters is ordered by cluster, the adjacency matrix may look something like \\[\nW = \\begin{bmatrix} W_1 & & & \\\\ & W_2 & & \\\\ & & \\ddots & \\\\ & & & W_k \\end{bmatrix}\\ ,\n\\tag{1}\\]\nwhich you may recognize as a block diagonal matrix.\nSo, what role do eigenvalues play here? Turns out, if \\(v\\) is an eigenvalue of \\(W_1\\), then the same \\(v\\) padded by a bunch of zeros at the bottom is an eigenvalue of \\(W\\). You can verify this via Equation 1; \\(W_1 v = v \\lambda\\) necessarily implies \\(W \\begin{bmatrix} v \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} v \\\\ 0 \\end{bmatrix} \\lambda\\). The same applies for eigenvectors of any cluster, they just need to placed appropriately to act on their cluster’s similarity block \\(W_i\\). As such, the eigenvalues of \\(W\\) will be grouped according to cluster, having zero values in all other locations. Finding these eigenvalues would then reveal the clusters.\nWell… sort of. In reality, we still have some problems: the eigenvalues can have small/zero entries, and finding all \\(n\\) eigenvalues is just too difficult. Instead, we define \\(D\\) to be the diagonal degree matrix, its diagonal entries being the sum of the corresponding rows/columns of \\(W\\). We then obtain the graph Laplacian \\(L = D - W\\), another diagonal block matrix with an additional property that its rows/columns sum to zero. This makes it so multiplying each block \\(L_i\\) by a vector with all entries being \\(1\\) produces the zero vector, making it an eigenvector of \\(L_i\\) with a zero eigenvalue. Additionally, the only vectors satisfying this property are just scalar multiples of that identity vector. As such, the Laplacian \\(L\\) will have its zero eigenspace spanned by precisely \\(k\\) such identity vectors. This adresses both of our previous problems, making it so we only have to find \\(k\\) out of \\(n\\) eigenvectors. Using inverse iteration will obtain these eigenvectors only, and do so very efficiently. Putting the eigenvectors together as a matrix would then make its rows indicators for which cluster a particular point belongs to. In case there is still any ambiguity, the rows can be grouped together using a simpler clustering algorithm like \\(k\\)-means. For more details about spectral clustering, see Luxburg (2007).\n\n\nA pretty cool interpretation of this Laplacian comes from thinking of the nodes in the similarity graph as being connected by springs - the higher the weight between two nodes, the stronger their connecting spring. What happens if we move a single node by some amount? The nodes connected to it would be pulled in the same direction according to the strengths of the springs that connect them to the displaced node, the latter being pulled back with the sum of those strengths. Note that this force is precisely described by the (negative of) the column of \\(L\\) corresponding to the displaced node. If \\(x\\) is the vector representing the displacement of each node, the corresponding force (in the other direction) is given by \\(Lx\\). In the case where the displacement is an eigenvector of \\(L\\), we have \\[\n\\sum F = - L v = - \\lambda v\\ ,\n\\tag{2}\\]\nwhich is precisely Hooke’s law. Thus given an initial displacement \\(v\\), the graph will undergo simple harmonic motion with frequency \\(\\lambda^2\\). In other words, the eigenpairs \\((\\lambda, v)\\) of \\(L\\) actually describe the resonant frequencies \\(\\lambda^2\\) and corresponding vibration modes of its graph. It is for this reason that eigenvalues are crucial to analyzing mechanical resonance - where the graph represents connected parts of a physical object. This could be applied to, for intance, designing buildings that are resistant to strong winds and earthquakes.\nIn our case we saw that the similarity graph had some zero eigenvalues. This corresponds to zero frequency - i.e., once we displace the nodes of a single cluster, it experiences no force from the others and just stays in the same place.\n\n\n\n\nSo, that’s all well and good when similarity relates to Euclidean distance - but as noted before, this becomes less true for high-dimensional data. Instead, data may lie in linear subspaces, such as when representing the same scene illuminated from different angles, or objects moving at different speeds Elhamifar and Vidal (2013). A simplified image of what such data could look like is seen in Figure 2 below.\n\n\n\nFigure 2: Three linear subspaces. Figure by Elhamifar and Vidal (2013).\n\n\nThe point where the lines and plane intersect is the origin. You may note that this matches the mathematical definiton of linear subspaces, i.e., non-empty and closed under addition and scalar multiplication. Alternatively, we may pick any number of vectors and have their span be a linear subspaces. Any point in the subspaces would then be a linear combination of such vectors, which are themselves points in the subspace.\nYou may note that the new dataset is somewhat like a flattened version of Figure 2, with the lines \\(\\mathcal{S}_2\\) and \\(\\mathcal{S}_3\\) being the diagonals making up the ‘X’ shape, and the plane \\(\\mathcal{S}_1\\) being the remaining points in the background square.\n# The 'X'-looking dataset\n\nlinear = np.concatenate((\n        rng.rand(n_samples//2, 2),                              # □ Square\n        rng.rand(n_samples//4, 2) * 0.05\n            + np.repeat(rng.rand(n_samples//4, 1), 2, axis=1),  # / Diagonal\n        rng.rand(n_samples//4, 2) * 0.05\n            + np.repeat(rng.rand(n_samples//4, 1), 2, axis=1)   # \\ Diagonal\n            * [1, -1] + [0, 1]\n    ), axis=0) - [0.5, 0.5], None\nOur goal is then to separate the two diagonals and the square."
  },
  {
    "objectID": "posts/2-clustering.html#sparse-subspace-clustering",
    "href": "posts/2-clustering.html#sparse-subspace-clustering",
    "title": "Clustering",
    "section": "Sparse Subspace Clustering",
    "text": "Sparse Subspace Clustering\nTo this end, we will analyze an algorithm based on spectral clustering, proposed by Elhamifar and Vidal (2013). This algorithm relies on the self-expressiveness property described above, i.e., that points in a subspace can be described by a linear combination of other points from the same subspace. Specifically, let \\(Y = \\begin{bmatrix} y_1 & y_2 & \\dots & y_n \\end{bmatrix}\\) be a \\(d\\)-by-\\(n\\) matrix of data. Then, with every data point \\(y_i\\) we can associate a coefficient vector \\(c_i\\) such that \\[\ny_i = Y c_i\\ ,\\qquad\nc_{ii} = 0\\ .\n\\tag{3}\\]\nAs the above has infinitely many solutions, we will additionally \\(\\text{minimize} ||c_i||_q\\). Using smaller \\(q\\) will make more coefficients zero, and for \\(q=1\\) will generally only have nonzero coefficients for points within the same subspace.1\nLetting \\(C = \\begin{bmatrix} c_1 & c_2 & \\dots & c_n \\end{bmatrix}\\), we can rewrite the problem in matrix form as \\[\n\\begin{align}\n    \\text{minimize} \\quad& ||C||_1 \\\\\n    \\text{such that} \\quad& Y = Y C\\ ,\\\\\n    & \\mathrm{diag}(C) = 0\\ .\n\\end{align}\n\\tag{4}\\]\nNote that the objective is convex and the constraints affine, so this can be solved efficiently using\n\nConvex optimization\nA framework for problems with certain nice properties, convex optimization can be applied to a multitude of tasks in various fields. While I first encountered it in exactly this context of sparse subspace clustering, I now use a variant of it (semidefinite programming) all the time for quantum-related stuff.\nLet’s solve an instance of Equation 4 in practice to see why this matrix \\(C\\) could be useful. First, let’s construct a simplified version of the dataset shown above, containing fewer points and only the diagonals.\n\n\nGenerate a simpler dataset\nn = 20\nY = np.concatenate((\n    rng.rand(2, n//2) * 0.01\n        + np.repeat(rng.rand(1, n//2), 2, axis=0),  # / Diagonal\n    rng.rand(2, n//2) * 0.01\n        + np.repeat(rng.rand(1, n//2), 2, axis=0)   # \\ Diagonal\n        * [[1], [-1]] + [[0], [1]]\n), axis=1) - [[0.5], [0.5]]\n\n\nTo construct and solve the convex optimization problem, we will use the python package cvxpy. We essentially just need to declare \\(C\\) as a variable, then specify our objective and constraints. The rest is handled by the solver - everything from verifying that the problem is convex to returning its solution.\n\n\nSolve and visualize a convex optimization problem\nfrom cvxpy import Variable, Minimize, norm, diag, Problem, ECOS\nimport plotly.express as px\n\n# Variable matrix\nC = Variable((n, n))\n\n# Minimize ||C||_1\nobjective = Minimize(norm(C,1))\n\n# Subject to Y = YC, diag(C) = 0\nconstraints = [Y == Y @ C, diag(C) == 0]\nprob = Problem(objective, constraints)\nprob.solve(solver=ECOS)\n\n# Normalize coefficient matrix\nC = C.value / np.max(np.abs(C.value), axis=0)\npx.imshow(C)\n\n\n\n\n                                                \nFigure 3: A heatmap of the obtained coefficient matrix \\(C\\).\n\n\n\n?@fig-C above shows what the (normalized) solution looks like - you may note that it’s a block diagonal matrix much like Equation 1. This is due to the self-expressiveness property; points from a subspace will only have nonzero coefficients corresponding to other points from the same subspace. As such, we may essentially use this \\(C\\) matrix as similarity for (spectral clustering)[#spectral-clustering].\nMore specifically, you may note that there are two rows with quite large coefficients whereas the others are quite small. This comes from minimizing the norm of \\(C\\), leading to us to prioritize points with larger distances from the origin. In the case of one-dimensional subspaces like this, we end up picking the farthest point from the origin and expressing the remaining points as scalar multiples of it. To better approximate the block structures and make the similarity graph undirected, we thus symmetrize the matrix via \\(W = |C| + |C|^\\top\\).\n\n\nImplementation\nAs one last detail, Elhamifar and Vidal (2013) proposes a modified version of Equation 4 that deals with noise \\[\n\\begin{align}\n    \\text{minimize} \\quad& ||C||_1 + \\lambda_e ||E||_1 + \\frac{\\lambda_z}{2} ||Z||_F^2 \\\\\n    \\text{such that} \\quad& Y = Y C + E + Z\\ ,\\\\\n    & \\mathrm{diag}(C) = 0\\ .\n\\end{align}\n\\tag{5}\\]\nThe \\(E\\) matrix is meant to account for outliers and the \\(Z\\) matrix for other noise. The reference material suggests setting \\(\\lambda_e = \\alpha_e / \\mu_e\\) and \\(\\lambda_e = \\alpha_e / \\mu_e\\) with \\[\n\\mu_e = \\min_i \\max_{j \\neq i} ||y_j||_1\\ ,\\qquad\n\\mu_z = \\min_i \\max_{j \\neq i} |y_i^\\top y_j|\\ .\n\\tag{6}\\]\nPutting everything together, the algorithm looks like this:\ndef fit(self, Y):\n    Y = Y.T\n    d, n = Y.shape\n    assert d &lt; n\n\n    ## 1. Solve sparse optimization problem\n\n    # Starting form, equation (5)\n    C = Variable((n, n))\n    objective = norm(C,1)\n    constraint = Y @ C\n\n    # Account for outliers\n    if self.use_E:\n        mu_e = np.partition(np.sum(np.abs(Y), axis=0), -2)[-2]\n        l_e = 20 / mu_e\n        E = Variable((d, n))\n        objective += l_e * norm(E,1)\n        constraint += E\n\n    # Account for noise\n    if self.use_Z:\n        G = np.abs(Y.T @ Y)\n        mu_z = np.min(np.max(G - np.diag(np.diag(G)), axis=1))\n        l_z = 800 / mu_z\n        Z = Variable((d, n))\n        objective += l_z/2 * norm(Z,\"fro\")**2\n        constraint += Z\n\n    constraints = [Y == constraint, diag(C) == 0]\n    prob = Problem(Minimize(objective), constraints)\n    try:\n        prob.solve(solver=ECOS)\n\n        ## 2. Normalize the columns of C\n        C = C.value / np.max(np.abs(C.value), axis=0)\n\n        ## 3. Form the weights of a similarity graph\n        W = np.abs(C)\n        W = W + W.T\n\n        ## 4. Apply spectral clustering on the graph\n        self.labels_ = SpectralClustering(\n                n_clusters=self.n_clusters,\n                affinity='precomputed'\n            ).fit_predict(W)\n    except SolverError:\n        self.labels_ = np.zeros(n)"
  },
  {
    "objectID": "posts/2-clustering.html#footnotes",
    "href": "posts/2-clustering.html#footnotes",
    "title": "Clustering",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that we could also solve for \\(q=0\\), but this is an \\(\\mathsf{NP}\\)-hard problem.↩︎"
  },
  {
    "objectID": "posts/4-classification.html",
    "href": "posts/4-classification.html",
    "title": "Classification",
    "section": "",
    "text": "Image credit: sklearn"
  },
  {
    "objectID": "posts/4-classification.html#simple-dataset",
    "href": "posts/4-classification.html#simple-dataset",
    "title": "Classification",
    "section": "Simple dataset",
    "text": "Simple dataset\n\nIris\n\n\n\nImage credit: Sebastian Raschka\n\n\n\n\n\nImage credit: Gaurav Chauhan\n\n\n\n\n2D scatterplot of iris\nimport plotly.express as px\nfrom sklearn.preprocessing import MinMaxScaler\n\niris = px.data.iris().drop('species_id', axis=1)\nfig = px.scatter(iris, x=\"sepal_width\", y=\"sepal_length\", size=\"petal_length\",\n    color=\"species\", marginal_y=\"violin\", marginal_x=\"violin\",\n    template=\"simple_white\", hover_data=['petal_width'])\nop = MinMaxScaler(feature_range=(0.5, 1.0)).fit_transform(\n    iris['petal_width'].values[:, None])\nfig.update_traces(marker=dict(opacity=op), selector=dict(mode='markers'))\nfig.show();\n\n\n\n                                                \n\n\n\n\n3D scatterplot of iris\nimport plotly.express as px\n\nfig = px.scatter_3d(iris, x=\"sepal_width\", y=\"sepal_length\", z=\"petal_width\",\n    size=\"petal_length\", color=\"species\", template=\"simple_white\")\nfig.show();\n\n\n\n                                                \n\n\n\n\nNaive Bayes\n\n\nNaive bayes classifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, precision_score\n\n# Fit and predict\nX, y = iris.drop('species', axis=1), iris['species']\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.5, random_state=5805)\nnb = GaussianNB().fit(X_train, y_train)\niris['predict'] = nb.predict(X)\ny_pred = nb.predict(X_test)\nprint(f\"Accuracy:  {accuracy_score(y_test, y_pred):.4f}\\n\"\n      f\"Precision: {precision_score(y_test, y_pred, average='macro'):.4f}\")\n\n\nAccuracy:  0.9467\nPrecision: 0.9444\n\n\n\n\nPerformance\n\n\nCode for plotting confusion matrix\nimport seaborn as sn\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\n\nnames = iris['species'].unique()\nconf = confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(conf, index=names, columns=names)\nplt.figure(figsize = (4,3))\nsn.heatmap(df_cm, annot=True, fmt='d');\n\n\n\n\n\n\n\n\n\n\n\n2D scatterplot of iris & predictions\nimport numpy as np\n\nfix = {\"circle\": \"#1F77B4\", \"diamond\": \"#FF7F0E\", \"square\": \"#2CA02C\"}\nfig = px.scatter(iris, x=\"sepal_width\", y=\"sepal_length\", size=\"petal_length\",\n    color=\"species\", symbol=\"predict\",\n    template=\"simple_white\", hover_data=['petal_width'])\nfig.update_traces(marker=dict(opacity=op), selector=dict(mode='markers'))\nfor symbol, color in fix.items():\n    fig.update_traces(marker=dict(line=dict(width=3, color=color),\n                      symbol=\"circle\"),\n                      selector=dict(mode='markers', marker_symbol=symbol))\nfig.show();\n\n\n\n                                                \n\n\n\n\n3D scatterplot of iris & predictions\nimport plotly.express as px\n\nfig = px.scatter_3d(iris, x=\"sepal_width\", y=\"sepal_length\", z=\"petal_width\",\n    size=\"petal_length\", color=\"species\", symbol=\"predict\",\n    template=\"simple_white\")\nfor symbol, color in fix.items():\n    fig.update_traces(marker=dict(line=dict(color=color)),\n                      selector=dict(mode='markers', marker_symbol=symbol))\nfig.show();"
  },
  {
    "objectID": "posts/4-classification.html#complex-dataset",
    "href": "posts/4-classification.html#complex-dataset",
    "title": "Classification",
    "section": "Complex dataset",
    "text": "Complex dataset\n\nMNIST\n\n\n\nImage credit: Orhan G. Yalçın\n\n\n\n\nNeural network\npytorch tutorial\n\n\nImport pytorch\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\nimport torchvision.transforms as transforms\nimport torch.optim as optim\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n\nUsing device: cpu\n\n\n\n\nLoad and preprocess data\ntransform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5), (0.5))\n    ])\nbatch_size = 4\n\ntrainset = MNIST(root='./data', train=True, download=True, transform=transform)\ntrainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n\ntestset = MNIST(root='./data', train=False, download=True, transform=transform)\ntestloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n\nnames = list(range(10))\n\n\n\n\nCreate a neural network\nclass NeuralNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 3, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(3, 6, 5)\n        self.fc1 = nn.Linear(6 * 4 * 4, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        return x\n\nmodel = NeuralNet()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\n\n\n\nTrain a neural network\n# Do one pass over the dataset\nfor i, data in enumerate(trainloader, 0):\n    inputs, labels = data\n\n    # Resent the gradients\n    optimizer.zero_grad()\n\n    # Forward/backward pass\n    outputs = model(inputs)\n    loss = criterion(outputs, labels)\n    loss.backward()\n    optimizer.step()\n\n# Evaluate network\ny_test, y_pred = [], []\nwith torch.no_grad():\n    for data in testloader:\n        inputs, labels = data\n        outputs = model(inputs)\n        _, pred = torch.max(outputs.data, 1)\n        y_test.extend(labels)\n        y_pred.extend(pred)\nprint(f\"Accuracy:  {accuracy_score(y_test, y_pred):.4f}\\n\"\n      f\"Precision: {precision_score(y_test, y_pred, average='macro'):.4f}\")\n\n\nAccuracy:  0.9040\nPrecision: 0.9041\n\n\n\n\nPerformance\n\n\nCode for plotting confusion matrix\nconf = confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(conf, index=names, columns=names)\nplt.figure(figsize = (8.5,7))\nsn.heatmap(df_cm, annot=True, fmt='d');\n\n\n\n\n\n\n\n\n\n\n\nShow test samples with mistakes\nimport torchvision\nimport numpy as np\n\ndef imshow(img):\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n\n# Get samples where mistakes were made\ndef wrong(dataloader):\n    for i, (images, labels) in enumerate(testloader):\n        for j, (image, label) in enumerate(zip(images, labels)):\n            label, pred = label.item(), y_pred[batch_size*i+j].item()\n            if pred != label:\n                yield (image, label, pred)\n\n\n# Plot several examples\nloader = wrong(testloader)\nfor _ in range(4):\n    image, label, pred = next(loader)\n    img = np.transpose(image, (1, 2, 0))\n    plt.imshow(img, cmap='Greys')\n    plt.title(f\"Actual: {label}\\nPredicted: {pred}\", size=42)\n    plt.xticks([],[])\n    plt.yticks([],[])\n    plt.show()"
  }
]