---
title: Anomaly/outlier detection
subtitle: Changepoint detection
description: In which we analyze ways to detect anomalies and account for outliers. Features robust regression and changepoint detection.
date: 2023/12/05
categories:
  - Outliers
  - Regression
  - Changepoint detection
  - Online learning
bibliography: blog.bib
---
<!-- Detect anomaly/outlier behavior and the treatment techniques. -->

Methods for detecting and treating outliers/anomalies are hard to categorize as supervised/unsupervised or as relating more so to [clustering](2-clustering.qmd), [regression](3-regression.qmd), or [classification](4-classification.qmd).
Outliers and anomalies appear all over the place, in all kinds of data and frameworks - and in each case we need to deal with them accordingly.
For instance, we saw that certain [clustering](2-clustering.qmd) methods can label points as outliers; while they can thus be applied to, say, labeled [classification](4-classification.qmd) data opaquely and even only to a select label, this isn't likely to be as effective as considering all classes in unison.
Additionally, neither would apply well to dealing with outliers in [regression](3-regression.qmd), which is what we'll focus on first.


## Outliers

It's hard to give a definition that's both precise and widely applicable, so for now let's treat *outliers* as any kind of unexpected deviations from general trends.
For instance, they could be points away from others or the prediction, points labeled differently, points generated by a different process.

![Image credit: [Ajitesh Kumar](https://vitalflux.com/outlier-detection-techniques-in-python/).](https://i0.wp.com/vitalflux.com/wp-content/uploads/2023/05/Outlier-detection-Python-Machine-Learning.png){width=50%}

In regression particularly, we can denote outliers as points that are very far from the otherwise fitted distribution.
Equivalently, outliers could be considered to be points that significantly disrupt the fitted trend.

Let's try to illustrate this with some (artificial) data.

```{python}
#| code-summary: Run linear regression on uncorrelated data.
#| fig-cap: Uncorrelated data
#| label: fig-base
from pandas import DataFrame
from numpy.random import normal, random, seed
import plotly.express as px
seed(5805)

n = 100
df = DataFrame({'x': normal(size=n-1) + random(n-1), 'y': normal(size=n-1)})
fig = px.scatter(df, x='x', y='y', trendline='ols').show();
```

The above shows a scatterplot of two variables that have no relationship; the fitted line through them is flat only because the deviation of `x` is higher.

Now, watch what happens when we add a very distant point:

```{python}
#| code-summary: Run linear regression on data with an outlier.
#| fig-cap: Regular OLS
#| label: fig-outlier
df.loc[n-1] = {'x': 10, 'y': 10, 'outlier': True}
fig = px.scatter(df, x='x', y='y', trendline='ols')
fig.update_traces(marker=dict(color=["#636efa"] * (n-1) + ["#EF553B"]))
fig.show();
```

The trendline has shifted towards the outlier.
This isn't good, as it could lead us to infer an erroneous correlation between the variables.

Judging from our definition of outliers, one obvious solution already presents itself - why not just remove all points farther than some $s$ standard deviations away from the regressed line?
This actually works fine in practice, though with a caveat that the outlier may actually bring the line towards itself so much that it's no longer considered an outlier.
If we want to catch this, we'll need to fit a trend while excluding one point at a time from the data - which is fairly tedious.

In addition, maybe we don't want to entirely remove the outlier, but just mitigate it to not be as disruptive.
This is useful when it's hard to justify to just throw away data for whichever reason, or when the appropriately adjusted effects of outliers are desireable.

### Huber loss

This specific loss function is used in **robust regression**, a type of regression less sensitive to outliers.
It is defined as

$$
L_\delta(a) = \begin{cases}
    \frac{1}{2} a^2 & \text{if } |a| \leq \delta\ ,\\
    \delta(|a| - \frac{1}{2} \delta) & \text{otherwise}\ ,
\end{cases}
$$ {#eq-huber}

which as you may note is the $\ell_2$ loss when $|a| \leq \delta$ and a scaled $\ell_1$ loss otherwise (joined together nicely).
Specifically, it looks like this:

![Image credit: [Wikipedia](https://en.wikipedia.org/wiki/Huber_loss).](https://upload.wikimedia.org/wikipedia/commons/c/cc/Huber_loss.svg){width=50%}

As such, $\delta$ denotes a boundary around the regression past which the points' effects start to scale more slowly.
Huber loss has the advantage of functioning exactly as, or very close to the $\ell_2$ norm, but avoiding being strongly pulled by outliers.

```{python}
#| code-summary: Run linear regression with Huber loss.
#| fig-cap: OLS with Huber loss
#| label: fig-huber
from numpy import linspace
import plotly.graph_objects as go
from sklearn.linear_model import HuberRegressor

fig = px.scatter(df, x='x', y='y')
fig.update_traces(marker=dict(color=["#636efa"] * (n-1) + ["#EF553B"]))
X = df['x'].values.reshape(-1, 1)
x_range = linspace(X.min(), X.max(), 100).reshape(-1, 1)
y = df['y']

model = HuberRegressor(epsilon=1)
model.fit(X, y)
y_fit = model.predict(x_range)
fig.add_traces(go.Scatter(x=x_range.squeeze(), y=y_fit,
    showlegend=False, line=dict(color="#636efa"))).show();
```

As seen above, the erroneous slope of the line was just about halved.
The outlier thus still has an effect on the regression, just not as pronounced.


## Changepoint detection

The second example I wanted to showcase deals specifically with anomally detection, and applies to a different framework - **online learning**.
We have seen it make an appearance when discussing [multi-armed bandits](1-probability.qmd), which needed to collect information and learn about the environment over time.
Now, we'll consider an algorithm that must similarly receive data and make decisions sequentially.

### Setting

Suppose you are given some *time series* data and tasked to determine whether the distribution producing the data has changed at some point.
We may additionally consider a specific instance called **step detection**, where the data is promised to have a constant mean, until it potentially changes.
So, you take this data and compute some statistics, maybe a windowed average to approximate the mean, and fairly easily narrow down the point where the shift happens.
Fairly easy, right?

Now, imagine that you're given one point of this data at a time, and every time asked to make a decision on whether the change occured.
You can't just wait until the end to announce your decision, as you're retroactively penalized for every data point you consume after the change happens.
You also can't announce a change when you aren't very certain, as you're also penalized for doing so before the change actually happened.
So, what to do?

Turns out this problem is relevant in many real-world scenarios such as intruder detection, stock trading, and quality control.
Today, we'll look at the following dataset, showing the yearly volumes of the Nile at Aswan.

```{python}
#| code-summary: Load and display the Nile dataset
#| fig-cap: Yearly volume of the Nile river at Aswan. A dam was built in 1898.
#| label: fig-nile
import statsmodels.api as sm
import plotly.express as px

# Time series data
nile = sm.datasets.get_rdataset("Nile").data
fig = px.line(nile, x="time", y="value")

# Ground truth change point
fig.add_vline(x=1899, line_width=2, line_dash="dash", line_color="black")
```

The dashed line seen above corresponds to the completion of a dam.
If this change was unexpected, then identifying it and doing so as quickly as possible would be extremely crucial.

### CUSUM

This algorithm / statistical test is fairly old by most standards, being first described by @page_continuous_1954.
It is quite frequent in statistical testing, though I still somehow wasn't able to find a `python` implementation.
For comparison, `MATLAB` has a built-in function [`cusum`](https://www.mathworks.com/help/signal/ref/cusum.html) function, which I'll be using as a rough guide form a ground-up implementation.

#### Motivation

But first, let's look at the underlying idea of CUSUM.
As the name suggests, this algorithm involves keeping track of a **cu**mulative **sum** - let's see one case where this could be useful.

Suppose you're flipping a coin, which originally starts with a $45%$ of the outcome heads.
As you do this, you keep a cumulative sum of the heads minus tails obtained so far, a number that seems to vary but is generally decreasing.

However, after round $75$ you suddenly see this number shoot up, which as you later find out, happened because the coin changed to $75%$ chance heads after that round.
At first you not be sure - the outcomes could just be random but unlikely.
But after a few too many heads, it becomes clear that something's up with the coin and you successfully stop.

```{python}
#| code-summary: A visual example of changepoint detection
#| fig-cap: Cumulative outcomes of flipping a coin with changing probabilities.
#| label: fig-coin
from numpy import concatenate, cumsum, mean

# Simulate coin flipping
n, t = [75, 25], 50
p0, p1 = 0.45, 0.75
coin = concatenate((random((n[0], t)) < p0, random((n[1], t)) < p1))
cumul = mean(cumsum(2*coin-1, axis=0), axis=1)

# Display accumulated counts
avg0 = (2*p0 - 1) * n[0]
avg1 = (2*p1 - 1) * n[1] + avg0
fig = px.scatter(y=cumul, labels={'x': 'Flip count', 'y': "Accumulated (Heads - Tails)"})
fig.add_traces(go.Scatter(x=[n[0],n[0]+n[1]], y=[avg0, avg1],
    mode="lines", name="Changed", line=dict(dash='dash')))
fig.add_traces(go.Scatter(x=[0,n[0]], y=[0, avg0], mode="lines", name="Original"))
```

The ($50$-trial average) path of the cumulative sum is shown above, revealing how the trajectory of said path is influenced by the probability change.
One simple way to detect it would be to keep track of the lowest cusum count seen so far, and if the current round's value exceeds that minimum by a certain threshold, then you terminate the algorithm.

#### Implementation

The same idea applies to the actual algorithm, which keeps track of deviations from the expected mean.
Its first step is to *normalize* the data via $Z_n = (X_n - \bar{x}) / s$.
If $\bar{x}$ and $s$ are not *a priori* available, they are calculated from the first few data points.
Then, at each round, the algorithm keeps track of two *drift* values - one for each possible deviation direction.
They are updated according to
$$
\begin{align}
    H_n = \max(0, H_{n-1} + Z_n - \omega)\ ,\\
    L_n = \min(0, L_{n-1} + Z_n + \omega)\ ,
\end{align}
$$ {#eq-cusum}

where we set $H_0 = L_0 = 0$.
Essentially, @eq-cusum doesn't let $H_t$ drift into the negatives and $L_t$ into the positives.
At each round, both values are dampened by $\omega$.
If either deviates by more than some threshold, the algorithm stops and detects a changepoint.
Let's implement this algorithm and run it on the Nile dataset.

```{python}
#| code-summary: Implementation of the CUSUM algorithm
#| fig-cap: CUSUM control chart.
#| label: fig-cusum
import numpy as np

def cusum(X, lim=5, w=0, mean=None, std=None, plot=True):
    '''Detect changes in mean using the CUSUM test.

    ARGUMENTS:
    lim: number of standard deviations in drift to be detected.
    w: number of standard devitaions as damping coefficient.
    mean: expected mean or None to compute from 25 leading samples.
    std: expected standard deviation or None to compute from 25 leading samples.
    plot: whether to display a visualization.

    RETURNS:
    index of the first detected change or None.
    '''
    # Estimate mean and standard deviation
    if mean is None:
        mean = np.mean(X[:25])
    if std is None:
        std = np.mean(X[:25])
    Z = (X - mean) / std

    n = len(Z)
    H = np.zeros(n)
    L = np.zeros(n)
    for i, z in enumerate(Z):
        H[i] = max(0, H[i-1] + z - w)
        L[i] = min(0, L[i-1] + z + w)

    idx_H = np.argmax(np.append(H, lim) >= lim)
    idx_L = np.argmax(np.append(L, -lim) <= -lim)
    idx = min(idx_H, idx_L)

    if plot:
        fig = go.Figure()
        fig.add_traces(go.Scatter(y=H, name="Upper cumulative sum"))
        fig.add_traces(go.Scatter(y=L, name="Lower cumulative sum"))
        fig.add_hline(y=lim, line_dash='dash', line_color="#636efa")
        fig.add_hline(y=-lim, line_dash='dash', line_color="#EF553B")
        if idx < n:
            HL = H if idx_H <= idx_L else L
            fig.add_traces(go.Scatter(x=[idx], y=[HL[idx]],
                mode="markers", marker_symbol='circle-open', name="Changepoint",
                marker=dict(size=12, line_width=4, color='black')))
        fig.show()

    return idx if idx < n else None

change = cusum(nile['value'], lim=2)
print(f"Detected changepoint at year {nile['time'][change]}.")
```

As seen by the control chart, CUSUM predicts a change in the year 1905, only 6 years after the dam's construction.
This delay generally depends on the value of the threshold, which shouldn't be lowered too much due to a higher chance of false positives.
If on the contrary we set the threshold too high, the algorithm would still likely predict it, just after some additional time.
If we additionally had wanted to find the year where the change occured, we could trace the [red]{.red} line from the control chart back to when it first started drifting down, giving us pretty much the exact year construction was finished.

In summary, the CUSUM algorithm lets us detect a changepoint *anomaly* in time-series data in an online manner.
