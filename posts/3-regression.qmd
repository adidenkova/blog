---
title: Linear and nonlinear regression
description: TODO
date: 2023/12/03
categories:
  - TODO
---
<!-- Analyze multiple linear and nonlinear regression. -->


## Regression

`glm(y ~ x, ...)` or `glm(y ~ ., ...)`

```{python}
#| code-fold: true
#| code-summary: Create artificial data
#| label: fig-data
#| fig-cap: A couple of artificial datasets
#| fig-subcap:
#|   - Linear growth
#|   - Exponential growth
#| layout-ncol: 2
#| column: page
import plotly.express as px
from numpy.random import rand, normal, seed
from numpy import repeat, concatenate, power
from pandas import DataFrame
seed(5805)

# Create artificial data
n, r, s, b = 100, 100, 5, 10
lin = repeat(rand(n, 1) * r, 2, axis=1) + normal(scale=s, size=(n, 2))
exp = power(b, lin[:,1:2] / r) * r / b
exp = concatenate((lin[:,0:1], exp), axis=1)

# Plot data
lin_df = DataFrame({'x': lin[:,0], 'y': lin[:,1]})
exp_df = DataFrame({'x': exp[:,0], 'y': exp[:,1]})
px.scatter(lin_df, x='x', y='y').show()
px.scatter(exp_df, x='x', y='y').show();
```

## Linear regression

![Image credit: [Wikipedia](https://en.wikipedia.org/wiki/Linear_least_squares#Example)](https://upload.wikimedia.org/wikipedia/commons/b/b0/Linear_least_squares_example2.svg)

$$
\hat{\beta} = (X^\top X)^{-1} X^\top y = X^\dagger y\ ,
$$ {#eq-ols}

where $X^\dagger$ denotes the left pseudoinverse of $X$.

```{python}
#| code-fold: true
#| code-summary: Code for polynomial OLS
# Taken from https://plotly.com/python/ml-regression/
from numpy import linspace
import plotly.graph_objects as go
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

def plot(df, trend, **kwargs):
    fig = px.scatter(df, x='x', y='y')
    X = df['x'].values.reshape(-1, 1)
    x_range = linspace(X.min(), X.max(), 100).reshape(-1, 1)
    y = df['y']

    y_fit = trend(X, x_range, y, **kwargs)
    fig.add_traces(go.Scatter(x=x_range.squeeze(), y=y_fit,
        showlegend=False, line=dict(color='black')))
    return fig

def poly(X, x_range, y, degree=2):
    poly = PolynomialFeatures(degree)
    poly.fit(X)
    X_poly = poly.transform(X)
    x_range_poly = poly.transform(x_range)

    model = LinearRegression(fit_intercept=False)
    model.fit(X_poly, y)
    return model.predict(x_range_poly)

def show(fig):
    fig.update_layout(
            margin=dict(l=0, r=20, t=20, b=20),
            height=380
        ).show()
```

```{python}
#| code-summary: Fit trendlines via ordinary least squares
#| label: fig-ols
#| fig-cap: Trendlines via ordinary least squares
#| fig-subcap:
#|   - Ordinary least squares
#|   - OLS of y ~ x^2 + x
#|   - OLS of log(y) ~ x
#| layout-ncol: 3
#| column: page
show(px.scatter(lin_df, x='x', y='y', trendline='ols',
    trendline_color_override='black'))
show(plot(exp_df, poly, degree=2))
show(px.scatter(exp_df, x='x', y='y', trendline='ols',
    trendline_options=dict(log_y=True),
    trendline_color_override='black'))
```

```{python}
#| code-fold: true
#| code-summary: Code for KNN regression
# Taken from https://plotly.com/python/ml-regression/
import numpy as np
from sklearn.neighbors import KNeighborsRegressor

def knn(X, x_range, y, k=20, weights='uniform'):
    knn_uni = KNeighborsRegressor(10, weights=weights)
    knn_uni.fit(X, y)
    return knn_uni.predict(x_range.reshape(-1, 1))
```

```{python}
#| code-summary: Fit trendlines via miscellaneous methods
#| label: fig-other
#| fig-cap: Trendlines via other regression methods
#| fig-subcap:
#|   - Nonlinear least squares
#|   - KNN Regression
#|   - LOWESS
#| layout-ncol: 3
#| column: page
from scipy.optimize import curve_fit

# Nonlinear regression
f = lambda x, a, b, c: a * x / (b + x) + c
def nonlinear(X, x_range, y, f, init=None):
    args, _ = curve_fit(f, exp_df['x'], exp_df['y'], p0=init)
    return f(x_range, *args).squeeze()
show(plot(exp_df, nonlinear, f=f, init=[-90, -200, 10]))

# Non-parametric regression
show(plot(exp_df, knn, k=20))
show(px.scatter(exp_df, x='x', y='y', trendline='lowess',
   trendline_color_override='black'))
```

```{python}
#| code-fold: true
#| code-summary: Fit a multiratiate regression model
#| label: fig-multi
#| fig-cap: Support Vector Regression on the Iris dataset
#| warning: false
# Taken from https://plotly.com/python/ml-regression/
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from sklearn.svm import SVR

mesh_size = .02
margin = 0

df = px.data.iris()

X = df[['sepal_width', 'sepal_length']]
y = df['petal_width']

# Condition the model on sepal width and length, predict the petal width
model = SVR(C=1.)
model.fit(X, y)

# Create a mesh grid on which we will run our model
x_min, x_max = X.sepal_width.min() - margin, X.sepal_width.max() + margin
y_min, y_max = X.sepal_length.min() - margin, X.sepal_length.max() + margin
xrange = np.arange(x_min, x_max, mesh_size)
yrange = np.arange(y_min, y_max, mesh_size)
xx, yy = np.meshgrid(xrange, yrange)

# Run model
pred = model.predict(np.c_[xx.ravel(), yy.ravel()])
pred = pred.reshape(xx.shape)

# Generate the plot
fig = px.scatter_3d(df, x='sepal_width', y='sepal_length', z='petal_width')
fig.update_traces(marker=dict(size=5))
fig.add_traces(go.Surface(x=xrange, y=yrange, z=pred, name='pred_surface', opacity=0.75))
fig.show()
```