
@misc{elhamifar_sparse_2013,
	title = {Sparse {Subspace} {Clustering}: {Algorithm}, {Theory}, and {Applications}},
	shorttitle = {Sparse {Subspace} {Clustering}},
	url = {http://arxiv.org/abs/1203.1005},
	abstract = {Many real-world problems deal with collections of high-dimensional data, such as images, videos, text and web documents, DNA microarray data, and more. Often, such high-dimensional data lie close to low-dimensional structures corresponding to several classes or categories to which the data belong. In this paper, we propose and study an algorithm, called Sparse Subspace Clustering (SSC), to cluster data points that lie in a union of low-dimensional subspaces. The key idea is that, among the inﬁnitely many possible representations of a data point in terms of other points, a sparse representation corresponds to selecting a few points from the same subspace. This motivates solving a sparse optimization program whose solution is used in a spectral clustering framework to infer the clustering of the data into subspaces. Since solving the sparse optimization program is in general NP-hard, we consider a convex relaxation and show that, under appropriate conditions on the arrangement of the subspaces and the distribution of the data, the proposed minimization program succeeds in recovering the desired sparse representations. The proposed algorithm is efﬁcient and can handle data points near the intersections of subspaces. Another key advantage of the proposed algorithm with respect to the state of the art is that it can deal directly with data nuisances, such as noise, sparse outlying entries, and missing entries, by incorporating the model of the data into the sparse optimization program. We demonstrate the effectiveness of the proposed algorithm through experiments on synthetic data as well as the two real-world problems of motion segmentation and face clustering.},
	language = {en},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Elhamifar, Ehsan and Vidal, Rene},
	month = feb,
	year = {2013},
	note = {arXiv:1203.1005 [cs, math, stat]},
}

@misc{abhishek_introduction_2019,
	title = {Introduction to {Concentration} {Inequalities}},
	url = {http://arxiv.org/abs/1910.02884},
	abstract = {In this report, we aim to exemplify concentration inequalities and provide easy to understand proofs for it. Our focus is on the inequalities which are helpful in the design and analysis of machine learning algorithms.},
	language = {en},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Abhishek, Kumar and Maheshwari, Sneha and Gujar, Sujit},
	month = oct,
	year = {2019},
	note = {arXiv:1910.02884 [cs, math, stat]},
}

@book{fristedt_modern_1996,
	title = {A {Modern} {Approach} to {Probability} {Theory}},
	isbn = {978-0-8176-3807-8},
	abstract = {Overview This book is intended as a textbook in probability for graduate students in math ematics and related areas such as statistics, economics, physics, and operations research. Probability theory is a 'difficult' but productive marriage of mathemat ical abstraction and everyday intuition, and we have attempted to exhibit this fact. Thus we may appear at times to be obsessively careful in our presentation of the material, but our experience has shown that many students find them selves quite handicapped because they have never properly come to grips with the subtleties of the definitions and mathematical structures that form the foun dation of the field. Also, students may find many of the examples and problems to be computationally challenging, but it is our belief that one of the fascinat ing aspects of prob ability theory is its ability to say something concrete about the world around us, and we have done our best to coax the student into doing explicit calculations, often in the context of apparently elementary models. The practical applications of probability theory to various scientific fields are far-reaching, and a specialized treatment would be required to do justice to the interrelations between prob ability and any one of these areas. However, to give the reader a taste of the possibilities, we have included some examples, particularly from the field of statistics, such as order statistics, Dirichlet distri butions, and minimum variance unbiased estimation.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Fristedt, Bert E. and Gray, Lawrence F.},
	month = dec,
	year = {1996},
}

@misc{ji_online_2022,
	address = {CS6104 Advanced Topics in Theory of Computation},
	title = {Online {Learning} and {Sequential} {Decision} {Making}},
	author = {Ji, Bo},
	year = {2022},
}

@misc{slivkins_introduction_2022,
	title = {Introduction to {Multi}-{Armed} {Bandits}},
	url = {http://arxiv.org/abs/1904.07272},
	abstract = {Multi-armed bandits a simple but very powerful framework for algorithms that make decisions over time under uncertainty. An enormous body of work has accumulated over the years, covered in several books and surveys. This book provides a more introductory, textbook-like treatment of the subject. Each chapter tackles a particular line of work, providing a self-contained, teachable technical introduction and a brief review of the further developments; many of the chapters conclude with exercises.},
	language = {en},
	urldate = {2023-12-07},
	publisher = {arXiv},
	author = {Slivkins, Aleksandrs},
	month = jan,
	year = {2022},
	note = {arXiv:1904.07272 [cs, stat]},
}

@article{page_continuous_1954,
	title = {Continuous {Inspection} {Schemes}},
	volume = {41},
	issn = {0006-3444},
	doi = {10.2307/2333009},
	number = {1-2},
	journal = {Biometrika},
	author = {Page, E. S.},
	year = {1954},
	pages = {100--115},
}

@misc{von_luxburg_tutorial_2007,
	title = {A {Tutorial} on {Spectral} {Clustering}},
	url = {http://arxiv.org/abs/0711.0189},
	abstract = {In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved eﬃciently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the ﬁrst glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe diﬀerent graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several diﬀerent approaches. Advantages and disadvantages of the diﬀerent spectral clustering algorithms are discussed.},
	language = {en},
	urldate = {2023-12-11},
	publisher = {arXiv},
	author = {von Luxburg, Ulrike},
	month = nov,
	year = {2007},
	note = {arXiv:0711.0189 [cs]},
}
